Starting script at: Thu 12/12/2024 11:05:37.31
german_data dataset preparation
train set:  (600, 7) (600, 13) (600,)
val set:  (200, 7) (200, 13) (200,)
test set:  (200, 7) (200, 13) (200,)
pakdd_data dataset preparation
train set:  (30000, 8) (30000, 25) (30000,)
val set:  (10000, 8) (10000, 25) (10000,)
test set:  (10000, 8) (10000, 25) (10000,)
hmeq_data dataset preparation
train set:  (3576, 10) (3576, 2) (3576,)
val set:  (1192, 10) (1192, 2) (1192,)
test set:  (1192, 10) (1192, 2) (1192,)
taiwan dataset preparation
train set:  (18000, 14) (18000, 9) (18000,)
val set:  (6000, 14) (6000, 9) (6000,)
test set:  (6000, 14) (6000, 9) (6000,)
gmsc_data dataset preparation
train set:  (90000, 10) (90000, 0) (90000,)
val set:  (30000, 10) (30000, 0) (30000,)
test set:  (30000, 10) (30000, 0) (30000,)
Finished data prep at: Thu 12/12/2024 11:05:42.10
######################### myTabddpm (identity): uci_german #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 61}
mlp
Step 500/1000 MLoss: 0.8974 GLoss: 0.9615 Sum: 1.8589
Step 1000/1000 MLoss: 0.8748 GLoss: 0.8125 Sum: 1.6873
Found files in Experiments\tabddpm\identity\uci_german:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Discrete cols: [0, 2, 3, 5, 6]
Saving Synthetic Data at:  Experiments/tabddpm/identity/uci_german
Num shape:  (1055, 7)
Elapsed time: 0:02:55
Finished uci_german (identity) at: Thu 12/12/2024 11:08:43.07
######################### myTabddpm (identity): uci_taiwan #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 91}
mlp
Step 500/1000 MLoss: 0.701 GLoss: 0.8895 Sum: 1.5905
Step 1000/1000 MLoss: 0.6655 GLoss: 0.7005 Sum: 1.366
Found files in Experiments\tabddpm\identity\uci_taiwan:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/identity/uci_taiwan
Num shape:  (60209, 14)
Elapsed time: 0:23:13
Finished uci_taiwan (identity) at: Thu 12/12/2024 11:32:01.35
######################### myTabddpm (identity): hmeq #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 18}
mlp
Step 500/1000 MLoss: 0.9508 GLoss: 0.7167 Sum: 1.6675
Step 1000/1000 MLoss: 0.9459 GLoss: 0.6465 Sum: 1.5924
Found files in Experiments\tabddpm\identity\hmeq:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Discrete cols: [4, 5, 7]
Saving Synthetic Data at:  Experiments/tabddpm/identity/hmeq
Num shape:  (15067, 10)
Elapsed time: 0:03:37
Finished hmeq (identity) at: Thu 12/12/2024 11:35:54.71
######################### myTabddpm (identity): gmsc #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 10}
mlp
Step 500/1000 MLoss: 0.0 GLoss: 0.276 Sum: 0.276
Step 1000/1000 MLoss: 0.0 GLoss: 0.2592 Sum: 0.2592
Found files in Experiments\tabddpm\identity\gmsc:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: [2, 6, 7, 8, 9]
Saving Synthetic Data at:  Experiments/tabddpm/identity/gmsc
Num shape:  (1544386, 10)
Elapsed time: 2:08:59
Finished gmsc (identity) at: Thu 12/12/2024 13:45:00.15
######################### myTabddpm (identity): pakdd #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 181}
mlp
Found files in Experiments\tabddpm\identity\pakdd:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: [5, 6, 7]
Saving Synthetic Data at:  Experiments/tabddpm/identity/pakdd
Num shape:  (74187, 8)
Elapsed time: 0:44:50
Finished pakdd (identity) at: Thu 12/12/2024 14:30:04.95
######################### myTabddpm (bgm): uci_german #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 85}
mlp
Step 500/1000 MLoss: 0.8869 GLoss: 0.9723 Sum: 1.8592
Step 1000/1000 MLoss: 0.8227 GLoss: 0.814 Sum: 1.6366999999999998
Found files in Experiments\tabddpm\bgm\uci_german:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/uci_german
Num shape:  (1055, 7)
Elapsed time: 0:03:49
Finished uci_german (bgm) at: Thu 12/12/2024 14:33:59.87
######################### myTabddpm (bgm): uci_taiwan #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 200}
mlp
Step 500/1000 MLoss: 1.0527 GLoss: 0.9639 Sum: 2.0166
Step 1000/1000 MLoss: 1.0295 GLoss: 0.8524 Sum: 1.8819000000000001
Found files in Experiments\tabddpm\bgm\uci_taiwan:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/uci_taiwan
Num shape:  (60209, 14)
Elapsed time: 0:51:11
Finished uci_taiwan (bgm) at: Thu 12/12/2024 15:25:16.75
######################### myTabddpm (bgm): hmeq #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 81}
mlp
Step 500/1000 MLoss: 1.2927 GLoss: 0.8805 Sum: 2.1732
Step 1000/1000 MLoss: 1.2526 GLoss: 0.7047 Sum: 1.9573
Found files in Experiments\tabddpm\bgm\hmeq:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/hmeq
Num shape:  (15067, 10)
Elapsed time: 0:08:26
Finished hmeq (bgm) at: Thu 12/12/2024 15:33:48.78
######################### myTabddpm (bgm): gmsc #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 60}
mlp
Step 500/1000 MLoss: 0.7992 GLoss: 0.8864 Sum: 1.6856
Step 1000/1000 MLoss: 0.7465 GLoss: 0.7891 Sum: 1.5356
Found files in Experiments\tabddpm\bgm\gmsc:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/gmsc
Num shape:  (1544386, 10)
Elapsed time: 7:33:33
Finished gmsc (bgm) at: Thu 12/12/2024 23:07:28.25
######################### myTabddpm (bgm): pakdd #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 219}
mlp
Found files in Experiments\tabddpm\bgm\pakdd:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/pakdd
Num shape:  (74187, 8)
Elapsed time: 0:55:02
Finished pakdd (bgm) at: Fri 12/13/2024  0:02:37.84
######################### PYTHON SRC/MAIN.PY #########################
########## uci_german ##########
DEBUG:root:Dataloader: Loading uci_german
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: uci_german. Returning data.
########## uci_taiwan ##########
DEBUG:root:Dataloader: Loading uci_taiwan
DEBUG:root:Dataloader: Loaded available datasets.
       LIMIT_BAL    SEX EDUCATION MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  ... BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month
ID                                                                                  ...
1          20000  cat_2     cat_3    cat_2   24  cat_5  cat_5  cat_2  cat_2  cat_1  ...         0          0          0         0       689         0         0         0         0                           1
2         120000  cat_2     cat_3    cat_3   26  cat_2  cat_5  cat_3  cat_3  cat_3  ...      3272       3455       3261         0      1000      1000      1000         0      2000                           1
3          90000  cat_2     cat_3    cat_3   34  cat_3  cat_3  cat_3  cat_3  cat_3  ...     14331      14948      15549      1518      1500      1000      1000      1000      5000                           0
4          50000  cat_2     cat_3    cat_2   37  cat_3  cat_3  cat_3  cat_3  cat_3  ...     28314      28959      29547      2000      2019      1200      1100      1069      1000                           0
5          50000  cat_1     cat_3    cat_2   57  cat_2  cat_3  cat_2  cat_3  cat_3  ...     20940      19146      19131      2000     36681     10000      9000       689       679                           0
...          ...    ...       ...      ...  ...    ...    ...    ...    ...    ...  ...       ...        ...        ...       ...       ...       ...       ...       ...       ...                         ...
29996     220000  cat_1     cat_4    cat_2   39  cat_3  cat_3  cat_3  cat_3  cat_3  ...     88004      31237      15980      8500     20000      5003      3047      5000      1000                           0
29997     150000  cat_1     cat_4    cat_3   43  cat_2  cat_2  cat_2  cat_2  cat_3  ...      8979       5190          0      1837      3526      8998       129         0         0                           0
29998      30000  cat_1     cat_3    cat_3   37  cat_7  cat_6  cat_5  cat_2  cat_3  ...     20878      20582      19357         0         0     22000      4200      2000      3100                           1
29999      80000  cat_1     cat_4    cat_2   41  cat_4  cat_2  cat_3  cat_3  cat_3  ...     52774      11855      48944     85900      3409      1178      1926     52964      1804                           1
30000      50000  cat_1     cat_3    cat_2   46  cat_3  cat_3  cat_3  cat_3  cat_3  ...     36535      32428      15313      2078      1800      1430      1000      1000      1000                           1

[30000 rows x 24 columns]
INFO:root:Dataloader: Loaded dataset: uci_taiwan. Returning data.
########## pakdd ##########
DEBUG:root:Dataloader: Loading pakdd
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: pakdd. Returning data.
########## hmeq ##########
DEBUG:root:Dataloader: Loading hmeq
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: hmeq. Returning data.
########## gmsc ##########
DEBUG:root:Dataloader: Loading gmsc
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: gmsc. Returning data.
###########################################
###########################################
########### TRADITIONAL METHODS ###########
###########################################
###########################################
############### Dataset: uci_german, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1569
AUC-ROC: 0.7856
AUC-PR: 0.5637

############### Dataset: uci_german, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2418
AUC-ROC: 0.7625
AUC-PR: 0.4646

############### Dataset: uci_german, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1721
AUC-ROC: 0.7853
AUC-PR: 0.5456

############### Dataset: uci_german, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1811
AUC-ROC: 0.8020
AUC-PR: 0.6198

############### Dataset: uci_german, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2423
AUC-ROC: 0.7765
AUC-PR: 0.4412

############### Dataset: uci_german, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1859
AUC-ROC: 0.7859
AUC-PR: 0.4860

############### Dataset: uci_german, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2155
AUC-ROC: 0.6897
AUC-PR: 0.4301

############### Dataset: uci_german, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2438
AUC-ROC: 0.7079
AUC-PR: 0.4059

############### Dataset: uci_german, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2241
AUC-ROC: 0.6965
AUC-PR: 0.4348

############### Dataset: uci_german, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1999
AUC-ROC: 0.7853
AUC-PR: 0.5978

############### Dataset: uci_german, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2434
AUC-ROC: 0.7713
AUC-PR: 0.4440

############### Dataset: uci_german, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2177
AUC-ROC: 0.7775
AUC-PR: 0.5728

############### Dataset: uci_german, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1624
AUC-ROC: 0.7622
AUC-PR: 0.5401

############### Dataset: uci_german, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2398
AUC-ROC: 0.7734
AUC-PR: 0.4745

############### Dataset: uci_german, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1695
AUC-ROC: 0.7749
AUC-PR: 0.4645

############### Dataset: uci_german, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1568
AUC-ROC: 0.7916
AUC-PR: 0.5809

############### Dataset: uci_german, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.7859
AUC-PR: 0.4892

############### Dataset: uci_german, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1558
AUC-ROC: 0.8046
AUC-PR: 0.5629

############### Dataset: uci_german, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1524
AUC-ROC: 0.8004
AUC-PR: 0.6410

############### Dataset: uci_german, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2407
AUC-ROC: 0.7890
AUC-PR: 0.5031

############### Dataset: uci_german, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1578
AUC-ROC: 0.7989
AUC-PR: 0.5457

############### Dataset: uci_german, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1519
AUC-ROC: 0.8085
AUC-PR: 0.6025

############### Dataset: uci_german, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2404
AUC-ROC: 0.7620
AUC-PR: 0.4606

############### Dataset: uci_german, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1568
AUC-ROC: 0.8015
AUC-PR: 0.5237

############### Dataset: uci_german, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2043
AUC-ROC: 0.7911
AUC-PR: 0.5714

############### Dataset: uci_german, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2364
AUC-ROC: 0.7963
AUC-PR: 0.5324

############### Dataset: uci_german, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2417
AUC-ROC: 0.7885
AUC-PR: 0.5242

############### Dataset: uci_german, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1516
AUC-ROC: 0.8012
AUC-PR: 0.6394

############### Dataset: uci_german, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2406
AUC-ROC: 0.7957
AUC-PR: 0.4980

############### Dataset: uci_german, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1544
AUC-ROC: 0.7994
AUC-PR: 0.5772

############### Dataset: uci_german, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1489
AUC-ROC: 0.8121
AUC-PR: 0.6482

############### Dataset: uci_german, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2394
AUC-ROC: 0.7874
AUC-PR: 0.5093

############### Dataset: uci_german, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1567
AUC-ROC: 0.7931
AUC-PR: 0.5186

############### Dataset: uci_taiwan, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1358
AUC-ROC: 0.7786
AUC-PR: 0.5664

############### Dataset: uci_taiwan, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2470
AUC-ROC: 0.7929
AUC-PR: 0.5775

############### Dataset: uci_taiwan, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1711
AUC-ROC: 0.7986
AUC-PR: 0.5900

############### Dataset: uci_taiwan, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1804
AUC-ROC: 0.7869
AUC-PR: 0.5721

############### Dataset: uci_taiwan, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2469
AUC-ROC: 0.7934
AUC-PR: 0.5767

############### Dataset: uci_taiwan, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1733
AUC-ROC: 0.7992
AUC-PR: 0.5860

############### Dataset: uci_taiwan, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.3932
AUC-ROC: 0.6336
AUC-PR: 0.3352

############### Dataset: uci_taiwan, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2603
AUC-ROC: 0.6382
AUC-PR: 0.2960

############### Dataset: uci_taiwan, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.3404
AUC-ROC: 0.6587
AUC-PR: 0.3145

############### Dataset: uci_taiwan, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1559
AUC-ROC: 0.7860
AUC-PR: 0.5880

############### Dataset: uci_taiwan, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2436
AUC-ROC: 0.8009
AUC-PR: 0.5853

############### Dataset: uci_taiwan, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1470
AUC-ROC: 0.8021
AUC-PR: 0.5866

############### Dataset: uci_taiwan, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1331
AUC-ROC: 0.7831
AUC-PR: 0.5716

############### Dataset: uci_taiwan, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2436
AUC-ROC: 0.7951
AUC-PR: 0.5783

############### Dataset: uci_taiwan, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1279
AUC-ROC: 0.8012
AUC-PR: 0.5905

############### Dataset: uci_taiwan, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1502
AUC-ROC: 0.7675
AUC-PR: 0.5493

############### Dataset: uci_taiwan, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2467
AUC-ROC: 0.7713
AUC-PR: 0.5633

############### Dataset: uci_taiwan, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1577
AUC-ROC: 0.7862
AUC-PR: 0.5850

############### Dataset: uci_taiwan, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1537
AUC-ROC: 0.7670
AUC-PR: 0.5180

############### Dataset: uci_taiwan, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2474
AUC-ROC: 0.7592
AUC-PR: 0.5440

############### Dataset: uci_taiwan, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1698
AUC-ROC: 0.7836
AUC-PR: 0.5733

############### Dataset: uci_taiwan, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1506
AUC-ROC: 0.7733
AUC-PR: 0.5290

############### Dataset: uci_taiwan, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2475
AUC-ROC: 0.7806
AUC-PR: 0.5558

############### Dataset: uci_taiwan, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1731
AUC-ROC: 0.7876
AUC-PR: 0.5650

############### Dataset: uci_taiwan, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1726
AUC-ROC: 0.7749
AUC-PR: 0.5625

############### Dataset: uci_taiwan, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2453
AUC-ROC: 0.7813
AUC-PR: 0.5700

############### Dataset: uci_taiwan, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1789
AUC-ROC: 0.7891
AUC-PR: 0.5828

############### Dataset: uci_taiwan, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1516
AUC-ROC: 0.7630
AUC-PR: 0.5411

############### Dataset: uci_taiwan, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2465
AUC-ROC: 0.7646
AUC-PR: 0.5633

############### Dataset: uci_taiwan, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1576
AUC-ROC: 0.7854
AUC-PR: 0.5811

############### Dataset: uci_taiwan, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1546
AUC-ROC: 0.7614
AUC-PR: 0.5399

############### Dataset: uci_taiwan, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2473
AUC-ROC: 0.7601
AUC-PR: 0.5499

############### Dataset: uci_taiwan, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1677
AUC-ROC: 0.7803
AUC-PR: 0.5795

############### Dataset: pakdd, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1928
AUC-ROC: 0.6253
AUC-PR: 0.3474

############### Dataset: pakdd, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2494
AUC-ROC: 0.6398
AUC-PR: 0.3539

############### Dataset: pakdd, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2333
AUC-ROC: 0.6529
AUC-PR: 0.3671

############### Dataset: pakdd, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2409
AUC-ROC: 0.6247
AUC-PR: 0.3446

############### Dataset: pakdd, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2494
AUC-ROC: 0.6391
AUC-PR: 0.3528

############### Dataset: pakdd, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2346
AUC-ROC: 0.6525
AUC-PR: 0.3622

############### Dataset: pakdd, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.3210
AUC-ROC: 0.5483
AUC-PR: 0.2844

############### Dataset: pakdd, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2526
AUC-ROC: 0.5534
AUC-PR: 0.2812

############### Dataset: pakdd, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.3065
AUC-ROC: 0.5569
AUC-PR: 0.2893

############### Dataset: pakdd, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2367
AUC-ROC: 0.6366
AUC-PR: 0.3554

############### Dataset: pakdd, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2488
AUC-ROC: 0.6348
AUC-PR: 0.3512

############### Dataset: pakdd, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2194
AUC-ROC: 0.6473
AUC-PR: 0.3606

############### Dataset: pakdd, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1885
AUC-ROC: 0.6244
AUC-PR: 0.3499

############### Dataset: pakdd, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2471
AUC-ROC: 0.6361
AUC-PR: 0.3467

############### Dataset: pakdd, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1828
AUC-ROC: 0.6513
AUC-PR: 0.3661

############### Dataset: pakdd, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1909
AUC-ROC: 0.6186
AUC-PR: 0.3426

############### Dataset: pakdd, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2486
AUC-ROC: 0.6012
AUC-PR: 0.3290

############### Dataset: pakdd, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1922
AUC-ROC: 0.6351
AUC-PR: 0.3529

############### Dataset: pakdd, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1900
AUC-ROC: 0.6218
AUC-PR: 0.3458

############### Dataset: pakdd, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2485
AUC-ROC: 0.6114
AUC-PR: 0.3411

############### Dataset: pakdd, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1918
AUC-ROC: 0.6318
AUC-PR: 0.3502

############### Dataset: pakdd, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1915
AUC-ROC: 0.6232
AUC-PR: 0.3447

############### Dataset: pakdd, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2488
AUC-ROC: 0.6018
AUC-PR: 0.3259

############### Dataset: pakdd, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1956
AUC-ROC: 0.6294
AUC-PR: 0.3510

############### Dataset: pakdd, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2671
AUC-ROC: 0.6353
AUC-PR: 0.3554

############### Dataset: pakdd, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2503
AUC-ROC: 0.6221
AUC-PR: 0.3379

############### Dataset: pakdd, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2781
AUC-ROC: 0.6447
AUC-PR: 0.3671

############### Dataset: pakdd, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1901
AUC-ROC: 0.6207
AUC-PR: 0.3455

############### Dataset: pakdd, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2485
AUC-ROC: 0.6015
AUC-PR: 0.3285

############### Dataset: pakdd, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1915
AUC-ROC: 0.6330
AUC-PR: 0.3490

############### Dataset: pakdd, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1911
AUC-ROC: 0.6162
AUC-PR: 0.3446

############### Dataset: pakdd, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2487
AUC-ROC: 0.6049
AUC-PR: 0.3295

############### Dataset: pakdd, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1932
AUC-ROC: 0.6311
AUC-PR: 0.3484

############### Dataset: hmeq, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0574
AUC-ROC: 0.9697
AUC-PR: 0.9007

############### Dataset: hmeq, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2369
AUC-ROC: 0.9186
AUC-PR: 0.8294

############### Dataset: hmeq, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0920
AUC-ROC: 0.9207
AUC-PR: 0.8356

############### Dataset: hmeq, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0912
AUC-ROC: 0.9486
AUC-PR: 0.8532

############### Dataset: hmeq, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2371
AUC-ROC: 0.9162
AUC-PR: 0.8058

############### Dataset: hmeq, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0962
AUC-ROC: 0.9252
AUC-PR: 0.8354

############### Dataset: hmeq, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2608
AUC-ROC: 0.8410
AUC-PR: 0.5857

############### Dataset: hmeq, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2557
AUC-ROC: 0.8027
AUC-PR: 0.4805

############### Dataset: hmeq, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2311
AUC-ROC: 0.8537
AUC-PR: 0.6489

############### Dataset: hmeq, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0567
AUC-ROC: 0.9728
AUC-PR: 0.9166

############### Dataset: hmeq, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2328
AUC-ROC: 0.9110
AUC-PR: 0.8160

############### Dataset: hmeq, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0719
AUC-ROC: 0.9243
AUC-PR: 0.8419

############### Dataset: hmeq, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0553
AUC-ROC: 0.9733
AUC-PR: 0.9203

############### Dataset: hmeq, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2329
AUC-ROC: 0.9118
AUC-PR: 0.8180

############### Dataset: hmeq, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0711
AUC-ROC: 0.9196
AUC-PR: 0.8429

############### Dataset: hmeq, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0514
AUC-ROC: 0.9796
AUC-PR: 0.9340

############### Dataset: hmeq, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.9018
AUC-PR: 0.8018

############### Dataset: hmeq, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0862
AUC-ROC: 0.9191
AUC-PR: 0.8287

############### Dataset: hmeq, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0544
AUC-ROC: 0.9748
AUC-PR: 0.9217

############### Dataset: hmeq, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2396
AUC-ROC: 0.8913
AUC-PR: 0.7782

############### Dataset: hmeq, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0913
AUC-ROC: 0.9169
AUC-PR: 0.8187

############### Dataset: hmeq, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0516
AUC-ROC: 0.9791
AUC-PR: 0.9350

############### Dataset: hmeq, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2390
AUC-ROC: 0.8927
AUC-PR: 0.7677

############### Dataset: hmeq, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0948
AUC-ROC: 0.9199
AUC-PR: 0.8227

############### Dataset: hmeq, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0549
AUC-ROC: 0.9736
AUC-PR: 0.9095

############### Dataset: hmeq, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.8909
AUC-PR: 0.7867

############### Dataset: hmeq, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0850
AUC-ROC: 0.9157
AUC-PR: 0.8292

############### Dataset: hmeq, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0531
AUC-ROC: 0.9775
AUC-PR: 0.9277

############### Dataset: hmeq, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.8960
AUC-PR: 0.7968

############### Dataset: hmeq, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0873
AUC-ROC: 0.9173
AUC-PR: 0.8288

############### Dataset: hmeq, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0527
AUC-ROC: 0.9786
AUC-PR: 0.9274

############### Dataset: hmeq, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2393
AUC-ROC: 0.8959
AUC-PR: 0.7875

############### Dataset: hmeq, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0874
AUC-ROC: 0.9206
AUC-PR: 0.8301

############### Dataset: gmsc, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0560
AUC-ROC: 0.8228
AUC-PR: 0.3177

############### Dataset: gmsc, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2461
AUC-ROC: 0.8257
AUC-PR: 0.3570

############### Dataset: gmsc, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1470
AUC-ROC: 0.8305
AUC-PR: 0.3744

############### Dataset: gmsc, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1515
AUC-ROC: 0.8481
AUC-PR: 0.3592

############### Dataset: gmsc, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2461
AUC-ROC: 0.8257
AUC-PR: 0.3465

############### Dataset: gmsc, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1472
AUC-ROC: 0.8390
AUC-PR: 0.3755

############### Dataset: gmsc, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.4779
AUC-ROC: 0.7510
AUC-PR: 0.3221

############### Dataset: gmsc, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2756
AUC-ROC: 0.6883
AUC-PR: 0.1181

############### Dataset: gmsc, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.4115
AUC-ROC: 0.7448
AUC-PR: 0.2189

############### Dataset: gmsc, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0551
AUC-ROC: 0.8403
AUC-PR: 0.3664

############### Dataset: gmsc, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2352
AUC-ROC: 0.8253
AUC-PR: 0.3681

############### Dataset: gmsc, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0532
AUC-ROC: 0.8308
AUC-PR: 0.3808

############### Dataset: gmsc, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0517
AUC-ROC: 0.8376
AUC-PR: 0.3601

############### Dataset: gmsc, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2349
AUC-ROC: 0.8250
AUC-PR: 0.3537

############### Dataset: gmsc, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0503
AUC-ROC: 0.8303
AUC-PR: 0.3851

############### Dataset: gmsc, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0608
AUC-ROC: 0.8246
AUC-PR: 0.3058

############### Dataset: gmsc, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2435
AUC-ROC: 0.8008
AUC-PR: 0.3265

############### Dataset: gmsc, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0835
AUC-ROC: 0.8188
AUC-PR: 0.3447

############### Dataset: gmsc, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0575
AUC-ROC: 0.8319
AUC-PR: 0.3150

############### Dataset: gmsc, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2417
AUC-ROC: 0.8083
AUC-PR: 0.3232

############### Dataset: gmsc, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0789
AUC-ROC: 0.8217
AUC-PR: 0.3369

############### Dataset: gmsc, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0587
AUC-ROC: 0.8352
AUC-PR: 0.3028

############### Dataset: gmsc, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2430
AUC-ROC: 0.8076
AUC-PR: 0.3258

############### Dataset: gmsc, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1001
AUC-ROC: 0.8216
AUC-PR: 0.3409

############### Dataset: gmsc, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0680
AUC-ROC: 0.8406
AUC-PR: 0.3544

############### Dataset: gmsc, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2422
AUC-ROC: 0.8153
AUC-PR: 0.3428

############### Dataset: gmsc, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0918
AUC-ROC: 0.8286
AUC-PR: 0.3578

############### Dataset: gmsc, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0604
AUC-ROC: 0.8257
AUC-PR: 0.3066

############### Dataset: gmsc, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2433
AUC-ROC: 0.8097
AUC-PR: 0.3297

############### Dataset: gmsc, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0818
AUC-ROC: 0.8214
AUC-PR: 0.3473

############### Dataset: gmsc, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0620
AUC-ROC: 0.8184
AUC-PR: 0.3016

############### Dataset: gmsc, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2441
AUC-ROC: 0.8033
AUC-PR: 0.3187

############### Dataset: gmsc, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0890
AUC-ROC: 0.8159
AUC-PR: 0.3392

###########################################
###########################################
################## CWGAN ##################
###########################################
###########################################
############# uci_german #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.8356 AUC: 0.9095 BCE: 0.3637
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 15 iters per epoch to reach target of 4500.
[ 2500/4500] LG:0.358 LD:0.593 D:0.690 GP:0.006 AC: 0.481 RMSEAVG:0.089 NUM:0.104 SynTraiAuc:0.550 RFAcc:0.851
[ 4500/4500] LG:0.254 LD:0.765 D:0.873 GP:0.007 AC: 0.551 RMSEAVG:0.066 NUM:0.082 SynTraiAuc:0.550 RFAcc:0.836
INFO:root:Finished training after 4500/4500.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1690
AUC-ROC: 0.7583
AUC-PR: 0.4830

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2395
AUC-ROC: 0.7807
AUC-PR: 0.4757

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1604
AUC-ROC: 0.7687
AUC-PR: 0.4861

############# uci_taiwan #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.8279 AUC: 0.8064 BCE: 0.4113
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 422 iters per epoch to reach target of 126600.
[ 2500/126600] LG:1.145 LD:0.848 D:0.886 GP:0.003 AC: 0.344 RMSEAVG:0.035 NUM:0.048 SynTraiAuc:0.462 RFAcc:0.991
[ 5000/126600] LG:1.147 LD:0.824 D:0.859 GP:0.002 AC: 0.329 RMSEAVG:0.025 NUM:0.027 SynTraiAuc:0.689 RFAcc:0.906
[ 7500/126600] LG:1.435 LD:0.558 D:0.624 GP:0.004 AC: 0.328 RMSEAVG:0.041 NUM:0.027 SynTraiAuc:0.689 RFAcc:0.951
[10000/126600] LG:1.657 LD:0.608 D:0.651 GP:0.003 AC: 0.330 RMSEAVG:0.062 NUM:0.018 SynTraiAuc:0.732 RFAcc:0.916
[12500/126600] LG:1.900 LD:0.661 D:0.697 GP:0.002 AC: 0.327 RMSEAVG:0.037 NUM:0.024 SynTraiAuc:0.732 RFAcc:0.941
[15000/126600] LG:1.975 LD:0.732 D:0.777 GP:0.003 AC: 0.312 RMSEAVG:0.044 NUM:0.025 SynTraiAuc:0.735 RFAcc:0.909
[17500/126600] LG:2.077 LD:0.845 D:0.915 GP:0.005 AC: 0.325 RMSEAVG:0.040 NUM:0.015 SynTraiAuc:0.735 RFAcc:0.961
[20000/126600] LG:2.223 LD:0.840 D:0.887 GP:0.003 AC: 0.337 RMSEAVG:0.034 NUM:0.016 SynTraiAuc:0.756 RFAcc:0.941
[22500/126600] LG:2.393 LD:0.704 D:0.755 GP:0.003 AC: 0.312 RMSEAVG:0.036 NUM:0.023 SynTraiAuc:0.756 RFAcc:0.882
[25000/126600] LG:2.478 LD:0.772 D:0.856 GP:0.006 AC: 0.310 RMSEAVG:0.044 NUM:0.018 SynTraiAuc:0.734 RFAcc:0.941
[27500/126600] LG:2.590 LD:0.784 D:0.845 GP:0.004 AC: 0.312 RMSEAVG:0.042 NUM:0.020 SynTraiAuc:0.734 RFAcc:0.880
[30000/126600] LG:2.562 LD:0.745 D:0.809 GP:0.004 AC: 0.302 RMSEAVG:0.047 NUM:0.024 SynTraiAuc:0.732 RFAcc:0.901
[32500/126600] LG:2.559 LD:0.710 D:0.768 GP:0.004 AC: 0.320 RMSEAVG:0.057 NUM:0.019 SynTraiAuc:0.732 RFAcc:0.844
[35000/126600] LG:2.733 LD:0.777 D:0.812 GP:0.002 AC: 0.312 RMSEAVG:0.058 NUM:0.027 SynTraiAuc:0.745 RFAcc:0.887
[37500/126600] LG:2.743 LD:0.590 D:0.649 GP:0.004 AC: 0.308 RMSEAVG:0.055 NUM:0.022 SynTraiAuc:0.745 RFAcc:0.910
[40000/126600] LG:2.794 LD:0.679 D:0.726 GP:0.003 AC: 0.306 RMSEAVG:0.038 NUM:0.022 SynTraiAuc:0.738 RFAcc:0.895
[42500/126600] LG:2.897 LD:0.719 D:0.768 GP:0.003 AC: 0.307 RMSEAVG:0.048 NUM:0.018 SynTraiAuc:0.738 RFAcc:0.916
[45000/126600] LG:3.024 LD:0.623 D:0.677 GP:0.004 AC: 0.316 RMSEAVG:0.044 NUM:0.019 SynTraiAuc:0.712 RFAcc:0.901
[47500/126600] LG:3.096 LD:0.654 D:0.702 GP:0.003 AC: 0.319 RMSEAVG:0.034 NUM:0.028 SynTraiAuc:0.712 RFAcc:0.939
[50000/126600] LG:3.230 LD:0.621 D:0.672 GP:0.003 AC: 0.318 RMSEAVG:0.032 NUM:0.022 SynTraiAuc:0.743 RFAcc:0.955
[52500/126600] LG:3.337 LD:0.513 D:0.569 GP:0.004 AC: 0.306 RMSEAVG:0.042 NUM:0.025 SynTraiAuc:0.743 RFAcc:0.917
[55000/126600] LG:3.319 LD:0.622 D:0.673 GP:0.003 AC: 0.316 RMSEAVG:0.028 NUM:0.014 SynTraiAuc:0.732 RFAcc:0.956
[57500/126600] LG:3.479 LD:0.442 D:0.512 GP:0.005 AC: 0.315 RMSEAVG:0.027 NUM:0.026 SynTraiAuc:0.732 RFAcc:0.930
[60000/126600] LG:3.649 LD:0.421 D:0.482 GP:0.004 AC: 0.327 RMSEAVG:0.036 NUM:0.024 SynTraiAuc:0.709 RFAcc:0.897
[62500/126600] LG:3.723 LD:0.598 D:0.649 GP:0.003 AC: 0.319 RMSEAVG:0.046 NUM:0.023 SynTraiAuc:0.709 RFAcc:0.925
[65000/126600] LG:3.930 LD:0.532 D:0.568 GP:0.002 AC: 0.305 RMSEAVG:0.041 NUM:0.014 SynTraiAuc:0.728 RFAcc:0.916
[67500/126600] LG:3.979 LD:0.504 D:0.536 GP:0.002 AC: 0.314 RMSEAVG:0.030 NUM:0.028 SynTraiAuc:0.728 RFAcc:0.945
[70000/126600] LG:4.056 LD:0.523 D:0.590 GP:0.005 AC: 0.313 RMSEAVG:0.040 NUM:0.018 SynTraiAuc:0.722 RFAcc:0.927
[72500/126600] LG:4.039 LD:0.426 D:0.488 GP:0.004 AC: 0.304 RMSEAVG:0.026 NUM:0.021 SynTraiAuc:0.722 RFAcc:0.849
[75000/126600] LG:4.086 LD:0.460 D:0.508 GP:0.003 AC: 0.305 RMSEAVG:0.032 NUM:0.015 SynTraiAuc:0.719 RFAcc:0.897
[77500/126600] LG:4.134 LD:0.408 D:0.459 GP:0.003 AC: 0.304 RMSEAVG:0.025 NUM:0.020 SynTraiAuc:0.719 RFAcc:0.930
[80000/126600] LG:4.000 LD:0.486 D:0.542 GP:0.004 AC: 0.301 RMSEAVG:0.026 NUM:0.018 SynTraiAuc:0.718 RFAcc:0.910
[82500/126600] LG:4.123 LD:0.504 D:0.534 GP:0.002 AC: 0.306 RMSEAVG:0.027 NUM:0.024 SynTraiAuc:0.718 RFAcc:0.912
[85000/126600] LG:4.140 LD:0.366 D:0.415 GP:0.003 AC: 0.315 RMSEAVG:0.028 NUM:0.010 SynTraiAuc:0.730 RFAcc:0.956
[87500/126600] LG:4.174 LD:0.422 D:0.452 GP:0.002 AC: 0.303 RMSEAVG:0.031 NUM:0.017 SynTraiAuc:0.730 RFAcc:0.914
[90000/126600] LG:4.248 LD:0.344 D:0.384 GP:0.003 AC: 0.309 RMSEAVG:0.029 NUM:0.023 SynTraiAuc:0.727 RFAcc:0.901
[92500/126600] LG:4.256 LD:0.384 D:0.416 GP:0.002 AC: 0.303 RMSEAVG:0.030 NUM:0.023 SynTraiAuc:0.727 RFAcc:0.886
[95000/126600] LG:4.445 LD:0.304 D:0.351 GP:0.003 AC: 0.309 RMSEAVG:0.021 NUM:0.022 SynTraiAuc:0.753 RFAcc:0.891
[97500/126600] LG:4.459 LD:0.303 D:0.352 GP:0.003 AC: 0.303 RMSEAVG:0.029 NUM:0.016 SynTraiAuc:0.753 RFAcc:0.921
[100000/126600] LG:4.414 LD:0.412 D:0.437 GP:0.002 AC: 0.311 RMSEAVG:0.024 NUM:0.018 SynTraiAuc:0.705 RFAcc:0.961
[102500/126600] LG:4.477 LD:0.297 D:0.343 GP:0.003 AC: 0.303 RMSEAVG:0.029 NUM:0.022 SynTraiAuc:0.705 RFAcc:0.879
[105000/126600] LG:4.571 LD:0.365 D:0.413 GP:0.003 AC: 0.306 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.729 RFAcc:0.916
[107500/126600] LG:4.536 LD:0.361 D:0.408 GP:0.003 AC: 0.306 RMSEAVG:0.021 NUM:0.017 SynTraiAuc:0.729 RFAcc:0.927
[110000/126600] LG:4.552 LD:0.255 D:0.323 GP:0.004 AC: 0.313 RMSEAVG:0.024 NUM:0.027 SynTraiAuc:0.740 RFAcc:0.886
[112500/126600] LG:4.633 LD:0.418 D:0.458 GP:0.003 AC: 0.308 RMSEAVG:0.027 NUM:0.012 SynTraiAuc:0.740 RFAcc:0.925
[115000/126600] LG:4.610 LD:0.334 D:0.372 GP:0.003 AC: 0.303 RMSEAVG:0.026 NUM:0.019 SynTraiAuc:0.730 RFAcc:0.859
[117500/126600] LG:4.669 LD:0.310 D:0.349 GP:0.003 AC: 0.304 RMSEAVG:0.026 NUM:0.018 SynTraiAuc:0.730 RFAcc:0.901
[120000/126600] LG:4.608 LD:0.300 D:0.336 GP:0.002 AC: 0.311 RMSEAVG:0.029 NUM:0.025 SynTraiAuc:0.737 RFAcc:0.940
[122500/126600] LG:4.695 LD:0.282 D:0.322 GP:0.003 AC: 0.304 RMSEAVG:0.032 NUM:0.018 SynTraiAuc:0.737 RFAcc:0.884
[125000/126600] LG:4.728 LD:0.260 D:0.293 GP:0.002 AC: 0.304 RMSEAVG:0.028 NUM:0.019 SynTraiAuc:0.746 RFAcc:0.909
[126600/126600] LG:4.771 LD:0.276 D:0.304 GP:0.002 AC: 0.314 RMSEAVG:0.029 NUM:0.017 SynTraiAuc:0.746 RFAcc:0.902
INFO:root:Finished training after 126600/126600.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1319
AUC-ROC: 0.7821
AUC-PR: 0.5769

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2435
AUC-ROC: 0.7927
AUC-PR: 0.5796

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1287
AUC-ROC: 0.8017
AUC-PR: 0.5889

############# pakdd #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.7684 AUC: 0.7535 BCE: 0.4912
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 704 iters per epoch to reach target of 211200.
[ 2500/211200] LG:0.595 LD:1.195 D:1.240 GP:0.003 AC: 0.406 RMSEAVG:0.037 NUM:0.029 SynTraiAuc:0.505 RFAcc:0.975
[ 5000/211200] LG:0.694 LD:1.188 D:1.227 GP:0.003 AC: 0.447 RMSEAVG:0.032 NUM:0.024 SynTraiAuc:0.575 RFAcc:0.980
[ 7500/211200] LG:1.026 LD:1.048 D:1.200 GP:0.010 AC: 0.407 RMSEAVG:0.022 NUM:0.030 SynTraiAuc:0.575 RFAcc:0.973
[10000/211200] LG:1.278 LD:1.183 D:1.335 GP:0.010 AC: 0.382 RMSEAVG:0.024 NUM:0.017 SynTraiAuc:0.572 RFAcc:0.932
[12500/211200] LG:1.581 LD:1.306 D:1.413 GP:0.007 AC: 0.328 RMSEAVG:0.022 NUM:0.029 SynTraiAuc:0.572 RFAcc:0.970
[15000/211200] LG:1.681 LD:1.304 D:1.378 GP:0.005 AC: 0.366 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.569 RFAcc:0.984
[17500/211200] LG:1.871 LD:1.292 D:1.391 GP:0.007 AC: 0.377 RMSEAVG:0.023 NUM:0.041 SynTraiAuc:0.569 RFAcc:0.951
[20000/211200] LG:1.813 LD:1.202 D:1.279 GP:0.005 AC: 0.338 RMSEAVG:0.017 NUM:0.019 SynTraiAuc:0.579 RFAcc:0.966
[22500/211200] LG:1.747 LD:1.395 D:1.486 GP:0.006 AC: 0.323 RMSEAVG:0.024 NUM:0.027 SynTraiAuc:0.579 RFAcc:0.938
[25000/211200] LG:1.957 LD:1.246 D:1.320 GP:0.005 AC: 0.358 RMSEAVG:0.019 NUM:0.039 SynTraiAuc:0.566 RFAcc:0.894
[27500/211200] LG:2.080 LD:1.237 D:1.294 GP:0.004 AC: 0.362 RMSEAVG:0.016 NUM:0.019 SynTraiAuc:0.566 RFAcc:0.971
[30000/211200] LG:2.236 LD:1.198 D:1.281 GP:0.006 AC: 0.313 RMSEAVG:0.018 NUM:0.021 SynTraiAuc:0.578 RFAcc:0.943
[32500/211200] LG:2.332 LD:1.215 D:1.309 GP:0.006 AC: 0.321 RMSEAVG:0.015 NUM:0.034 SynTraiAuc:0.578 RFAcc:0.945
[35000/211200] LG:2.463 LD:1.229 D:1.351 GP:0.008 AC: 0.334 RMSEAVG:0.014 NUM:0.028 SynTraiAuc:0.589 RFAcc:0.984
[37500/211200] LG:2.631 LD:1.368 D:1.438 GP:0.005 AC: 0.338 RMSEAVG:0.018 NUM:0.024 SynTraiAuc:0.589 RFAcc:0.948
[40000/211200] LG:2.698 LD:1.321 D:1.419 GP:0.007 AC: 0.319 RMSEAVG:0.015 NUM:0.034 SynTraiAuc:0.584 RFAcc:0.960
[42500/211200] LG:2.707 LD:1.307 D:1.498 GP:0.013 AC: 0.366 RMSEAVG:0.016 NUM:0.033 SynTraiAuc:0.584 RFAcc:0.956
[45000/211200] LG:2.751 LD:1.209 D:1.285 GP:0.005 AC: 0.379 RMSEAVG:0.018 NUM:0.053 SynTraiAuc:0.564 RFAcc:0.973
[47500/211200] LG:2.719 LD:1.188 D:1.266 GP:0.005 AC: 0.335 RMSEAVG:0.013 NUM:0.020 SynTraiAuc:0.564 RFAcc:0.949
[50000/211200] LG:2.722 LD:1.170 D:1.260 GP:0.006 AC: 0.320 RMSEAVG:0.013 NUM:0.024 SynTraiAuc:0.580 RFAcc:0.976
[52500/211200] LG:2.743 LD:1.148 D:1.219 GP:0.005 AC: 0.327 RMSEAVG:0.015 NUM:0.011 SynTraiAuc:0.580 RFAcc:0.929
[55000/211200] LG:2.729 LD:0.962 D:1.026 GP:0.004 AC: 0.337 RMSEAVG:0.015 NUM:0.032 SynTraiAuc:0.581 RFAcc:0.965
[57500/211200] LG:2.697 LD:0.992 D:1.079 GP:0.006 AC: 0.330 RMSEAVG:0.017 NUM:0.039 SynTraiAuc:0.581 RFAcc:0.875
[60000/211200] LG:2.745 LD:1.028 D:1.114 GP:0.006 AC: 0.314 RMSEAVG:0.014 NUM:0.031 SynTraiAuc:0.582 RFAcc:0.974
[62500/211200] LG:2.776 LD:1.006 D:1.070 GP:0.004 AC: 0.312 RMSEAVG:0.012 NUM:0.028 SynTraiAuc:0.582 RFAcc:0.944
[65000/211200] LG:2.760 LD:0.991 D:1.064 GP:0.005 AC: 0.332 RMSEAVG:0.014 NUM:0.020 SynTraiAuc:0.548 RFAcc:0.951
[67500/211200] LG:2.783 LD:0.936 D:1.017 GP:0.005 AC: 0.338 RMSEAVG:0.014 NUM:0.030 SynTraiAuc:0.548 RFAcc:0.925
[70000/211200] LG:2.800 LD:0.854 D:0.930 GP:0.005 AC: 0.316 RMSEAVG:0.013 NUM:0.020 SynTraiAuc:0.575 RFAcc:0.970
[72500/211200] LG:2.940 LD:0.853 D:0.921 GP:0.005 AC: 0.337 RMSEAVG:0.013 NUM:0.031 SynTraiAuc:0.575 RFAcc:0.979
[75000/211200] LG:3.032 LD:0.926 D:1.006 GP:0.005 AC: 0.357 RMSEAVG:0.013 NUM:0.032 SynTraiAuc:0.559 RFAcc:0.981
[77500/211200] LG:2.858 LD:0.834 D:0.907 GP:0.005 AC: 0.339 RMSEAVG:0.013 NUM:0.036 SynTraiAuc:0.559 RFAcc:0.950
[80000/211200] LG:2.970 LD:0.830 D:0.909 GP:0.005 AC: 0.312 RMSEAVG:0.018 NUM:0.035 SynTraiAuc:0.577 RFAcc:0.953
[82500/211200] LG:2.862 LD:0.816 D:0.881 GP:0.004 AC: 0.312 RMSEAVG:0.012 NUM:0.027 SynTraiAuc:0.577 RFAcc:0.964
[85000/211200] LG:2.940 LD:0.733 D:0.825 GP:0.006 AC: 0.334 RMSEAVG:0.014 NUM:0.042 SynTraiAuc:0.579 RFAcc:0.950
[87500/211200] LG:2.945 LD:0.721 D:0.782 GP:0.004 AC: 0.316 RMSEAVG:0.013 NUM:0.040 SynTraiAuc:0.579 RFAcc:0.926
[90000/211200] LG:2.975 LD:0.844 D:0.911 GP:0.004 AC: 0.334 RMSEAVG:0.015 NUM:0.031 SynTraiAuc:0.578 RFAcc:0.998
[92500/211200] LG:3.002 LD:0.576 D:0.632 GP:0.004 AC: 0.337 RMSEAVG:0.012 NUM:0.038 SynTraiAuc:0.578 RFAcc:0.926
[95000/211200] LG:2.989 LD:0.582 D:0.694 GP:0.007 AC: 0.323 RMSEAVG:0.013 NUM:0.031 SynTraiAuc:0.570 RFAcc:0.915
[97500/211200] LG:3.040 LD:0.686 D:0.740 GP:0.004 AC: 0.359 RMSEAVG:0.013 NUM:0.033 SynTraiAuc:0.570 RFAcc:0.984
[100000/211200] LG:3.015 LD:0.654 D:0.696 GP:0.003 AC: 0.332 RMSEAVG:0.012 NUM:0.025 SynTraiAuc:0.547 RFAcc:0.999
[102500/211200] LG:3.099 LD:0.640 D:0.680 GP:0.003 AC: 0.336 RMSEAVG:0.014 NUM:0.037 SynTraiAuc:0.547 RFAcc:0.936
[105000/211200] LG:3.056 LD:0.635 D:0.710 GP:0.005 AC: 0.328 RMSEAVG:0.014 NUM:0.049 SynTraiAuc:0.549 RFAcc:0.966
[107500/211200] LG:3.170 LD:0.527 D:0.569 GP:0.003 AC: 0.346 RMSEAVG:0.011 NUM:0.045 SynTraiAuc:0.549 RFAcc:0.950
[110000/211200] LG:3.195 LD:0.567 D:0.610 GP:0.003 AC: 0.334 RMSEAVG:0.013 NUM:0.036 SynTraiAuc:0.527 RFAcc:0.951
[112500/211200] LG:3.266 LD:0.717 D:0.775 GP:0.004 AC: 0.362 RMSEAVG:0.012 NUM:0.027 SynTraiAuc:0.527 RFAcc:0.998
[115000/211200] LG:3.213 LD:0.561 D:0.625 GP:0.004 AC: 0.350 RMSEAVG:0.014 NUM:0.046 SynTraiAuc:0.537 RFAcc:0.930
[117500/211200] LG:3.209 LD:0.660 D:0.700 GP:0.003 AC: 0.380 RMSEAVG:0.016 NUM:0.042 SynTraiAuc:0.537 RFAcc:1.000
[120000/211200] LG:3.151 LD:0.601 D:0.668 GP:0.004 AC: 0.384 RMSEAVG:0.014 NUM:0.041 SynTraiAuc:0.540 RFAcc:0.960
[122500/211200] LG:3.104 LD:0.538 D:0.609 GP:0.005 AC: 0.340 RMSEAVG:0.011 NUM:0.036 SynTraiAuc:0.540 RFAcc:0.959
[125000/211200] LG:3.186 LD:0.602 D:0.659 GP:0.004 AC: 0.349 RMSEAVG:0.012 NUM:0.050 SynTraiAuc:0.535 RFAcc:0.986
[127500/211200] LG:3.159 LD:0.620 D:0.670 GP:0.003 AC: 0.333 RMSEAVG:0.014 NUM:0.052 SynTraiAuc:0.535 RFAcc:0.993
[130000/211200] LG:3.127 LD:0.616 D:0.665 GP:0.003 AC: 0.381 RMSEAVG:0.012 NUM:0.038 SynTraiAuc:0.552 RFAcc:0.981
[132500/211200] LG:3.029 LD:0.512 D:0.596 GP:0.006 AC: 0.311 RMSEAVG:0.013 NUM:0.045 SynTraiAuc:0.552 RFAcc:0.988
[135000/211200] LG:3.223 LD:0.545 D:0.585 GP:0.003 AC: 0.328 RMSEAVG:0.017 NUM:0.048 SynTraiAuc:0.540 RFAcc:0.975
[137500/211200] LG:3.184 LD:0.426 D:0.454 GP:0.002 AC: 0.337 RMSEAVG:0.012 NUM:0.037 SynTraiAuc:0.540 RFAcc:0.971
[140000/211200] LG:3.155 LD:0.528 D:0.637 GP:0.007 AC: 0.337 RMSEAVG:0.014 NUM:0.050 SynTraiAuc:0.537 RFAcc:0.978
[142500/211200] LG:3.174 LD:0.582 D:0.608 GP:0.002 AC: 0.341 RMSEAVG:0.012 NUM:0.040 SynTraiAuc:0.537 RFAcc:1.000
[145000/211200] LG:3.185 LD:0.449 D:0.535 GP:0.006 AC: 0.336 RMSEAVG:0.013 NUM:0.043 SynTraiAuc:0.550 RFAcc:0.891
[147500/211200] LG:3.164 LD:0.566 D:0.606 GP:0.003 AC: 0.324 RMSEAVG:0.014 NUM:0.046 SynTraiAuc:0.550 RFAcc:0.998
[150000/211200] LG:3.094 LD:0.547 D:0.584 GP:0.002 AC: 0.331 RMSEAVG:0.013 NUM:0.051 SynTraiAuc:0.515 RFAcc:0.995
[152500/211200] LG:3.049 LD:0.513 D:0.554 GP:0.003 AC: 0.327 RMSEAVG:0.013 NUM:0.035 SynTraiAuc:0.515 RFAcc:0.956
[155000/211200] LG:3.088 LD:0.556 D:0.622 GP:0.004 AC: 0.352 RMSEAVG:0.015 NUM:0.055 SynTraiAuc:0.518 RFAcc:0.999
[157500/211200] LG:3.267 LD:0.564 D:0.624 GP:0.004 AC: 0.321 RMSEAVG:0.016 NUM:0.041 SynTraiAuc:0.518 RFAcc:0.994
[160000/211200] LG:3.363 LD:0.351 D:0.435 GP:0.006 AC: 0.330 RMSEAVG:0.017 NUM:0.039 SynTraiAuc:0.517 RFAcc:0.944
[162500/211200] LG:3.305 LD:0.458 D:0.542 GP:0.006 AC: 0.312 RMSEAVG:0.013 NUM:0.042 SynTraiAuc:0.517 RFAcc:0.996
[165000/211200] LG:3.403 LD:0.501 D:0.589 GP:0.006 AC: 0.338 RMSEAVG:0.017 NUM:0.043 SynTraiAuc:0.534 RFAcc:0.998
[167500/211200] LG:3.396 LD:0.589 D:0.620 GP:0.002 AC: 0.333 RMSEAVG:0.014 NUM:0.054 SynTraiAuc:0.534 RFAcc:0.951
[170000/211200] LG:3.419 LD:0.464 D:0.548 GP:0.006 AC: 0.331 RMSEAVG:0.014 NUM:0.033 SynTraiAuc:0.537 RFAcc:0.995
[172500/211200] LG:3.432 LD:0.499 D:0.558 GP:0.004 AC: 0.336 RMSEAVG:0.016 NUM:0.054 SynTraiAuc:0.537 RFAcc:0.975
[175000/211200] LG:3.461 LD:0.554 D:0.628 GP:0.005 AC: 0.332 RMSEAVG:0.014 NUM:0.040 SynTraiAuc:0.559 RFAcc:0.996
[177500/211200] LG:3.503 LD:0.508 D:0.585 GP:0.005 AC: 0.313 RMSEAVG:0.013 NUM:0.041 SynTraiAuc:0.559 RFAcc:0.998
[180000/211200] LG:3.451 LD:0.451 D:0.501 GP:0.003 AC: 0.333 RMSEAVG:0.012 NUM:0.034 SynTraiAuc:0.535 RFAcc:1.000
[182500/211200] LG:3.565 LD:0.657 D:0.697 GP:0.003 AC: 0.343 RMSEAVG:0.014 NUM:0.062 SynTraiAuc:0.535 RFAcc:0.996
[185000/211200] LG:3.542 LD:0.419 D:0.466 GP:0.003 AC: 0.334 RMSEAVG:0.013 NUM:0.045 SynTraiAuc:0.565 RFAcc:0.969
[187500/211200] LG:3.600 LD:0.454 D:0.537 GP:0.006 AC: 0.349 RMSEAVG:0.013 NUM:0.040 SynTraiAuc:0.565 RFAcc:0.996
[190000/211200] LG:3.697 LD:0.469 D:0.512 GP:0.003 AC: 0.339 RMSEAVG:0.016 NUM:0.058 SynTraiAuc:0.503 RFAcc:1.000
[192500/211200] LG:3.691 LD:0.465 D:0.544 GP:0.005 AC: 0.328 RMSEAVG:0.013 NUM:0.041 SynTraiAuc:0.503 RFAcc:0.998
[195000/211200] LG:3.630 LD:0.450 D:0.517 GP:0.005 AC: 0.334 RMSEAVG:0.013 NUM:0.047 SynTraiAuc:0.494 RFAcc:1.000
[197500/211200] LG:3.687 LD:0.423 D:0.481 GP:0.004 AC: 0.326 RMSEAVG:0.011 NUM:0.035 SynTraiAuc:0.494 RFAcc:0.964
[200000/211200] LG:3.817 LD:0.436 D:0.556 GP:0.008 AC: 0.348 RMSEAVG:0.016 NUM:0.047 SynTraiAuc:0.546 RFAcc:0.996
[202500/211200] LG:3.722 LD:0.612 D:0.676 GP:0.004 AC: 0.352 RMSEAVG:0.014 NUM:0.050 SynTraiAuc:0.546 RFAcc:0.975
[205000/211200] LG:3.763 LD:0.515 D:0.562 GP:0.003 AC: 0.316 RMSEAVG:0.013 NUM:0.053 SynTraiAuc:0.531 RFAcc:0.991
[207500/211200] LG:3.787 LD:0.451 D:0.494 GP:0.003 AC: 0.394 RMSEAVG:0.013 NUM:0.044 SynTraiAuc:0.531 RFAcc:0.991
[210000/211200] LG:3.756 LD:0.463 D:0.484 GP:0.001 AC: 0.340 RMSEAVG:0.012 NUM:0.043 SynTraiAuc:0.517 RFAcc:0.994
[211200/211200] LG:3.799 LD:0.156 D:0.184 GP:0.002 AC: 0.331 RMSEAVG:0.012 NUM:0.043 SynTraiAuc:0.517 RFAcc:0.994
INFO:root:Finished training after 211200/211200.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1880
AUC-ROC: 0.6137
AUC-PR: 0.3445

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2470
AUC-ROC: 0.6365
AUC-PR: 0.3493

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1831
AUC-ROC: 0.6497
AUC-PR: 0.3677

############# hmeq #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.9008 AUC: 0.9102 BCE: 0.2649
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 84 iters per epoch to reach target of 25200.
[ 2500/25200] LG:0.588 LD:0.236 D:0.289 GP:0.004 AC: 0.349 RMSEAVG:0.173 NUM:0.057 SynTraiAuc:0.673 RFAcc:0.874
[ 5000/25200] LG:0.647 LD:0.251 D:0.290 GP:0.003 AC: 0.367 RMSEAVG:0.086 NUM:0.046 SynTraiAuc:0.818 RFAcc:0.826
[ 7500/25200] LG:0.713 LD:0.414 D:0.438 GP:0.002 AC: 0.377 RMSEAVG:0.092 NUM:0.027 SynTraiAuc:0.818 RFAcc:0.839
[10000/25200] LG:0.744 LD:0.366 D:0.382 GP:0.001 AC: 0.345 RMSEAVG:0.021 NUM:0.017 SynTraiAuc:0.818 RFAcc:0.714
[12500/25200] LG:0.706 LD:0.370 D:0.384 GP:0.001 AC: 0.358 RMSEAVG:0.030 NUM:0.023 SynTraiAuc:0.818 RFAcc:0.680
[15000/25200] LG:0.734 LD:0.330 D:0.351 GP:0.001 AC: 0.317 RMSEAVG:0.027 NUM:0.023 SynTraiAuc:0.743 RFAcc:0.767
[17500/25200] LG:0.745 LD:0.335 D:0.351 GP:0.001 AC: 0.305 RMSEAVG:0.046 NUM:0.032 SynTraiAuc:0.743 RFAcc:0.764
[20000/25200] LG:0.709 LD:0.322 D:0.340 GP:0.001 AC: 0.323 RMSEAVG:0.027 NUM:0.016 SynTraiAuc:0.810 RFAcc:0.765
[22500/25200] LG:0.733 LD:0.319 D:0.339 GP:0.001 AC: 0.356 RMSEAVG:0.032 NUM:0.029 SynTraiAuc:0.810 RFAcc:0.759
[25000/25200] LG:0.752 LD:0.278 D:0.300 GP:0.001 AC: 0.321 RMSEAVG:0.030 NUM:0.020 SynTraiAuc:0.792 RFAcc:0.767
[25200/25200] LG:0.769 LD:0.233 D:0.247 GP:0.001 AC: 0.325 RMSEAVG:0.030 NUM:0.020 SynTraiAuc:0.792 RFAcc:0.767
INFO:root:Finished training after 25200/25200.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0535
AUC-ROC: 0.9759
AUC-PR: 0.9266

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2351
AUC-ROC: 0.8993
AUC-PR: 0.8062

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0732
AUC-ROC: 0.9108
AUC-PR: 0.8297

############# gmsc #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.9362 AUC: 0.8339 BCE: 0.1888
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 2110 iters per epoch to reach target of 633000.
[ 2500/633000] LG:-0.383 LD:-0.026 D:0.054 GP:0.005 AC: 0.558 RMSEAVG:0.039 NUM:0.039 SynTraiAuc:0.460 RFAcc:0.945
[ 5000/633000] LG:-0.298 LD:0.022 D:0.025 GP:0.000 AC: 0.463 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.669 RFAcc:0.950
[ 7500/633000] LG:-0.300 LD:0.002 D:0.007 GP:0.000 AC: 0.317 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.669 RFAcc:0.956
[10000/633000] LG:-0.246 LD:0.001 D:0.006 GP:0.000 AC: 0.318 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.749 RFAcc:0.949
[12500/633000] LG:-0.214 LD:0.006 D:0.009 GP:0.000 AC: 0.363 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.749 RFAcc:0.955
[15000/633000] LG:-0.178 LD:0.004 D:0.007 GP:0.000 AC: 0.357 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.775 RFAcc:0.912
[17500/633000] LG:-0.081 LD:-0.007 D:-0.001 GP:0.000 AC: 0.333 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.775 RFAcc:0.966
[20000/633000] LG:-0.030 LD:-0.010 D:-0.004 GP:0.000 AC: 0.395 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.642 RFAcc:0.907
[22500/633000] LG:0.030 LD:0.003 D:0.004 GP:0.000 AC: 0.425 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.642 RFAcc:0.912
[25000/633000] LG:0.051 LD:-0.005 D:-0.001 GP:0.000 AC: 0.717 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.679 RFAcc:0.939
[27500/633000] LG:-0.005 LD:-0.008 D:0.002 GP:0.001 AC: 0.312 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.679 RFAcc:0.924
[30000/633000] LG:0.049 LD:0.005 D:0.007 GP:0.000 AC: 0.773 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.523 RFAcc:0.929
[32500/633000] LG:0.026 LD:-0.022 D:-0.013 GP:0.001 AC: 0.452 RMSEAVG:0.026 NUM:0.026 SynTraiAuc:0.523 RFAcc:0.931
[35000/633000] LG:0.078 LD:-0.005 D:-0.001 GP:0.000 AC: 0.336 RMSEAVG:0.026 NUM:0.026 SynTraiAuc:0.571 RFAcc:0.912
[37500/633000] LG:-0.001 LD:-0.010 D:-0.003 GP:0.000 AC: 0.412 RMSEAVG:0.025 NUM:0.025 SynTraiAuc:0.571 RFAcc:0.901
[40000/633000] LG:0.054 LD:-0.014 D:-0.003 GP:0.001 AC: 0.513 RMSEAVG:0.029 NUM:0.029 SynTraiAuc:0.320 RFAcc:0.941
[42500/633000] LG:0.001 LD:-0.007 D:-0.005 GP:0.000 AC: 0.350 RMSEAVG:0.040 NUM:0.040 SynTraiAuc:0.320 RFAcc:0.949
[45000/633000] LG:0.050 LD:-0.005 D:-0.001 GP:0.000 AC: 0.460 RMSEAVG:0.048 NUM:0.048 SynTraiAuc:0.533 RFAcc:0.927
[47500/633000] LG:0.096 LD:-0.004 D:-0.000 GP:0.000 AC: 0.395 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.533 RFAcc:0.912
[50000/633000] LG:0.107 LD:-0.000 D:0.003 GP:0.000 AC: 0.369 RMSEAVG:0.027 NUM:0.027 SynTraiAuc:0.690 RFAcc:0.912
[52500/633000] LG:0.095 LD:-0.019 D:-0.014 GP:0.000 AC: 0.845 RMSEAVG:0.076 NUM:0.076 SynTraiAuc:0.690 RFAcc:0.929
[55000/633000] LG:0.191 LD:-0.005 D:-0.002 GP:0.000 AC: 0.777 RMSEAVG:0.035 NUM:0.035 SynTraiAuc:0.418 RFAcc:0.946
[57500/633000] LG:0.109 LD:-0.004 D:0.017 GP:0.001 AC: 0.379 RMSEAVG:0.074 NUM:0.074 SynTraiAuc:0.418 RFAcc:0.963
[60000/633000] LG:0.187 LD:-0.011 D:-0.003 GP:0.001 AC: 0.355 RMSEAVG:0.052 NUM:0.052 SynTraiAuc:0.621 RFAcc:0.895
[62500/633000] LG:0.144 LD:-0.006 D:-0.002 GP:0.000 AC: 0.390 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.621 RFAcc:0.915
[65000/633000] LG:0.140 LD:-0.022 D:-0.008 GP:0.001 AC: 0.431 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.720 RFAcc:0.988
[67500/633000] LG:0.302 LD:0.003 D:0.012 GP:0.001 AC: 0.311 RMSEAVG:0.052 NUM:0.052 SynTraiAuc:0.720 RFAcc:0.932
[70000/633000] LG:0.263 LD:0.008 D:0.024 GP:0.001 AC: 0.709 RMSEAVG:0.050 NUM:0.050 SynTraiAuc:0.406 RFAcc:0.935
[72500/633000] LG:0.135 LD:0.001 D:0.002 GP:0.000 AC: 0.433 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.406 RFAcc:0.904
[75000/633000] LG:0.036 LD:-0.020 D:-0.010 GP:0.001 AC: 0.684 RMSEAVG:0.026 NUM:0.026 SynTraiAuc:0.316 RFAcc:0.979
[77500/633000] LG:0.216 LD:0.004 D:0.008 GP:0.000 AC: 0.305 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.316 RFAcc:0.956
[80000/633000] LG:0.167 LD:-0.002 D:0.002 GP:0.000 AC: 0.338 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.759 RFAcc:0.980
[82500/633000] LG:0.193 LD:0.013 D:0.018 GP:0.000 AC: 0.369 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.759 RFAcc:0.951
[85000/633000] LG:0.160 LD:-0.007 D:0.000 GP:0.000 AC: 0.337 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.770 RFAcc:0.965
[87500/633000] LG:0.108 LD:-0.000 D:0.001 GP:0.000 AC: 0.333 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.770 RFAcc:0.954
[90000/633000] LG:0.094 LD:-0.004 D:0.001 GP:0.000 AC: 0.312 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.757 RFAcc:0.953
[92500/633000] LG:0.072 LD:-0.001 D:0.001 GP:0.000 AC: 0.302 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.757 RFAcc:0.929
[95000/633000] LG:0.165 LD:0.005 D:0.011 GP:0.000 AC: 0.353 RMSEAVG:0.119 NUM:0.119 SynTraiAuc:0.517 RFAcc:0.980
[97500/633000] LG:0.065 LD:-0.005 D:-0.001 GP:0.000 AC: 0.399 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.517 RFAcc:0.940
[100000/633000] LG:0.356 LD:0.029 D:0.031 GP:0.000 AC: 0.773 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.362 RFAcc:0.964
[102500/633000] LG:0.147 LD:-0.002 D:-0.000 GP:0.000 AC: 0.334 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.362 RFAcc:0.969
[105000/633000] LG:0.179 LD:0.019 D:0.027 GP:0.000 AC: 0.376 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.810 RFAcc:0.924
[107500/633000] LG:0.222 LD:-0.002 D:0.003 GP:0.000 AC: 0.343 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.810 RFAcc:0.935
[110000/633000] LG:0.161 LD:-0.001 D:0.000 GP:0.000 AC: 0.348 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.792 RFAcc:0.945
[112500/633000] LG:0.134 LD:-0.004 D:-0.001 GP:0.000 AC: 0.362 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.792 RFAcc:0.917
[115000/633000] LG:0.162 LD:-0.002 D:-0.001 GP:0.000 AC: 0.384 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.770 RFAcc:0.944
[117500/633000] LG:0.053 LD:-0.007 D:-0.002 GP:0.000 AC: 0.548 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.770 RFAcc:0.881
[120000/633000] LG:0.131 LD:0.001 D:0.002 GP:0.000 AC: 0.328 RMSEAVG:0.026 NUM:0.026 SynTraiAuc:0.777 RFAcc:0.971
[122500/633000] LG:0.173 LD:-0.003 D:0.005 GP:0.001 AC: 0.311 RMSEAVG:0.025 NUM:0.025 SynTraiAuc:0.777 RFAcc:0.934
[125000/633000] LG:-0.012 LD:-0.004 D:0.000 GP:0.000 AC: 0.311 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.768 RFAcc:0.953
[127500/633000] LG:-0.034 LD:0.002 D:0.003 GP:0.000 AC: 0.327 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.768 RFAcc:0.911
[130000/633000] LG:-0.057 LD:-0.002 D:0.000 GP:0.000 AC: 0.429 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.791 RFAcc:0.884
[132500/633000] LG:-0.041 LD:-0.002 D:0.000 GP:0.000 AC: 0.517 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.791 RFAcc:0.955
[135000/633000] LG:-0.110 LD:-0.003 D:-0.002 GP:0.000 AC: 0.328 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.634 RFAcc:0.974
[137500/633000] LG:-0.103 LD:-0.002 D:-0.000 GP:0.000 AC: 0.409 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.634 RFAcc:0.956
[140000/633000] LG:-0.087 LD:-0.002 D:-0.001 GP:0.000 AC: 0.364 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.809 RFAcc:0.935
[142500/633000] LG:-0.143 LD:0.023 D:0.025 GP:0.000 AC: 0.332 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.809 RFAcc:0.920
[145000/633000] LG:-0.141 LD:-0.001 D:-0.000 GP:0.000 AC: 0.324 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.796 RFAcc:0.934
[147500/633000] LG:-0.153 LD:-0.004 D:-0.001 GP:0.000 AC: 0.332 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.796 RFAcc:0.959
[150000/633000] LG:-0.159 LD:-0.000 D:0.001 GP:0.000 AC: 0.340 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.776 RFAcc:0.901
[152500/633000] LG:-0.178 LD:-0.007 D:0.000 GP:0.000 AC: 0.328 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.776 RFAcc:0.959
[155000/633000] LG:-0.263 LD:-0.002 D:0.000 GP:0.000 AC: 0.319 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.781 RFAcc:0.927
[157500/633000] LG:-0.275 LD:-0.001 D:0.001 GP:0.000 AC: 0.336 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.781 RFAcc:0.921
[160000/633000] LG:-0.293 LD:-0.004 D:0.001 GP:0.000 AC: 0.338 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.801 RFAcc:0.944
[162500/633000] LG:-0.323 LD:0.003 D:0.004 GP:0.000 AC: 0.311 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.801 RFAcc:0.956
[165000/633000] LG:-0.321 LD:0.024 D:0.025 GP:0.000 AC: 0.331 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.790 RFAcc:0.953
[167500/633000] LG:-0.389 LD:0.001 D:0.002 GP:0.000 AC: 0.300 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.790 RFAcc:0.954
[170000/633000] LG:-0.371 LD:-0.000 D:0.001 GP:0.000 AC: 0.316 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.730 RFAcc:0.960
[172500/633000] LG:-0.390 LD:0.022 D:0.026 GP:0.000 AC: 0.302 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.730 RFAcc:0.953
[175000/633000] LG:-0.384 LD:0.000 D:0.002 GP:0.000 AC: 0.313 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.768 RFAcc:0.925
[177500/633000] LG:-0.388 LD:0.002 D:0.003 GP:0.000 AC: 0.332 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.768 RFAcc:0.939
[180000/633000] LG:-0.421 LD:-0.003 D:-0.002 GP:0.000 AC: 0.315 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.782 RFAcc:0.917
[182500/633000] LG:-0.464 LD:0.000 D:0.002 GP:0.000 AC: 0.312 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.782 RFAcc:0.912
[185000/633000] LG:-0.489 LD:0.000 D:0.001 GP:0.000 AC: 0.313 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.722 RFAcc:0.959
[187500/633000] LG:-0.514 LD:0.001 D:0.002 GP:0.000 AC: 0.312 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.722 RFAcc:0.936
[190000/633000] LG:-0.558 LD:-0.001 D:-0.000 GP:0.000 AC: 0.300 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.748 RFAcc:0.926
[192500/633000] LG:-0.508 LD:-0.004 D:0.001 GP:0.000 AC: 0.334 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.748 RFAcc:0.930
[195000/633000] LG:-0.536 LD:-0.001 D:0.000 GP:0.000 AC: 0.309 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.799 RFAcc:0.968
[197500/633000] LG:-0.528 LD:-0.000 D:0.002 GP:0.000 AC: 0.304 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.799 RFAcc:0.914
[200000/633000] LG:-0.524 LD:0.001 D:0.002 GP:0.000 AC: 0.332 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.778 RFAcc:0.924
[202500/633000] LG:-0.582 LD:0.001 D:0.003 GP:0.000 AC: 0.303 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.778 RFAcc:0.970
[205000/633000] LG:-0.589 LD:-0.002 D:0.001 GP:0.000 AC: 0.323 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.758 RFAcc:0.910
[207500/633000] LG:-0.574 LD:0.001 D:0.002 GP:0.000 AC: 0.313 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.758 RFAcc:0.945
[210000/633000] LG:-0.559 LD:-0.005 D:-0.000 GP:0.000 AC: 0.317 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.759 RFAcc:0.963
[212500/633000] LG:-0.536 LD:0.027 D:0.030 GP:0.000 AC: 0.317 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.759 RFAcc:0.954
[215000/633000] LG:-0.569 LD:0.001 D:0.003 GP:0.000 AC: 0.311 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.751 RFAcc:0.951
[217500/633000] LG:-0.607 LD:0.002 D:0.004 GP:0.000 AC: 0.301 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.751 RFAcc:0.958
[220000/633000] LG:-0.579 LD:0.002 D:0.002 GP:0.000 AC: 0.313 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.693 RFAcc:0.940
[222500/633000] LG:-0.590 LD:0.001 D:0.002 GP:0.000 AC: 0.317 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.693 RFAcc:0.934
[225000/633000] LG:-0.574 LD:0.001 D:0.004 GP:0.000 AC: 0.308 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.788 RFAcc:0.916
[227500/633000] LG:-0.622 LD:0.027 D:0.028 GP:0.000 AC: 0.306 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.788 RFAcc:0.981
[230000/633000] LG:-0.563 LD:-0.000 D:0.000 GP:0.000 AC: 0.312 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.750 RFAcc:0.931
[232500/633000] LG:-0.566 LD:-0.001 D:-0.001 GP:0.000 AC: 0.310 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.750 RFAcc:0.943
[235000/633000] LG:-0.502 LD:-0.000 D:0.001 GP:0.000 AC: 0.330 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.756 RFAcc:0.921
[237500/633000] LG:-0.538 LD:-0.004 D:-0.000 GP:0.000 AC: 0.315 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.756 RFAcc:0.916
[240000/633000] LG:-0.541 LD:-0.001 D:0.000 GP:0.000 AC: 0.320 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.717 RFAcc:0.916
[242500/633000] LG:-0.536 LD:-0.001 D:-0.001 GP:0.000 AC: 0.327 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.717 RFAcc:0.951
[245000/633000] LG:-0.531 LD:-0.005 D:-0.001 GP:0.000 AC: 0.324 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.766 RFAcc:0.901
[247500/633000] LG:-0.517 LD:-0.001 D:0.000 GP:0.000 AC: 0.336 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.766 RFAcc:0.882
[250000/633000] LG:-0.573 LD:0.020 D:0.024 GP:0.000 AC: 0.315 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.749 RFAcc:0.926
[252500/633000] LG:-0.553 LD:-0.000 D:0.002 GP:0.000 AC: 0.326 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.749 RFAcc:0.920
[255000/633000] LG:-0.473 LD:0.009 D:0.013 GP:0.000 AC: 0.310 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.592 RFAcc:0.986
[257500/633000] LG:-0.563 LD:-0.001 D:-0.000 GP:0.000 AC: 0.309 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.592 RFAcc:0.965
[260000/633000] LG:-0.640 LD:-0.001 D:0.004 GP:0.000 AC: 0.303 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.709 RFAcc:0.934
[262500/633000] LG:-0.670 LD:-0.002 D:0.000 GP:0.000 AC: 0.332 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.709 RFAcc:0.965
[265000/633000] LG:-0.718 LD:0.001 D:0.004 GP:0.000 AC: 0.310 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.720 RFAcc:0.924
[267500/633000] LG:-0.738 LD:-0.002 D:-0.001 GP:0.000 AC: 0.310 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.720 RFAcc:0.932
[270000/633000] LG:-0.713 LD:0.000 D:0.001 GP:0.000 AC: 0.324 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.738 RFAcc:0.916
[272500/633000] LG:-0.769 LD:-0.011 D:0.001 GP:0.001 AC: 0.307 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.738 RFAcc:0.896
[275000/633000] LG:-0.743 LD:0.025 D:0.026 GP:0.000 AC: 0.320 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.757 RFAcc:0.925
[277500/633000] LG:-0.814 LD:-0.002 D:0.001 GP:0.000 AC: 0.314 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.757 RFAcc:0.944
[280000/633000] LG:-0.820 LD:0.001 D:0.002 GP:0.000 AC: 0.314 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.728 RFAcc:0.907
[282500/633000] LG:-0.704 LD:0.019 D:0.021 GP:0.000 AC: 0.318 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.728 RFAcc:0.920
[285000/633000] LG:-0.808 LD:-0.003 D:-0.001 GP:0.000 AC: 0.311 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.788 RFAcc:0.963
[287500/633000] LG:-0.782 LD:0.000 D:0.001 GP:0.000 AC: 0.305 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.788 RFAcc:0.919
[290000/633000] LG:-0.807 LD:-0.007 D:0.001 GP:0.001 AC: 0.307 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.746 RFAcc:0.944
[292500/633000] LG:-0.763 LD:-0.002 D:0.001 GP:0.000 AC: 0.303 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.746 RFAcc:0.921
[295000/633000] LG:-0.772 LD:0.001 D:0.003 GP:0.000 AC: 0.312 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.719 RFAcc:0.922
[297500/633000] LG:-0.745 LD:0.000 D:0.002 GP:0.000 AC: 0.310 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.719 RFAcc:0.853
[300000/633000] LG:-0.723 LD:0.002 D:0.003 GP:0.000 AC: 0.307 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.755 RFAcc:0.906
[302500/633000] LG:-0.710 LD:0.001 D:0.002 GP:0.000 AC: 0.311 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.755 RFAcc:0.956
[305000/633000] LG:-0.673 LD:-0.001 D:-0.001 GP:0.000 AC: 0.320 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.753 RFAcc:0.916
[307500/633000] LG:-0.749 LD:-0.003 D:-0.002 GP:0.000 AC: 0.304 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.753 RFAcc:0.869
[310000/633000] LG:-0.673 LD:0.003 D:0.004 GP:0.000 AC: 0.312 RMSEAVG:0.027 NUM:0.027 SynTraiAuc:0.795 RFAcc:0.935
[312500/633000] LG:-0.766 LD:-0.002 D:-0.001 GP:0.000 AC: 0.305 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.795 RFAcc:0.925
[315000/633000] LG:-0.727 LD:0.001 D:0.002 GP:0.000 AC: 0.314 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.736 RFAcc:0.904
[317500/633000] LG:-0.672 LD:-0.002 D:-0.000 GP:0.000 AC: 0.319 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.736 RFAcc:0.889
[320000/633000] LG:-0.732 LD:-0.010 D:0.021 GP:0.002 AC: 0.304 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.722 RFAcc:0.961
[322500/633000] LG:-0.740 LD:-0.002 D:0.002 GP:0.000 AC: 0.307 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.722 RFAcc:0.950
[325000/633000] LG:-0.696 LD:-0.001 D:0.003 GP:0.000 AC: 0.315 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.770 RFAcc:0.892
[327500/633000] LG:-0.673 LD:0.002 D:0.002 GP:0.000 AC: 0.322 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.770 RFAcc:0.950
[330000/633000] LG:-0.743 LD:0.000 D:0.001 GP:0.000 AC: 0.306 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.779 RFAcc:0.945
[332500/633000] LG:-0.691 LD:0.029 D:0.029 GP:0.000 AC: 0.315 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.779 RFAcc:0.951
[335000/633000] LG:-0.686 LD:-0.006 D:0.002 GP:0.001 AC: 0.311 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.758 RFAcc:0.945
[337500/633000] LG:-0.716 LD:0.000 D:0.003 GP:0.000 AC: 0.302 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.758 RFAcc:0.948
[340000/633000] LG:-0.642 LD:0.006 D:0.007 GP:0.000 AC: 0.308 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.763 RFAcc:0.946
[342500/633000] LG:-0.587 LD:-0.000 D:0.000 GP:0.000 AC: 0.318 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.763 RFAcc:0.935
[345000/633000] LG:-0.663 LD:0.002 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.782 RFAcc:0.931
[347500/633000] LG:-0.638 LD:0.005 D:0.006 GP:0.000 AC: 0.311 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.782 RFAcc:0.829
[350000/633000] LG:-0.608 LD:0.000 D:0.001 GP:0.000 AC: 0.316 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.773 RFAcc:0.940
[352500/633000] LG:-0.574 LD:-0.003 D:-0.003 GP:0.000 AC: 0.309 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.773 RFAcc:0.948
[355000/633000] LG:-0.594 LD:-0.003 D:-0.000 GP:0.000 AC: 0.323 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.792 RFAcc:0.910
[357500/633000] LG:-0.629 LD:0.025 D:0.028 GP:0.000 AC: 0.304 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.792 RFAcc:0.936
[360000/633000] LG:-0.638 LD:0.002 D:0.003 GP:0.000 AC: 0.309 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.659 RFAcc:0.996
[362500/633000] LG:-0.623 LD:0.001 D:0.001 GP:0.000 AC: 0.309 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.659 RFAcc:0.944
[365000/633000] LG:-0.564 LD:-0.007 D:-0.003 GP:0.000 AC: 0.322 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.725 RFAcc:0.959
[367500/633000] LG:-0.609 LD:0.002 D:0.004 GP:0.000 AC: 0.314 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.725 RFAcc:0.948
[370000/633000] LG:-0.559 LD:0.001 D:0.002 GP:0.000 AC: 0.317 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.771 RFAcc:0.943
[372500/633000] LG:-0.604 LD:0.002 D:0.003 GP:0.000 AC: 0.311 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.771 RFAcc:0.886
[375000/633000] LG:-0.614 LD:-0.000 D:0.001 GP:0.000 AC: 0.309 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.763 RFAcc:0.914
[377500/633000] LG:-1.167 LD:-0.003 D:0.003 GP:0.000 AC: 0.306 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.763 RFAcc:0.944
[380000/633000] LG:-1.268 LD:0.019 D:0.028 GP:0.001 AC: 0.304 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.746 RFAcc:0.954
[382500/633000] LG:-1.259 LD:0.001 D:0.002 GP:0.000 AC: 0.304 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.746 RFAcc:0.974
[385000/633000] LG:-1.298 LD:0.002 D:0.002 GP:0.000 AC: 0.302 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.717 RFAcc:0.890
[387500/633000] LG:-1.142 LD:-0.001 D:0.003 GP:0.000 AC: 0.310 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.717 RFAcc:0.961
[390000/633000] LG:-1.106 LD:0.000 D:0.001 GP:0.000 AC: 0.311 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.797 RFAcc:0.956
[392500/633000] LG:-1.337 LD:0.001 D:0.002 GP:0.000 AC: 0.301 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.797 RFAcc:0.874
[395000/633000] LG:-1.273 LD:0.015 D:0.020 GP:0.000 AC: 0.304 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.741 RFAcc:0.911
[397500/633000] LG:-1.297 LD:0.001 D:0.002 GP:0.000 AC: 0.302 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.741 RFAcc:0.956
[400000/633000] LG:-1.286 LD:0.002 D:0.002 GP:0.000 AC: 0.307 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.760 RFAcc:0.958
[402500/633000] LG:-1.259 LD:0.003 D:0.004 GP:0.000 AC: 0.305 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.760 RFAcc:0.961
[405000/633000] LG:-1.162 LD:-0.001 D:0.004 GP:0.000 AC: 0.304 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.730 RFAcc:0.934
[407500/633000] LG:-1.175 LD:-0.003 D:-0.001 GP:0.000 AC: 0.307 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.730 RFAcc:0.936
[410000/633000] LG:-1.349 LD:-0.000 D:-0.000 GP:0.000 AC: 0.300 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.712 RFAcc:0.912
[412500/633000] LG:-1.323 LD:0.001 D:0.002 GP:0.000 AC: 0.304 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.712 RFAcc:0.906
[415000/633000] LG:-1.302 LD:-0.000 D:0.002 GP:0.000 AC: 0.302 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.701 RFAcc:0.955
[417500/633000] LG:-1.260 LD:0.003 D:0.003 GP:0.000 AC: 0.304 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.701 RFAcc:0.960
[420000/633000] LG:-1.151 LD:0.004 D:0.004 GP:0.000 AC: 0.308 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.674 RFAcc:0.934
[422500/633000] LG:-1.243 LD:-0.000 D:0.001 GP:0.000 AC: 0.306 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.674 RFAcc:0.955
[425000/633000] LG:-1.189 LD:-0.002 D:-0.000 GP:0.000 AC: 0.305 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.721 RFAcc:0.965
[427500/633000] LG:-1.234 LD:0.003 D:0.003 GP:0.000 AC: 0.305 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.721 RFAcc:0.927
[430000/633000] LG:-1.249 LD:-0.001 D:0.002 GP:0.000 AC: 0.302 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.757 RFAcc:0.944
[432500/633000] LG:-1.310 LD:-0.003 D:-0.001 GP:0.000 AC: 0.305 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.757 RFAcc:0.945
[435000/633000] LG:-1.327 LD:-0.001 D:-0.000 GP:0.000 AC: 0.300 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.733 RFAcc:0.948
[437500/633000] LG:-1.169 LD:0.002 D:0.003 GP:0.000 AC: 0.305 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.733 RFAcc:0.940
[440000/633000] LG:-1.158 LD:-0.002 D:-0.001 GP:0.000 AC: 0.308 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.722 RFAcc:0.860
[442500/633000] LG:-1.315 LD:0.005 D:0.005 GP:0.000 AC: 0.310 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.722 RFAcc:0.948
[445000/633000] LG:-1.301 LD:0.001 D:0.001 GP:0.000 AC: 0.304 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.708 RFAcc:0.955
[447500/633000] LG:-1.336 LD:0.001 D:0.002 GP:0.000 AC: 0.302 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.708 RFAcc:0.920
[450000/633000] LG:-1.336 LD:0.001 D:0.001 GP:0.000 AC: 0.302 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.731 RFAcc:0.948
[452500/633000] LG:-1.239 LD:-0.000 D:0.004 GP:0.000 AC: 0.306 RMSEAVG:0.025 NUM:0.025 SynTraiAuc:0.731 RFAcc:0.915
[455000/633000] LG:-1.370 LD:-0.002 D:-0.001 GP:0.000 AC: 0.305 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.699 RFAcc:0.912
[457500/633000] LG:-1.145 LD:0.007 D:0.008 GP:0.000 AC: 0.313 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.699 RFAcc:0.915
[460000/633000] LG:-1.259 LD:0.001 D:0.002 GP:0.000 AC: 0.308 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.725 RFAcc:0.979
[462500/633000] LG:-1.280 LD:0.000 D:0.000 GP:0.000 AC: 0.304 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.725 RFAcc:0.958
[465000/633000] LG:-1.257 LD:0.003 D:0.003 GP:0.000 AC: 0.306 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.738 RFAcc:0.925
[467500/633000] LG:-1.264 LD:0.003 D:0.003 GP:0.000 AC: 0.304 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.738 RFAcc:0.975
[470000/633000] LG:-1.300 LD:0.024 D:0.026 GP:0.000 AC: 0.305 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.684 RFAcc:0.944
[472500/633000] LG:-1.296 LD:0.003 D:0.004 GP:0.000 AC: 0.307 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.684 RFAcc:0.935
[475000/633000] LG:-1.338 LD:0.017 D:0.023 GP:0.000 AC: 0.305 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.725 RFAcc:0.924
[477500/633000] LG:-1.337 LD:0.000 D:0.001 GP:0.000 AC: 0.306 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.725 RFAcc:0.969
[480000/633000] LG:-1.410 LD:0.001 D:0.001 GP:0.000 AC: 0.301 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.692 RFAcc:0.949
[482500/633000] LG:-1.367 LD:0.004 D:0.004 GP:0.000 AC: 0.303 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.692 RFAcc:0.946
[485000/633000] LG:-1.338 LD:-0.001 D:-0.000 GP:0.000 AC: 0.304 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.706 RFAcc:0.969
[487500/633000] LG:-1.255 LD:0.003 D:0.004 GP:0.000 AC: 0.312 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.706 RFAcc:0.974
[490000/633000] LG:-1.330 LD:0.000 D:0.001 GP:0.000 AC: 0.307 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.733 RFAcc:0.877
[492500/633000] LG:-1.363 LD:0.003 D:0.003 GP:0.000 AC: 0.302 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.733 RFAcc:0.909
[495000/633000] LG:-1.202 LD:0.004 D:0.005 GP:0.000 AC: 0.307 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.651 RFAcc:0.983
[497500/633000] LG:-1.421 LD:0.001 D:0.002 GP:0.000 AC: 0.301 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.651 RFAcc:0.948
[500000/633000] LG:-1.285 LD:0.002 D:0.004 GP:0.000 AC: 0.303 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.716 RFAcc:0.940
[502500/633000] LG:-1.371 LD:-0.000 D:0.000 GP:0.000 AC: 0.305 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.716 RFAcc:0.889
[505000/633000] LG:-1.218 LD:0.002 D:0.002 GP:0.000 AC: 0.304 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.713 RFAcc:0.943
[507500/633000] LG:-1.126 LD:0.002 D:0.002 GP:0.000 AC: 0.314 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.713 RFAcc:0.921
[510000/633000] LG:-1.282 LD:0.003 D:0.004 GP:0.000 AC: 0.309 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.777 RFAcc:0.934
[512500/633000] LG:-1.311 LD:0.003 D:0.003 GP:0.000 AC: 0.307 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.777 RFAcc:0.938
[515000/633000] LG:-1.375 LD:0.005 D:0.005 GP:0.000 AC: 0.301 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.750 RFAcc:0.951
[517500/633000] LG:-1.161 LD:0.027 D:0.027 GP:0.000 AC: 0.307 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.750 RFAcc:0.914
[520000/633000] LG:-1.381 LD:0.001 D:0.001 GP:0.000 AC: 0.303 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.733 RFAcc:0.986
[522500/633000] LG:-1.261 LD:0.005 D:0.005 GP:0.000 AC: 0.305 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.733 RFAcc:0.926
[525000/633000] LG:-1.284 LD:0.004 D:0.004 GP:0.000 AC: 0.304 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.728 RFAcc:0.966
[527500/633000] LG:-1.306 LD:0.004 D:0.005 GP:0.000 AC: 0.304 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.728 RFAcc:0.968
[530000/633000] LG:-1.335 LD:0.027 D:0.031 GP:0.000 AC: 0.304 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.732 RFAcc:0.932
[532500/633000] LG:-1.268 LD:0.005 D:0.005 GP:0.000 AC: 0.303 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.732 RFAcc:0.945
[535000/633000] LG:-1.306 LD:0.004 D:0.004 GP:0.000 AC: 0.310 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.729 RFAcc:0.931
[537500/633000] LG:-1.167 LD:0.006 D:0.006 GP:0.000 AC: 0.314 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.729 RFAcc:0.941
[540000/633000] LG:-1.347 LD:0.002 D:0.002 GP:0.000 AC: 0.303 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.739 RFAcc:0.896
[542500/633000] LG:-1.272 LD:0.004 D:0.006 GP:0.000 AC: 0.341 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.739 RFAcc:0.941
[545000/633000] LG:-1.384 LD:0.002 D:0.020 GP:0.001 AC: 0.302 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.755 RFAcc:0.973
[547500/633000] LG:-1.308 LD:0.004 D:0.004 GP:0.000 AC: 0.306 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.755 RFAcc:0.941
[550000/633000] LG:-1.308 LD:0.004 D:0.004 GP:0.000 AC: 0.306 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.712 RFAcc:0.960
[552500/633000] LG:-1.283 LD:0.006 D:0.006 GP:0.000 AC: 0.304 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.712 RFAcc:0.949
[555000/633000] LG:-1.231 LD:0.001 D:0.003 GP:0.000 AC: 0.305 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.697 RFAcc:0.979
[557500/633000] LG:-1.259 LD:0.002 D:0.003 GP:0.000 AC: 0.307 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.697 RFAcc:0.931
[560000/633000] LG:-1.230 LD:0.000 D:0.001 GP:0.000 AC: 0.310 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.756 RFAcc:0.981
[562500/633000] LG:-1.177 LD:0.001 D:0.003 GP:0.000 AC: 0.312 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.756 RFAcc:0.953
[565000/633000] LG:-1.260 LD:0.001 D:0.002 GP:0.000 AC: 0.303 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.696 RFAcc:0.970
[567500/633000] LG:-1.225 LD:0.002 D:0.002 GP:0.000 AC: 0.311 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.696 RFAcc:0.984
[570000/633000] LG:-1.221 LD:0.006 D:0.006 GP:0.000 AC: 0.305 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.741 RFAcc:0.931
[572500/633000] LG:-1.299 LD:-0.001 D:-0.001 GP:0.000 AC: 0.304 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.741 RFAcc:0.954
[575000/633000] LG:-1.280 LD:0.002 D:0.004 GP:0.000 AC: 0.307 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.783 RFAcc:0.935
[577500/633000] LG:-1.350 LD:-0.008 D:0.001 GP:0.001 AC: 0.300 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.783 RFAcc:0.895
[580000/633000] LG:-1.292 LD:-0.001 D:0.000 GP:0.000 AC: 0.303 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.700 RFAcc:0.944
[582500/633000] LG:-1.296 LD:0.001 D:0.002 GP:0.000 AC: 0.303 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.700 RFAcc:0.910
[585000/633000] LG:-1.320 LD:0.001 D:0.002 GP:0.000 AC: 0.302 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.739 RFAcc:0.949
[587500/633000] LG:-1.283 LD:0.001 D:0.001 GP:0.000 AC: 0.305 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.739 RFAcc:0.949
[590000/633000] LG:-1.357 LD:-0.003 D:0.002 GP:0.000 AC: 0.301 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.732 RFAcc:0.943
[592500/633000] LG:-1.310 LD:0.005 D:0.006 GP:0.000 AC: 0.303 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.732 RFAcc:0.938
[595000/633000] LG:-1.265 LD:0.004 D:0.004 GP:0.000 AC: 0.307 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.702 RFAcc:0.968
[597500/633000] LG:-1.252 LD:0.002 D:0.003 GP:0.000 AC: 0.308 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.702 RFAcc:0.924
[600000/633000] LG:-1.274 LD:0.001 D:0.002 GP:0.000 AC: 0.307 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.773 RFAcc:0.978
[602500/633000] LG:-1.298 LD:0.000 D:0.001 GP:0.000 AC: 0.303 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.773 RFAcc:0.984
[605000/633000] LG:-1.244 LD:0.003 D:0.004 GP:0.000 AC: 0.310 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.700 RFAcc:0.956
[607500/633000] LG:-1.297 LD:0.002 D:0.002 GP:0.000 AC: 0.301 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.700 RFAcc:0.912
[610000/633000] LG:-1.204 LD:0.026 D:0.029 GP:0.000 AC: 0.306 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.754 RFAcc:0.973
[612500/633000] LG:-1.208 LD:0.001 D:0.004 GP:0.000 AC: 0.308 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.754 RFAcc:0.910
[615000/633000] LG:-1.290 LD:-0.000 D:0.002 GP:0.000 AC: 0.303 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.732 RFAcc:0.914
[617500/633000] LG:-1.227 LD:0.032 D:0.033 GP:0.000 AC: 0.306 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.732 RFAcc:0.950
[620000/633000] LG:-1.260 LD:0.004 D:0.004 GP:0.000 AC: 0.302 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.757 RFAcc:0.940
[622500/633000] LG:-1.231 LD:0.002 D:0.002 GP:0.000 AC: 0.306 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.757 RFAcc:0.922
[625000/633000] LG:-1.247 LD:0.003 D:0.003 GP:0.000 AC: 0.305 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.731 RFAcc:0.931
[627500/633000] LG:-1.239 LD:0.003 D:0.004 GP:0.000 AC: 0.305 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.731 RFAcc:0.963
[630000/633000] LG:-1.161 LD:-0.002 D:-0.001 GP:0.000 AC: 0.311 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.732 RFAcc:0.891
[632500/633000] LG:-1.145 LD:0.004 D:0.005 GP:0.000 AC: 0.311 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.732 RFAcc:0.961
[633000/633000] LG:-1.227 LD:0.002 D:0.002 GP:0.000 AC: 0.305 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.732 RFAcc:0.961
INFO:root:Finished training after 633000/633000.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.0515
AUC-ROC: 0.8308
AUC-PR: 0.3607

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2349
AUC-ROC: 0.8220
AUC-PR: 0.3551

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.0508
AUC-ROC: 0.8286
AUC-PR: 0.3751

###########################################
###########################################
################# TABDDPM #################
###########################################
###########################################
[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 800 samples, Labels: {0: 559, 1: 241}
- Generated data: 1055 samples, Labels: {0: 740, 1: 315}
- Combined training data: 1115 samples, Labels: {0: 559, 1: 556}
Training set merged, preprocessed, and saved in Merged/uci_german.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1599
AUC-ROC: 0.8085
AUC-PR: 0.6587

Results for Random Forest:
 Brier-Score: 0.1599, AUC-ROC: 0.8085, AUC-PR: 0.6587

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2438
AUC-ROC: 0.8303
AUC-PR: 0.6207

Results for AdaBoost:
 Brier-Score: 0.2438, AUC-ROC: 0.8303, AUC-PR: 0.6207

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1540
AUC-ROC: 0.8071
AUC-PR: 0.6393

Results for Gradient Boosting:
 Brier-Score: 0.1540, AUC-ROC: 0.8071, AUC-PR: 0.6393

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 800 samples, Labels: {0: 559, 1: 241}
- Generated data: 1055 samples, Labels: {0: 752, 1: 303}
- Combined training data: 1103 samples, Labels: {0: 559, 1: 544}
Training set merged, preprocessed, and saved in Merged/uci_german.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1617
AUC-ROC: 0.7920
AUC-PR: 0.6468

Results for Random Forest:
 Brier-Score: 0.1617, AUC-ROC: 0.7920, AUC-PR: 0.6468

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2431
AUC-ROC: 0.7980
AUC-PR: 0.6219

Results for AdaBoost:
 Brier-Score: 0.2431, AUC-ROC: 0.7980, AUC-PR: 0.6219

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1585
AUC-ROC: 0.8114
AUC-PR: 0.6217

Results for Gradient Boosting:
 Brier-Score: 0.1585, AUC-ROC: 0.8114, AUC-PR: 0.6217

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 24000 samples, Labels: {0: 18677, 1: 5323}
- Generated data: 60209 samples, Labels: {0: 46883, 1: 13326}
- Combined training data: 37326 samples, Labels: {0: 18677, 1: 18649}
Training set merged, preprocessed, and saved in Merged/uci_taiwan.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1390
AUC-ROC: 0.7589
AUC-PR: 0.5219

Results for Random Forest:
 Brier-Score: 0.1390, AUC-ROC: 0.7589, AUC-PR: 0.5219

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2437
AUC-ROC: 0.7738
AUC-PR: 0.5269

Results for AdaBoost:
 Brier-Score: 0.2437, AUC-ROC: 0.7738, AUC-PR: 0.5269

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1350
AUC-ROC: 0.7776
AUC-PR: 0.5373

Results for Gradient Boosting:
 Brier-Score: 0.1350, AUC-ROC: 0.7776, AUC-PR: 0.5373

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 24000 samples, Labels: {0: 18677, 1: 5323}
- Generated data: 60209 samples, Labels: {0: 46709, 1: 13500}
- Combined training data: 37500 samples, Labels: {1: 18823, 0: 18677}
Training set merged, preprocessed, and saved in Merged/uci_taiwan.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1400
AUC-ROC: 0.7539
AUC-PR: 0.5202

Results for Random Forest:
 Brier-Score: 0.1400, AUC-ROC: 0.7539, AUC-PR: 0.5202

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2438
AUC-ROC: 0.7775
AUC-PR: 0.5280

Results for AdaBoost:
 Brier-Score: 0.2438, AUC-ROC: 0.7775, AUC-PR: 0.5280

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1353
AUC-ROC: 0.7766
AUC-PR: 0.5372

Results for Gradient Boosting:
 Brier-Score: 0.1353, AUC-ROC: 0.7766, AUC-PR: 0.5372

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 40000 samples, Labels: {0: 29623, 1: 10377}
- Generated data: 74187 samples, Labels: {0: 54961, 1: 19226}
- Combined training data: 59226 samples, Labels: {0: 29623, 1: 29603}
Training set merged, preprocessed, and saved in Merged/pakdd.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1928
AUC-ROC: 0.6022
AUC-PR: 0.3408

Results for Random Forest:
 Brier-Score: 0.1928, AUC-ROC: 0.6022, AUC-PR: 0.3408

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2471
AUC-ROC: 0.6211
AUC-PR: 0.3516

Results for AdaBoost:
 Brier-Score: 0.2471, AUC-ROC: 0.6211, AUC-PR: 0.3516

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1877
AUC-ROC: 0.6301
AUC-PR: 0.3692

Results for Gradient Boosting:
 Brier-Score: 0.1877, AUC-ROC: 0.6301, AUC-PR: 0.3692

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 40000 samples, Labels: {0: 29623, 1: 10377}
- Generated data: 74187 samples, Labels: {0: 54910, 1: 19277}
- Combined training data: 59277 samples, Labels: {1: 29654, 0: 29623}
Training set merged, preprocessed, and saved in Merged/pakdd.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1927
AUC-ROC: 0.6020
AUC-PR: 0.3419

Results for Random Forest:
 Brier-Score: 0.1927, AUC-ROC: 0.6020, AUC-PR: 0.3419

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2471
AUC-ROC: 0.6131
AUC-PR: 0.3417

Results for AdaBoost:
 Brier-Score: 0.2471, AUC-ROC: 0.6131, AUC-PR: 0.3417

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1882
AUC-ROC: 0.6259
AUC-PR: 0.3593

Results for Gradient Boosting:
 Brier-Score: 0.1882, AUC-ROC: 0.6259, AUC-PR: 0.3593

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 4768 samples, Labels: {0: 3844, 1: 924}
- Generated data: 15067 samples, Labels: {0: 12232, 1: 2835}
- Combined training data: 7603 samples, Labels: {0: 3844, 1: 3759}
Training set merged, preprocessed, and saved in Merged/hmeq.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0665
AUC-ROC: 0.9639
AUC-PR: 0.8855

Results for Random Forest:
 Brier-Score: 0.0665, AUC-ROC: 0.9639, AUC-PR: 0.8855

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2330
AUC-ROC: 0.9134
AUC-PR: 0.8210

Results for AdaBoost:
 Brier-Score: 0.2330, AUC-ROC: 0.9134, AUC-PR: 0.8210

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0761
AUC-ROC: 0.9326
AUC-PR: 0.8495

Results for Gradient Boosting:
 Brier-Score: 0.0761, AUC-ROC: 0.9326, AUC-PR: 0.8495

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 4768 samples, Labels: {0: 3844, 1: 924}
- Generated data: 15067 samples, Labels: {0: 12301, 1: 2766}
- Combined training data: 7534 samples, Labels: {0: 3844, 1: 3690}
Training set merged, preprocessed, and saved in Merged/hmeq.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0663
AUC-ROC: 0.9646
AUC-PR: 0.8813

Results for Random Forest:
 Brier-Score: 0.0663, AUC-ROC: 0.9646, AUC-PR: 0.8813

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2341
AUC-ROC: 0.8992
AUC-PR: 0.7948

Results for AdaBoost:
 Brier-Score: 0.2341, AUC-ROC: 0.8992, AUC-PR: 0.7948

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0787
AUC-ROC: 0.9296
AUC-PR: 0.8412

Results for Gradient Boosting:
 Brier-Score: 0.0787, AUC-ROC: 0.9296, AUC-PR: 0.8412

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 120000 samples, Labels: {0: 111930, 1: 8070}
- Generated data: 1544386 samples, Labels: {0: 1441031, 1: 103355}
- Combined training data: 223355 samples, Labels: {0: 111930, 1: 111425}
Training set merged, preprocessed, and saved in Merged/gmsc.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.1309
AUC-ROC: 0.7829
AUC-PR: 0.2488

Results for Random Forest:
 Brier-Score: 0.1309, AUC-ROC: 0.7829, AUC-PR: 0.2488

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2450
AUC-ROC: 0.8188
AUC-PR: 0.3363

Results for AdaBoost:
 Brier-Score: 0.2450, AUC-ROC: 0.8188, AUC-PR: 0.3363

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.1166
AUC-ROC: 0.8273
AUC-PR: 0.3347

Results for Gradient Boosting:
 Brier-Score: 0.1166, AUC-ROC: 0.8273, AUC-PR: 0.3347

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 120000 samples, Labels: {0: 111930, 1: 8070}
- Generated data: 1544386 samples, Labels: {0: 1441816, 1: 102570}
- Combined training data: 222570 samples, Labels: {0: 111930, 1: 110640}
Training set merged, preprocessed, and saved in Merged/gmsc.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.0536
AUC-ROC: 0.7950
AUC-PR: 0.3180

Results for Random Forest:
 Brier-Score: 0.0536, AUC-ROC: 0.7950, AUC-PR: 0.3180

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2356
AUC-ROC: 0.8142
AUC-PR: 0.3280

Results for AdaBoost:
 Brier-Score: 0.2356, AUC-ROC: 0.8142, AUC-PR: 0.3280

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.0500
AUC-ROC: 0.8269
AUC-PR: 0.3552

Results for Gradient Boosting:
 Brier-Score: 0.0500, AUC-ROC: 0.8269, AUC-PR: 0.3552

Complete Results DataFrame:
        dataset resample_method         classifier       metric     value
0    uci_german             ros      Random Forest  Brier-Score  0.156852
1    uci_german             ros      Random Forest      AUC-ROC  0.785603
2    uci_german             ros      Random Forest       AUC-PR  0.563668
3    uci_german             ros           AdaBoost  Brier-Score  0.241799
4    uci_german             ros           AdaBoost      AUC-ROC  0.762474
..          ...             ...                ...          ...       ...
625        gmsc     tabddpm_bgm           AdaBoost      AUC-ROC  0.814214
626        gmsc     tabddpm_bgm           AdaBoost       AUC-PR  0.327993
627        gmsc     tabddpm_bgm  Gradient Boosting  Brier-Score  0.049964
628        gmsc     tabddpm_bgm  Gradient Boosting      AUC-ROC  0.826865
629        gmsc     tabddpm_bgm  Gradient Boosting       AUC-PR  0.355200

[630 rows x 5 columns]

================================================================================


ANALYSIS FOR DATASET: uci_german
==================================================

All Results:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
adasyn           AdaBoost           0.509310  0.787422     0.239379
                 Gradient Boosting  0.518638  0.793139     0.156741
                 Random Forest      0.648193  0.812110     0.148913
cwgan            AdaBoost           0.475681  0.780665     0.239534
                 Gradient Boosting  0.486071  0.768711     0.160424
                 Random Forest      0.483043  0.758316     0.168958
enn              AdaBoost           0.443985  0.771310     0.243384
                 Gradient Boosting  0.572804  0.777547     0.217696
                 Random Forest      0.597760  0.785343     0.199940
nearmiss         AdaBoost           0.405866  0.707900     0.243842
                 Gradient Boosting  0.434809  0.696466     0.224133
                 Random Forest      0.430121  0.689709     0.215533
ros              AdaBoost           0.464614  0.762474     0.241799
                 Gradient Boosting  0.545642  0.785343     0.172145
                 Random Forest      0.563668  0.785603     0.156852
rus              AdaBoost           0.441203  0.776507     0.242263
                 Gradient Boosting  0.485986  0.785863     0.185932
                 Random Forest      0.619757  0.801975     0.181109
smote            AdaBoost           0.489220  0.785863     0.238692
                 Gradient Boosting  0.562884  0.804574     0.155846
                 Random Forest      0.580915  0.791580     0.156839
smote_bs1        AdaBoost           0.503087  0.788981     0.240741
                 Gradient Boosting  0.545742  0.798857     0.157834
                 Random Forest      0.641044  0.800416     0.152443
smote_bs2        AdaBoost           0.460603  0.761954     0.240370
                 Gradient Boosting  0.523710  0.801455     0.156831
                 Random Forest      0.602453  0.808472     0.151907
smote_enn        AdaBoost           0.532423  0.796258     0.236447
                 Gradient Boosting  0.524243  0.788462     0.241737
                 Random Forest      0.571405  0.791060     0.204302
smote_tomek      AdaBoost           0.498032  0.795738     0.240586
                 Gradient Boosting  0.577250  0.799376     0.154447
                 Random Forest      0.639356  0.801195     0.151624
tabddpm_bgm      AdaBoost           0.621904  0.797993     0.243076
                 Gradient Boosting  0.621707  0.811396     0.158547
                 Random Forest      0.646803  0.791982     0.161737
tabddpm_identity AdaBoost           0.620721  0.830268     0.243807
                 Gradient Boosting  0.639277  0.807068     0.154043
                 Random Forest      0.658690  0.808511     0.159947
tomek            AdaBoost           0.474484  0.773389     0.239779
                 Gradient Boosting  0.464456  0.774948     0.169526
                 Random Forest      0.540138  0.762214     0.162409

Sorted by AUC-PR:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
tabddpm_identity Random Forest      0.658690  0.808511     0.159947
adasyn           Random Forest      0.648193  0.812110     0.148913
tabddpm_bgm      Random Forest      0.646803  0.791982     0.161737
smote_bs1        Random Forest      0.641044  0.800416     0.152443
smote_tomek      Random Forest      0.639356  0.801195     0.151624
tabddpm_identity Gradient Boosting  0.639277  0.807068     0.154043
tabddpm_bgm      AdaBoost           0.621904  0.797993     0.243076
                 Gradient Boosting  0.621707  0.811396     0.158547
tabddpm_identity AdaBoost           0.620721  0.830268     0.243807
rus              Random Forest      0.619757  0.801975     0.181109
smote_bs2        Random Forest      0.602453  0.808472     0.151907
enn              Random Forest      0.597760  0.785343     0.199940
smote            Random Forest      0.580915  0.791580     0.156839
smote_tomek      Gradient Boosting  0.577250  0.799376     0.154447
enn              Gradient Boosting  0.572804  0.777547     0.217696
smote_enn        Random Forest      0.571405  0.791060     0.204302
ros              Random Forest      0.563668  0.785603     0.156852
smote            Gradient Boosting  0.562884  0.804574     0.155846
smote_bs1        Gradient Boosting  0.545742  0.798857     0.157834
ros              Gradient Boosting  0.545642  0.785343     0.172145
tomek            Random Forest      0.540138  0.762214     0.162409
smote_enn        AdaBoost           0.532423  0.796258     0.236447
                 Gradient Boosting  0.524243  0.788462     0.241737
smote_bs2        Gradient Boosting  0.523710  0.801455     0.156831
adasyn           Gradient Boosting  0.518638  0.793139     0.156741
                 AdaBoost           0.509310  0.787422     0.239379
smote_bs1        AdaBoost           0.503087  0.788981     0.240741
smote_tomek      AdaBoost           0.498032  0.795738     0.240586
smote            AdaBoost           0.489220  0.785863     0.238692
cwgan            Gradient Boosting  0.486071  0.768711     0.160424
rus              Gradient Boosting  0.485986  0.785863     0.185932
cwgan            Random Forest      0.483043  0.758316     0.168958
                 AdaBoost           0.475681  0.780665     0.239534
tomek            AdaBoost           0.474484  0.773389     0.239779
ros              AdaBoost           0.464614  0.762474     0.241799
tomek            Gradient Boosting  0.464456  0.774948     0.169526
smote_bs2        AdaBoost           0.460603  0.761954     0.240370
enn              AdaBoost           0.443985  0.771310     0.243384
rus              AdaBoost           0.441203  0.776507     0.242263
nearmiss         Gradient Boosting  0.434809  0.696466     0.224133
                 Random Forest      0.430121  0.689709     0.215533
                 AdaBoost           0.405866  0.707900     0.243842

Sorted by AUC-ROC:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
tabddpm_identity AdaBoost           0.620721  0.830268     0.243807
adasyn           Random Forest      0.648193  0.812110     0.148913
tabddpm_bgm      Gradient Boosting  0.621707  0.811396     0.158547
tabddpm_identity Random Forest      0.658690  0.808511     0.159947
smote_bs2        Random Forest      0.602453  0.808472     0.151907
tabddpm_identity Gradient Boosting  0.639277  0.807068     0.154043
smote            Gradient Boosting  0.562884  0.804574     0.155846
rus              Random Forest      0.619757  0.801975     0.181109
smote_bs2        Gradient Boosting  0.523710  0.801455     0.156831
smote_tomek      Random Forest      0.639356  0.801195     0.151624
smote_bs1        Random Forest      0.641044  0.800416     0.152443
smote_tomek      Gradient Boosting  0.577250  0.799376     0.154447
smote_bs1        Gradient Boosting  0.545742  0.798857     0.157834
tabddpm_bgm      AdaBoost           0.621904  0.797993     0.243076
smote_enn        AdaBoost           0.532423  0.796258     0.236447
smote_tomek      AdaBoost           0.498032  0.795738     0.240586
adasyn           Gradient Boosting  0.518638  0.793139     0.156741
tabddpm_bgm      Random Forest      0.646803  0.791982     0.161737
smote            Random Forest      0.580915  0.791580     0.156839
smote_enn        Random Forest      0.571405  0.791060     0.204302
smote_bs1        AdaBoost           0.503087  0.788981     0.240741
smote_enn        Gradient Boosting  0.524243  0.788462     0.241737
adasyn           AdaBoost           0.509310  0.787422     0.239379
smote            AdaBoost           0.489220  0.785863     0.238692
rus              Gradient Boosting  0.485986  0.785863     0.185932
ros              Random Forest      0.563668  0.785603     0.156852
                 Gradient Boosting  0.545642  0.785343     0.172145
enn              Random Forest      0.597760  0.785343     0.199940
cwgan            AdaBoost           0.475681  0.780665     0.239534
enn              Gradient Boosting  0.572804  0.777547     0.217696
rus              AdaBoost           0.441203  0.776507     0.242263
tomek            Gradient Boosting  0.464456  0.774948     0.169526
                 AdaBoost           0.474484  0.773389     0.239779
enn              AdaBoost           0.443985  0.771310     0.243384
cwgan            Gradient Boosting  0.486071  0.768711     0.160424
ros              AdaBoost           0.464614  0.762474     0.241799
tomek            Random Forest      0.540138  0.762214     0.162409
smote_bs2        AdaBoost           0.460603  0.761954     0.240370
cwgan            Random Forest      0.483043  0.758316     0.168958
nearmiss         AdaBoost           0.405866  0.707900     0.243842
                 Gradient Boosting  0.434809  0.696466     0.224133
                 Random Forest      0.430121  0.689709     0.215533

Sorted by Brier-Score:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
nearmiss         AdaBoost           0.405866  0.707900     0.243842
tabddpm_identity AdaBoost           0.620721  0.830268     0.243807
enn              AdaBoost           0.443985  0.771310     0.243384
tabddpm_bgm      AdaBoost           0.621904  0.797993     0.243076
rus              AdaBoost           0.441203  0.776507     0.242263
ros              AdaBoost           0.464614  0.762474     0.241799
smote_enn        Gradient Boosting  0.524243  0.788462     0.241737
smote_bs1        AdaBoost           0.503087  0.788981     0.240741
smote_tomek      AdaBoost           0.498032  0.795738     0.240586
smote_bs2        AdaBoost           0.460603  0.761954     0.240370
tomek            AdaBoost           0.474484  0.773389     0.239779
cwgan            AdaBoost           0.475681  0.780665     0.239534
adasyn           AdaBoost           0.509310  0.787422     0.239379
smote            AdaBoost           0.489220  0.785863     0.238692
smote_enn        AdaBoost           0.532423  0.796258     0.236447
nearmiss         Gradient Boosting  0.434809  0.696466     0.224133
enn              Gradient Boosting  0.572804  0.777547     0.217696
nearmiss         Random Forest      0.430121  0.689709     0.215533
smote_enn        Random Forest      0.571405  0.791060     0.204302
enn              Random Forest      0.597760  0.785343     0.199940
rus              Gradient Boosting  0.485986  0.785863     0.185932
                 Random Forest      0.619757  0.801975     0.181109
ros              Gradient Boosting  0.545642  0.785343     0.172145
tomek            Gradient Boosting  0.464456  0.774948     0.169526
cwgan            Random Forest      0.483043  0.758316     0.168958
tomek            Random Forest      0.540138  0.762214     0.162409
tabddpm_bgm      Random Forest      0.646803  0.791982     0.161737
cwgan            Gradient Boosting  0.486071  0.768711     0.160424
tabddpm_identity Random Forest      0.658690  0.808511     0.159947
tabddpm_bgm      Gradient Boosting  0.621707  0.811396     0.158547
smote_bs1        Gradient Boosting  0.545742  0.798857     0.157834
ros              Random Forest      0.563668  0.785603     0.156852
smote            Random Forest      0.580915  0.791580     0.156839
smote_bs2        Gradient Boosting  0.523710  0.801455     0.156831
adasyn           Gradient Boosting  0.518638  0.793139     0.156741
smote            Gradient Boosting  0.562884  0.804574     0.155846
smote_tomek      Gradient Boosting  0.577250  0.799376     0.154447
tabddpm_identity Gradient Boosting  0.639277  0.807068     0.154043
smote_bs1        Random Forest      0.641044  0.800416     0.152443
smote_bs2        Random Forest      0.602453  0.808472     0.151907
smote_tomek      Random Forest      0.639356  0.801195     0.151624
adasyn           Random Forest      0.648193  0.812110     0.148913

================================================================================


ANALYSIS FOR DATASET: uci_taiwan
==================================================

All Results:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
adasyn           AdaBoost           0.549939  0.760056     0.247338
                 Gradient Boosting  0.579500  0.780291     0.167737
                 Random Forest      0.539947  0.761406     0.154567
cwgan            AdaBoost           0.579610  0.792694     0.243481
                 Gradient Boosting  0.588930  0.801745     0.128724
                 Random Forest      0.576890  0.782142     0.131917
enn              AdaBoost           0.585322  0.800890     0.243640
                 Gradient Boosting  0.586578  0.802124     0.147026
                 Random Forest      0.587964  0.786008     0.155922
nearmiss         AdaBoost           0.295983  0.638243     0.260278
                 Gradient Boosting  0.314471  0.658749     0.340387
                 Random Forest      0.335191  0.633629     0.393159
ros              AdaBoost           0.577516  0.792857     0.246968
                 Gradient Boosting  0.590013  0.798558     0.171097
                 Random Forest      0.566425  0.778623     0.135838
rus              AdaBoost           0.576733  0.793379     0.246919
                 Gradient Boosting  0.586022  0.799156     0.173272
                 Random Forest      0.572139  0.786932     0.180427
smote            AdaBoost           0.563264  0.771261     0.246718
                 Gradient Boosting  0.585007  0.786239     0.157707
                 Random Forest      0.549329  0.767539     0.150224
smote_bs1        AdaBoost           0.544023  0.759201     0.247450
                 Gradient Boosting  0.573286  0.783568     0.169800
                 Random Forest      0.518001  0.767028     0.153724
smote_bs2        AdaBoost           0.555773  0.780616     0.247492
                 Gradient Boosting  0.565018  0.787615     0.173067
                 Random Forest      0.529024  0.773336     0.150569
smote_enn        AdaBoost           0.569976  0.781311     0.245323
                 Gradient Boosting  0.582789  0.789103     0.178942
                 Random Forest      0.562460  0.774917     0.172613
smote_tomek      AdaBoost           0.563349  0.764640     0.246503
                 Gradient Boosting  0.581067  0.785447     0.157570
                 Random Forest      0.541150  0.762999     0.151578
tabddpm_bgm      AdaBoost           0.528044  0.777453     0.243848
                 Gradient Boosting  0.537174  0.776633     0.135257
                 Random Forest      0.520188  0.753912     0.139967
tabddpm_identity AdaBoost           0.526922  0.773764     0.243700
                 Gradient Boosting  0.537294  0.777620     0.134989
                 Random Forest      0.521853  0.758888     0.138955
tomek            AdaBoost           0.578309  0.795087     0.243567
                 Gradient Boosting  0.590492  0.801174     0.127933
                 Random Forest      0.571649  0.783089     0.133052

Sorted by AUC-PR:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
tomek            Gradient Boosting  0.590492  0.801174     0.127933
ros              Gradient Boosting  0.590013  0.798558     0.171097
cwgan            Gradient Boosting  0.588930  0.801745     0.128724
enn              Random Forest      0.587964  0.786008     0.155922
                 Gradient Boosting  0.586578  0.802124     0.147026
rus              Gradient Boosting  0.586022  0.799156     0.173272
enn              AdaBoost           0.585322  0.800890     0.243640
smote            Gradient Boosting  0.585007  0.786239     0.157707
smote_enn        Gradient Boosting  0.582789  0.789103     0.178942
smote_tomek      Gradient Boosting  0.581067  0.785447     0.157570
cwgan            AdaBoost           0.579610  0.792694     0.243481
adasyn           Gradient Boosting  0.579500  0.780291     0.167737
tomek            AdaBoost           0.578309  0.795087     0.243567
ros              AdaBoost           0.577516  0.792857     0.246968
cwgan            Random Forest      0.576890  0.782142     0.131917
rus              AdaBoost           0.576733  0.793379     0.246919
smote_bs1        Gradient Boosting  0.573286  0.783568     0.169800
rus              Random Forest      0.572139  0.786932     0.180427
tomek            Random Forest      0.571649  0.783089     0.133052
smote_enn        AdaBoost           0.569976  0.781311     0.245323
ros              Random Forest      0.566425  0.778623     0.135838
smote_bs2        Gradient Boosting  0.565018  0.787615     0.173067
smote_tomek      AdaBoost           0.563349  0.764640     0.246503
smote            AdaBoost           0.563264  0.771261     0.246718
smote_enn        Random Forest      0.562460  0.774917     0.172613
smote_bs2        AdaBoost           0.555773  0.780616     0.247492
adasyn           AdaBoost           0.549939  0.760056     0.247338
smote            Random Forest      0.549329  0.767539     0.150224
smote_bs1        AdaBoost           0.544023  0.759201     0.247450
smote_tomek      Random Forest      0.541150  0.762999     0.151578
adasyn           Random Forest      0.539947  0.761406     0.154567
tabddpm_identity Gradient Boosting  0.537294  0.777620     0.134989
tabddpm_bgm      Gradient Boosting  0.537174  0.776633     0.135257
smote_bs2        Random Forest      0.529024  0.773336     0.150569
tabddpm_bgm      AdaBoost           0.528044  0.777453     0.243848
tabddpm_identity AdaBoost           0.526922  0.773764     0.243700
                 Random Forest      0.521853  0.758888     0.138955
tabddpm_bgm      Random Forest      0.520188  0.753912     0.139967
smote_bs1        Random Forest      0.518001  0.767028     0.153724
nearmiss         Random Forest      0.335191  0.633629     0.393159
                 Gradient Boosting  0.314471  0.658749     0.340387
                 AdaBoost           0.295983  0.638243     0.260278

Sorted by AUC-ROC:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
enn              Gradient Boosting  0.586578  0.802124     0.147026
cwgan            Gradient Boosting  0.588930  0.801745     0.128724
tomek            Gradient Boosting  0.590492  0.801174     0.127933
enn              AdaBoost           0.585322  0.800890     0.243640
rus              Gradient Boosting  0.586022  0.799156     0.173272
ros              Gradient Boosting  0.590013  0.798558     0.171097
tomek            AdaBoost           0.578309  0.795087     0.243567
rus              AdaBoost           0.576733  0.793379     0.246919
ros              AdaBoost           0.577516  0.792857     0.246968
cwgan            AdaBoost           0.579610  0.792694     0.243481
smote_enn        Gradient Boosting  0.582789  0.789103     0.178942
smote_bs2        Gradient Boosting  0.565018  0.787615     0.173067
rus              Random Forest      0.572139  0.786932     0.180427
smote            Gradient Boosting  0.585007  0.786239     0.157707
enn              Random Forest      0.587964  0.786008     0.155922
smote_tomek      Gradient Boosting  0.581067  0.785447     0.157570
smote_bs1        Gradient Boosting  0.573286  0.783568     0.169800
tomek            Random Forest      0.571649  0.783089     0.133052
cwgan            Random Forest      0.576890  0.782142     0.131917
smote_enn        AdaBoost           0.569976  0.781311     0.245323
smote_bs2        AdaBoost           0.555773  0.780616     0.247492
adasyn           Gradient Boosting  0.579500  0.780291     0.167737
ros              Random Forest      0.566425  0.778623     0.135838
tabddpm_identity Gradient Boosting  0.537294  0.777620     0.134989
tabddpm_bgm      AdaBoost           0.528044  0.777453     0.243848
                 Gradient Boosting  0.537174  0.776633     0.135257
smote_enn        Random Forest      0.562460  0.774917     0.172613
tabddpm_identity AdaBoost           0.526922  0.773764     0.243700
smote_bs2        Random Forest      0.529024  0.773336     0.150569
smote            AdaBoost           0.563264  0.771261     0.246718
                 Random Forest      0.549329  0.767539     0.150224
smote_bs1        Random Forest      0.518001  0.767028     0.153724
smote_tomek      AdaBoost           0.563349  0.764640     0.246503
                 Random Forest      0.541150  0.762999     0.151578
adasyn           Random Forest      0.539947  0.761406     0.154567
                 AdaBoost           0.549939  0.760056     0.247338
smote_bs1        AdaBoost           0.544023  0.759201     0.247450
tabddpm_identity Random Forest      0.521853  0.758888     0.138955
tabddpm_bgm      Random Forest      0.520188  0.753912     0.139967
nearmiss         Gradient Boosting  0.314471  0.658749     0.340387
                 AdaBoost           0.295983  0.638243     0.260278
                 Random Forest      0.335191  0.633629     0.393159

Sorted by Brier-Score:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
nearmiss         Random Forest      0.335191  0.633629     0.393159
                 Gradient Boosting  0.314471  0.658749     0.340387
                 AdaBoost           0.295983  0.638243     0.260278
smote_bs2        AdaBoost           0.555773  0.780616     0.247492
smote_bs1        AdaBoost           0.544023  0.759201     0.247450
adasyn           AdaBoost           0.549939  0.760056     0.247338
ros              AdaBoost           0.577516  0.792857     0.246968
rus              AdaBoost           0.576733  0.793379     0.246919
smote            AdaBoost           0.563264  0.771261     0.246718
smote_tomek      AdaBoost           0.563349  0.764640     0.246503
smote_enn        AdaBoost           0.569976  0.781311     0.245323
tabddpm_bgm      AdaBoost           0.528044  0.777453     0.243848
tabddpm_identity AdaBoost           0.526922  0.773764     0.243700
enn              AdaBoost           0.585322  0.800890     0.243640
tomek            AdaBoost           0.578309  0.795087     0.243567
cwgan            AdaBoost           0.579610  0.792694     0.243481
rus              Random Forest      0.572139  0.786932     0.180427
smote_enn        Gradient Boosting  0.582789  0.789103     0.178942
rus              Gradient Boosting  0.586022  0.799156     0.173272
smote_bs2        Gradient Boosting  0.565018  0.787615     0.173067
smote_enn        Random Forest      0.562460  0.774917     0.172613
ros              Gradient Boosting  0.590013  0.798558     0.171097
smote_bs1        Gradient Boosting  0.573286  0.783568     0.169800
adasyn           Gradient Boosting  0.579500  0.780291     0.167737
smote            Gradient Boosting  0.585007  0.786239     0.157707
smote_tomek      Gradient Boosting  0.581067  0.785447     0.157570
enn              Random Forest      0.587964  0.786008     0.155922
adasyn           Random Forest      0.539947  0.761406     0.154567
smote_bs1        Random Forest      0.518001  0.767028     0.153724
smote_tomek      Random Forest      0.541150  0.762999     0.151578
smote_bs2        Random Forest      0.529024  0.773336     0.150569
smote            Random Forest      0.549329  0.767539     0.150224
enn              Gradient Boosting  0.586578  0.802124     0.147026
tabddpm_bgm      Random Forest      0.520188  0.753912     0.139967
tabddpm_identity Random Forest      0.521853  0.758888     0.138955
ros              Random Forest      0.566425  0.778623     0.135838
tabddpm_bgm      Gradient Boosting  0.537174  0.776633     0.135257
tabddpm_identity Gradient Boosting  0.537294  0.777620     0.134989
tomek            Random Forest      0.571649  0.783089     0.133052
cwgan            Random Forest      0.576890  0.782142     0.131917
                 Gradient Boosting  0.588930  0.801745     0.128724
tomek            Gradient Boosting  0.590492  0.801174     0.127933

================================================================================


ANALYSIS FOR DATASET: pakdd
==================================================

All Results:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
adasyn           AdaBoost           0.329513  0.604949     0.248661
                 Gradient Boosting  0.348370  0.631064     0.193206
                 Random Forest      0.344634  0.616172     0.191097
cwgan            AdaBoost           0.349269  0.636500     0.246951
                 Gradient Boosting  0.367661  0.649676     0.183053
                 Random Forest      0.344494  0.613718     0.188014
enn              AdaBoost           0.351188  0.634802     0.248762
                 Gradient Boosting  0.360567  0.647261     0.219432
                 Random Forest      0.355352  0.636638     0.236706
nearmiss         AdaBoost           0.281189  0.553401     0.252630
                 Gradient Boosting  0.289347  0.556893     0.306519
                 Random Forest      0.284431  0.548321     0.320995
ros              AdaBoost           0.353928  0.639810     0.249410
                 Gradient Boosting  0.367111  0.652908     0.233295
                 Random Forest      0.347424  0.625295     0.192818
rus              AdaBoost           0.352757  0.639060     0.249390
                 Gradient Boosting  0.362194  0.652481     0.234595
                 Random Forest      0.344642  0.624728     0.240920
smote            AdaBoost           0.329032  0.601221     0.248598
                 Gradient Boosting  0.352901  0.635081     0.192165
                 Random Forest      0.342644  0.618555     0.190855
smote_bs1        AdaBoost           0.341141  0.611425     0.248537
                 Gradient Boosting  0.350235  0.631838     0.191759
                 Random Forest      0.345815  0.621807     0.190027
smote_bs2        AdaBoost           0.325868  0.601831     0.248812
                 Gradient Boosting  0.351021  0.629359     0.195611
                 Random Forest      0.344742  0.623183     0.191537
smote_enn        AdaBoost           0.337868  0.622056     0.250333
                 Gradient Boosting  0.367150  0.644687     0.278120
                 Random Forest      0.355362  0.635259     0.267145
smote_tomek      AdaBoost           0.328541  0.601498     0.248525
                 Gradient Boosting  0.349003  0.632973     0.191462
                 Random Forest      0.345516  0.620719     0.190116
tabddpm_bgm      AdaBoost           0.341749  0.613121     0.247145
                 Gradient Boosting  0.359335  0.625891     0.188181
                 Random Forest      0.341865  0.601992     0.192702
tabddpm_identity AdaBoost           0.351568  0.621137     0.247101
                 Gradient Boosting  0.369232  0.630076     0.187671
                 Random Forest      0.340785  0.602164     0.192825
tomek            AdaBoost           0.346741  0.636121     0.247143
                 Gradient Boosting  0.366080  0.651303     0.182835
                 Random Forest      0.349886  0.624441     0.188486

Sorted by AUC-PR:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
tabddpm_identity Gradient Boosting  0.369232  0.630076     0.187671
cwgan            Gradient Boosting  0.367661  0.649676     0.183053
smote_enn        Gradient Boosting  0.367150  0.644687     0.278120
ros              Gradient Boosting  0.367111  0.652908     0.233295
tomek            Gradient Boosting  0.366080  0.651303     0.182835
rus              Gradient Boosting  0.362194  0.652481     0.234595
enn              Gradient Boosting  0.360567  0.647261     0.219432
tabddpm_bgm      Gradient Boosting  0.359335  0.625891     0.188181
smote_enn        Random Forest      0.355362  0.635259     0.267145
enn              Random Forest      0.355352  0.636638     0.236706
ros              AdaBoost           0.353928  0.639810     0.249410
smote            Gradient Boosting  0.352901  0.635081     0.192165
rus              AdaBoost           0.352757  0.639060     0.249390
tabddpm_identity AdaBoost           0.351568  0.621137     0.247101
enn              AdaBoost           0.351188  0.634802     0.248762
smote_bs2        Gradient Boosting  0.351021  0.629359     0.195611
smote_bs1        Gradient Boosting  0.350235  0.631838     0.191759
tomek            Random Forest      0.349886  0.624441     0.188486
cwgan            AdaBoost           0.349269  0.636500     0.246951
smote_tomek      Gradient Boosting  0.349003  0.632973     0.191462
adasyn           Gradient Boosting  0.348370  0.631064     0.193206
ros              Random Forest      0.347424  0.625295     0.192818
tomek            AdaBoost           0.346741  0.636121     0.247143
smote_bs1        Random Forest      0.345815  0.621807     0.190027
smote_tomek      Random Forest      0.345516  0.620719     0.190116
smote_bs2        Random Forest      0.344742  0.623183     0.191537
rus              Random Forest      0.344642  0.624728     0.240920
adasyn           Random Forest      0.344634  0.616172     0.191097
cwgan            Random Forest      0.344494  0.613718     0.188014
smote            Random Forest      0.342644  0.618555     0.190855
tabddpm_bgm      Random Forest      0.341865  0.601992     0.192702
                 AdaBoost           0.341749  0.613121     0.247145
smote_bs1        AdaBoost           0.341141  0.611425     0.248537
tabddpm_identity Random Forest      0.340785  0.602164     0.192825
smote_enn        AdaBoost           0.337868  0.622056     0.250333
adasyn           AdaBoost           0.329513  0.604949     0.248661
smote            AdaBoost           0.329032  0.601221     0.248598
smote_tomek      AdaBoost           0.328541  0.601498     0.248525
smote_bs2        AdaBoost           0.325868  0.601831     0.248812
nearmiss         Gradient Boosting  0.289347  0.556893     0.306519
                 Random Forest      0.284431  0.548321     0.320995
                 AdaBoost           0.281189  0.553401     0.252630

Sorted by AUC-ROC:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
ros              Gradient Boosting  0.367111  0.652908     0.233295
rus              Gradient Boosting  0.362194  0.652481     0.234595
tomek            Gradient Boosting  0.366080  0.651303     0.182835
cwgan            Gradient Boosting  0.367661  0.649676     0.183053
enn              Gradient Boosting  0.360567  0.647261     0.219432
smote_enn        Gradient Boosting  0.367150  0.644687     0.278120
ros              AdaBoost           0.353928  0.639810     0.249410
rus              AdaBoost           0.352757  0.639060     0.249390
enn              Random Forest      0.355352  0.636638     0.236706
cwgan            AdaBoost           0.349269  0.636500     0.246951
tomek            AdaBoost           0.346741  0.636121     0.247143
smote_enn        Random Forest      0.355362  0.635259     0.267145
smote            Gradient Boosting  0.352901  0.635081     0.192165
enn              AdaBoost           0.351188  0.634802     0.248762
smote_tomek      Gradient Boosting  0.349003  0.632973     0.191462
smote_bs1        Gradient Boosting  0.350235  0.631838     0.191759
adasyn           Gradient Boosting  0.348370  0.631064     0.193206
tabddpm_identity Gradient Boosting  0.369232  0.630076     0.187671
smote_bs2        Gradient Boosting  0.351021  0.629359     0.195611
tabddpm_bgm      Gradient Boosting  0.359335  0.625891     0.188181
ros              Random Forest      0.347424  0.625295     0.192818
rus              Random Forest      0.344642  0.624728     0.240920
tomek            Random Forest      0.349886  0.624441     0.188486
smote_bs2        Random Forest      0.344742  0.623183     0.191537
smote_enn        AdaBoost           0.337868  0.622056     0.250333
smote_bs1        Random Forest      0.345815  0.621807     0.190027
tabddpm_identity AdaBoost           0.351568  0.621137     0.247101
smote_tomek      Random Forest      0.345516  0.620719     0.190116
smote            Random Forest      0.342644  0.618555     0.190855
adasyn           Random Forest      0.344634  0.616172     0.191097
cwgan            Random Forest      0.344494  0.613718     0.188014
tabddpm_bgm      AdaBoost           0.341749  0.613121     0.247145
smote_bs1        AdaBoost           0.341141  0.611425     0.248537
adasyn           AdaBoost           0.329513  0.604949     0.248661
tabddpm_identity Random Forest      0.340785  0.602164     0.192825
tabddpm_bgm      Random Forest      0.341865  0.601992     0.192702
smote_bs2        AdaBoost           0.325868  0.601831     0.248812
smote_tomek      AdaBoost           0.328541  0.601498     0.248525
smote            AdaBoost           0.329032  0.601221     0.248598
nearmiss         Gradient Boosting  0.289347  0.556893     0.306519
                 AdaBoost           0.281189  0.553401     0.252630
                 Random Forest      0.284431  0.548321     0.320995

Sorted by Brier-Score:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
nearmiss         Random Forest      0.284431  0.548321     0.320995
                 Gradient Boosting  0.289347  0.556893     0.306519
smote_enn        Gradient Boosting  0.367150  0.644687     0.278120
                 Random Forest      0.355362  0.635259     0.267145
nearmiss         AdaBoost           0.281189  0.553401     0.252630
smote_enn        AdaBoost           0.337868  0.622056     0.250333
ros              AdaBoost           0.353928  0.639810     0.249410
rus              AdaBoost           0.352757  0.639060     0.249390
smote_bs2        AdaBoost           0.325868  0.601831     0.248812
enn              AdaBoost           0.351188  0.634802     0.248762
adasyn           AdaBoost           0.329513  0.604949     0.248661
smote            AdaBoost           0.329032  0.601221     0.248598
smote_bs1        AdaBoost           0.341141  0.611425     0.248537
smote_tomek      AdaBoost           0.328541  0.601498     0.248525
tabddpm_bgm      AdaBoost           0.341749  0.613121     0.247145
tomek            AdaBoost           0.346741  0.636121     0.247143
tabddpm_identity AdaBoost           0.351568  0.621137     0.247101
cwgan            AdaBoost           0.349269  0.636500     0.246951
rus              Random Forest      0.344642  0.624728     0.240920
enn              Random Forest      0.355352  0.636638     0.236706
rus              Gradient Boosting  0.362194  0.652481     0.234595
ros              Gradient Boosting  0.367111  0.652908     0.233295
enn              Gradient Boosting  0.360567  0.647261     0.219432
smote_bs2        Gradient Boosting  0.351021  0.629359     0.195611
adasyn           Gradient Boosting  0.348370  0.631064     0.193206
tabddpm_identity Random Forest      0.340785  0.602164     0.192825
ros              Random Forest      0.347424  0.625295     0.192818
tabddpm_bgm      Random Forest      0.341865  0.601992     0.192702
smote            Gradient Boosting  0.352901  0.635081     0.192165
smote_bs1        Gradient Boosting  0.350235  0.631838     0.191759
smote_bs2        Random Forest      0.344742  0.623183     0.191537
smote_tomek      Gradient Boosting  0.349003  0.632973     0.191462
adasyn           Random Forest      0.344634  0.616172     0.191097
smote            Random Forest      0.342644  0.618555     0.190855
smote_tomek      Random Forest      0.345516  0.620719     0.190116
smote_bs1        Random Forest      0.345815  0.621807     0.190027
tomek            Random Forest      0.349886  0.624441     0.188486
tabddpm_bgm      Gradient Boosting  0.359335  0.625891     0.188181
cwgan            Random Forest      0.344494  0.613718     0.188014
tabddpm_identity Gradient Boosting  0.369232  0.630076     0.187671
cwgan            Gradient Boosting  0.367661  0.649676     0.183053
tomek            Gradient Boosting  0.366080  0.651303     0.182835

================================================================================


ANALYSIS FOR DATASET: hmeq
==================================================

All Results:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
adasyn           AdaBoost           0.787464  0.895937     0.239297
                 Gradient Boosting  0.830110  0.920592     0.087420
                 Random Forest      0.927409  0.978556     0.052694
cwgan            AdaBoost           0.806220  0.899330     0.235056
                 Gradient Boosting  0.829722  0.910831     0.073196
                 Random Forest      0.926633  0.975937     0.053537
enn              AdaBoost           0.815958  0.911005     0.232829
                 Gradient Boosting  0.841927  0.924332     0.071879
                 Random Forest      0.916634  0.972788     0.056714
nearmiss         AdaBoost           0.480540  0.802732     0.255737
                 Gradient Boosting  0.648859  0.853728     0.231082
                 Random Forest      0.585694  0.840957     0.260766
ros              AdaBoost           0.829392  0.918643     0.236899
                 Gradient Boosting  0.835648  0.920661     0.092015
                 Random Forest      0.900732  0.969743     0.057369
rus              AdaBoost           0.805823  0.916216     0.237071
                 Gradient Boosting  0.835387  0.925176     0.096152
                 Random Forest      0.853232  0.948586     0.091188
smote            AdaBoost           0.801818  0.901836     0.238698
                 Gradient Boosting  0.828723  0.919060     0.086151
                 Random Forest      0.934014  0.979635     0.051378
smote_bs1        AdaBoost           0.778182  0.891283     0.239635
                 Gradient Boosting  0.818741  0.916938     0.091256
                 Random Forest      0.921685  0.974833     0.054418
smote_bs2        AdaBoost           0.767721  0.892736     0.238990
                 Gradient Boosting  0.822701  0.919861     0.094760
                 Random Forest      0.934954  0.979104     0.051630
smote_enn        AdaBoost           0.786747  0.890883     0.238745
                 Gradient Boosting  0.829212  0.915685     0.084983
                 Random Forest      0.909533  0.973554     0.054862
smote_tomek      AdaBoost           0.796829  0.895963     0.238739
                 Gradient Boosting  0.828820  0.917251     0.087283
                 Random Forest      0.927662  0.977512     0.053104
tabddpm_bgm      AdaBoost           0.794839  0.899196     0.234129
                 Gradient Boosting  0.841231  0.929635     0.078671
                 Random Forest      0.881305  0.964627     0.066340
tabddpm_identity AdaBoost           0.820997  0.913436     0.233030
                 Gradient Boosting  0.849453  0.932643     0.076128
                 Random Forest      0.885545  0.963909     0.066478
tomek            AdaBoost           0.818017  0.911779     0.232932
                 Gradient Boosting  0.842925  0.919565     0.071121
                 Random Forest      0.920331  0.973301     0.055338

Sorted by AUC-PR:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
smote_bs2        Random Forest      0.934954  0.979104     0.051630
smote            Random Forest      0.934014  0.979635     0.051378
smote_tomek      Random Forest      0.927662  0.977512     0.053104
adasyn           Random Forest      0.927409  0.978556     0.052694
cwgan            Random Forest      0.926633  0.975937     0.053537
smote_bs1        Random Forest      0.921685  0.974833     0.054418
tomek            Random Forest      0.920331  0.973301     0.055338
enn              Random Forest      0.916634  0.972788     0.056714
smote_enn        Random Forest      0.909533  0.973554     0.054862
ros              Random Forest      0.900732  0.969743     0.057369
tabddpm_identity Random Forest      0.885545  0.963909     0.066478
tabddpm_bgm      Random Forest      0.881305  0.964627     0.066340
rus              Random Forest      0.853232  0.948586     0.091188
tabddpm_identity Gradient Boosting  0.849453  0.932643     0.076128
tomek            Gradient Boosting  0.842925  0.919565     0.071121
enn              Gradient Boosting  0.841927  0.924332     0.071879
tabddpm_bgm      Gradient Boosting  0.841231  0.929635     0.078671
ros              Gradient Boosting  0.835648  0.920661     0.092015
rus              Gradient Boosting  0.835387  0.925176     0.096152
adasyn           Gradient Boosting  0.830110  0.920592     0.087420
cwgan            Gradient Boosting  0.829722  0.910831     0.073196
ros              AdaBoost           0.829392  0.918643     0.236899
smote_enn        Gradient Boosting  0.829212  0.915685     0.084983
smote_tomek      Gradient Boosting  0.828820  0.917251     0.087283
smote            Gradient Boosting  0.828723  0.919060     0.086151
smote_bs2        Gradient Boosting  0.822701  0.919861     0.094760
tabddpm_identity AdaBoost           0.820997  0.913436     0.233030
smote_bs1        Gradient Boosting  0.818741  0.916938     0.091256
tomek            AdaBoost           0.818017  0.911779     0.232932
enn              AdaBoost           0.815958  0.911005     0.232829
cwgan            AdaBoost           0.806220  0.899330     0.235056
rus              AdaBoost           0.805823  0.916216     0.237071
smote            AdaBoost           0.801818  0.901836     0.238698
smote_tomek      AdaBoost           0.796829  0.895963     0.238739
tabddpm_bgm      AdaBoost           0.794839  0.899196     0.234129
adasyn           AdaBoost           0.787464  0.895937     0.239297
smote_enn        AdaBoost           0.786747  0.890883     0.238745
smote_bs1        AdaBoost           0.778182  0.891283     0.239635
smote_bs2        AdaBoost           0.767721  0.892736     0.238990
nearmiss         Gradient Boosting  0.648859  0.853728     0.231082
                 Random Forest      0.585694  0.840957     0.260766
                 AdaBoost           0.480540  0.802732     0.255737

Sorted by AUC-ROC:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
smote            Random Forest      0.934014  0.979635     0.051378
smote_bs2        Random Forest      0.934954  0.979104     0.051630
adasyn           Random Forest      0.927409  0.978556     0.052694
smote_tomek      Random Forest      0.927662  0.977512     0.053104
cwgan            Random Forest      0.926633  0.975937     0.053537
smote_bs1        Random Forest      0.921685  0.974833     0.054418
smote_enn        Random Forest      0.909533  0.973554     0.054862
tomek            Random Forest      0.920331  0.973301     0.055338
enn              Random Forest      0.916634  0.972788     0.056714
ros              Random Forest      0.900732  0.969743     0.057369
tabddpm_bgm      Random Forest      0.881305  0.964627     0.066340
tabddpm_identity Random Forest      0.885545  0.963909     0.066478
rus              Random Forest      0.853232  0.948586     0.091188
tabddpm_identity Gradient Boosting  0.849453  0.932643     0.076128
tabddpm_bgm      Gradient Boosting  0.841231  0.929635     0.078671
rus              Gradient Boosting  0.835387  0.925176     0.096152
enn              Gradient Boosting  0.841927  0.924332     0.071879
ros              Gradient Boosting  0.835648  0.920661     0.092015
adasyn           Gradient Boosting  0.830110  0.920592     0.087420
smote_bs2        Gradient Boosting  0.822701  0.919861     0.094760
tomek            Gradient Boosting  0.842925  0.919565     0.071121
smote            Gradient Boosting  0.828723  0.919060     0.086151
ros              AdaBoost           0.829392  0.918643     0.236899
smote_tomek      Gradient Boosting  0.828820  0.917251     0.087283
smote_bs1        Gradient Boosting  0.818741  0.916938     0.091256
rus              AdaBoost           0.805823  0.916216     0.237071
smote_enn        Gradient Boosting  0.829212  0.915685     0.084983
tabddpm_identity AdaBoost           0.820997  0.913436     0.233030
tomek            AdaBoost           0.818017  0.911779     0.232932
enn              AdaBoost           0.815958  0.911005     0.232829
cwgan            Gradient Boosting  0.829722  0.910831     0.073196
smote            AdaBoost           0.801818  0.901836     0.238698
cwgan            AdaBoost           0.806220  0.899330     0.235056
tabddpm_bgm      AdaBoost           0.794839  0.899196     0.234129
smote_tomek      AdaBoost           0.796829  0.895963     0.238739
adasyn           AdaBoost           0.787464  0.895937     0.239297
smote_bs2        AdaBoost           0.767721  0.892736     0.238990
smote_bs1        AdaBoost           0.778182  0.891283     0.239635
smote_enn        AdaBoost           0.786747  0.890883     0.238745
nearmiss         Gradient Boosting  0.648859  0.853728     0.231082
                 Random Forest      0.585694  0.840957     0.260766
                 AdaBoost           0.480540  0.802732     0.255737

Sorted by Brier-Score:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
nearmiss         Random Forest      0.585694  0.840957     0.260766
                 AdaBoost           0.480540  0.802732     0.255737
smote_bs1        AdaBoost           0.778182  0.891283     0.239635
adasyn           AdaBoost           0.787464  0.895937     0.239297
smote_bs2        AdaBoost           0.767721  0.892736     0.238990
smote_enn        AdaBoost           0.786747  0.890883     0.238745
smote_tomek      AdaBoost           0.796829  0.895963     0.238739
smote            AdaBoost           0.801818  0.901836     0.238698
rus              AdaBoost           0.805823  0.916216     0.237071
ros              AdaBoost           0.829392  0.918643     0.236899
cwgan            AdaBoost           0.806220  0.899330     0.235056
tabddpm_bgm      AdaBoost           0.794839  0.899196     0.234129
tabddpm_identity AdaBoost           0.820997  0.913436     0.233030
tomek            AdaBoost           0.818017  0.911779     0.232932
enn              AdaBoost           0.815958  0.911005     0.232829
nearmiss         Gradient Boosting  0.648859  0.853728     0.231082
rus              Gradient Boosting  0.835387  0.925176     0.096152
smote_bs2        Gradient Boosting  0.822701  0.919861     0.094760
ros              Gradient Boosting  0.835648  0.920661     0.092015
smote_bs1        Gradient Boosting  0.818741  0.916938     0.091256
rus              Random Forest      0.853232  0.948586     0.091188
adasyn           Gradient Boosting  0.830110  0.920592     0.087420
smote_tomek      Gradient Boosting  0.828820  0.917251     0.087283
smote            Gradient Boosting  0.828723  0.919060     0.086151
smote_enn        Gradient Boosting  0.829212  0.915685     0.084983
tabddpm_bgm      Gradient Boosting  0.841231  0.929635     0.078671
tabddpm_identity Gradient Boosting  0.849453  0.932643     0.076128
cwgan            Gradient Boosting  0.829722  0.910831     0.073196
enn              Gradient Boosting  0.841927  0.924332     0.071879
tomek            Gradient Boosting  0.842925  0.919565     0.071121
tabddpm_identity Random Forest      0.885545  0.963909     0.066478
tabddpm_bgm      Random Forest      0.881305  0.964627     0.066340
ros              Random Forest      0.900732  0.969743     0.057369
enn              Random Forest      0.916634  0.972788     0.056714
tomek            Random Forest      0.920331  0.973301     0.055338
smote_enn        Random Forest      0.909533  0.973554     0.054862
smote_bs1        Random Forest      0.921685  0.974833     0.054418
cwgan            Random Forest      0.926633  0.975937     0.053537
smote_tomek      Random Forest      0.927662  0.977512     0.053104
adasyn           Random Forest      0.927409  0.978556     0.052694
smote_bs2        Random Forest      0.934954  0.979104     0.051630
smote            Random Forest      0.934014  0.979635     0.051378

================================================================================


ANALYSIS FOR DATASET: gmsc
==================================================

All Results:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
adasyn           AdaBoost           0.318681  0.803322     0.244079
                 Gradient Boosting  0.339183  0.815890     0.089026
                 Random Forest      0.301639  0.818391     0.062014
cwgan            AdaBoost           0.355059  0.821964     0.234904
                 Gradient Boosting  0.375113  0.828563     0.050849
                 Random Forest      0.360671  0.830824     0.051512
enn              AdaBoost           0.368129  0.825312     0.235237
                 Gradient Boosting  0.380785  0.830812     0.053193
                 Random Forest      0.366399  0.840301     0.055119
nearmiss         AdaBoost           0.118109  0.688322     0.275609
                 Gradient Boosting  0.218863  0.744820     0.411491
                 Random Forest      0.322053  0.750976     0.477892
ros              AdaBoost           0.356996  0.825673     0.246057
                 Gradient Boosting  0.374420  0.830456     0.146957
                 Random Forest      0.317711  0.822828     0.055993
rus              AdaBoost           0.346518  0.825738     0.246070
                 Gradient Boosting  0.375477  0.838991     0.147161
                 Random Forest      0.359229  0.848064     0.151527
smote            AdaBoost           0.326516  0.800849     0.243516
                 Gradient Boosting  0.344699  0.818798     0.083547
                 Random Forest      0.305791  0.824649     0.060796
smote_bs1        AdaBoost           0.323186  0.808338     0.241669
                 Gradient Boosting  0.336933  0.821698     0.078912
                 Random Forest      0.315039  0.831935     0.057528
smote_bs2        AdaBoost           0.325775  0.807613     0.242992
                 Gradient Boosting  0.340910  0.821636     0.100125
                 Random Forest      0.302794  0.835216     0.058719
smote_enn        AdaBoost           0.342788  0.815325     0.242187
                 Gradient Boosting  0.357769  0.828603     0.091772
                 Random Forest      0.354440  0.840615     0.067961
smote_tomek      AdaBoost           0.329682  0.809689     0.243305
                 Gradient Boosting  0.347268  0.821383     0.081840
                 Random Forest      0.306641  0.825735     0.060382
tabddpm_bgm      AdaBoost           0.327993  0.814214     0.235645
                 Gradient Boosting  0.355200  0.826865     0.049964
                 Random Forest      0.317963  0.795032     0.053583
tabddpm_identity AdaBoost           0.336278  0.818791     0.245026
                 Gradient Boosting  0.334725  0.827308     0.116552
                 Random Forest      0.248803  0.782873     0.130914
tomek            AdaBoost           0.353659  0.825039     0.234946
                 Gradient Boosting  0.385126  0.830312     0.050311
                 Random Forest      0.360056  0.837585     0.051650

Sorted by AUC-PR:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
tomek            Gradient Boosting  0.385126  0.830312     0.050311
enn              Gradient Boosting  0.380785  0.830812     0.053193
rus              Gradient Boosting  0.375477  0.838991     0.147161
cwgan            Gradient Boosting  0.375113  0.828563     0.050849
ros              Gradient Boosting  0.374420  0.830456     0.146957
enn              AdaBoost           0.368129  0.825312     0.235237
                 Random Forest      0.366399  0.840301     0.055119
cwgan            Random Forest      0.360671  0.830824     0.051512
tomek            Random Forest      0.360056  0.837585     0.051650
rus              Random Forest      0.359229  0.848064     0.151527
smote_enn        Gradient Boosting  0.357769  0.828603     0.091772
ros              AdaBoost           0.356996  0.825673     0.246057
tabddpm_bgm      Gradient Boosting  0.355200  0.826865     0.049964
cwgan            AdaBoost           0.355059  0.821964     0.234904
smote_enn        Random Forest      0.354440  0.840615     0.067961
tomek            AdaBoost           0.353659  0.825039     0.234946
smote_tomek      Gradient Boosting  0.347268  0.821383     0.081840
rus              AdaBoost           0.346518  0.825738     0.246070
smote            Gradient Boosting  0.344699  0.818798     0.083547
smote_enn        AdaBoost           0.342788  0.815325     0.242187
smote_bs2        Gradient Boosting  0.340910  0.821636     0.100125
adasyn           Gradient Boosting  0.339183  0.815890     0.089026
smote_bs1        Gradient Boosting  0.336933  0.821698     0.078912
tabddpm_identity AdaBoost           0.336278  0.818791     0.245026
                 Gradient Boosting  0.334725  0.827308     0.116552
smote_tomek      AdaBoost           0.329682  0.809689     0.243305
tabddpm_bgm      AdaBoost           0.327993  0.814214     0.235645
smote            AdaBoost           0.326516  0.800849     0.243516
smote_bs2        AdaBoost           0.325775  0.807613     0.242992
smote_bs1        AdaBoost           0.323186  0.808338     0.241669
nearmiss         Random Forest      0.322053  0.750976     0.477892
adasyn           AdaBoost           0.318681  0.803322     0.244079
tabddpm_bgm      Random Forest      0.317963  0.795032     0.053583
ros              Random Forest      0.317711  0.822828     0.055993
smote_bs1        Random Forest      0.315039  0.831935     0.057528
smote_tomek      Random Forest      0.306641  0.825735     0.060382
smote            Random Forest      0.305791  0.824649     0.060796
smote_bs2        Random Forest      0.302794  0.835216     0.058719
adasyn           Random Forest      0.301639  0.818391     0.062014
tabddpm_identity Random Forest      0.248803  0.782873     0.130914
nearmiss         Gradient Boosting  0.218863  0.744820     0.411491
                 AdaBoost           0.118109  0.688322     0.275609

Sorted by AUC-ROC:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
rus              Random Forest      0.359229  0.848064     0.151527
smote_enn        Random Forest      0.354440  0.840615     0.067961
enn              Random Forest      0.366399  0.840301     0.055119
rus              Gradient Boosting  0.375477  0.838991     0.147161
tomek            Random Forest      0.360056  0.837585     0.051650
smote_bs2        Random Forest      0.302794  0.835216     0.058719
smote_bs1        Random Forest      0.315039  0.831935     0.057528
cwgan            Random Forest      0.360671  0.830824     0.051512
enn              Gradient Boosting  0.380785  0.830812     0.053193
ros              Gradient Boosting  0.374420  0.830456     0.146957
tomek            Gradient Boosting  0.385126  0.830312     0.050311
smote_enn        Gradient Boosting  0.357769  0.828603     0.091772
cwgan            Gradient Boosting  0.375113  0.828563     0.050849
tabddpm_identity Gradient Boosting  0.334725  0.827308     0.116552
tabddpm_bgm      Gradient Boosting  0.355200  0.826865     0.049964
rus              AdaBoost           0.346518  0.825738     0.246070
smote_tomek      Random Forest      0.306641  0.825735     0.060382
ros              AdaBoost           0.356996  0.825673     0.246057
enn              AdaBoost           0.368129  0.825312     0.235237
tomek            AdaBoost           0.353659  0.825039     0.234946
smote            Random Forest      0.305791  0.824649     0.060796
ros              Random Forest      0.317711  0.822828     0.055993
cwgan            AdaBoost           0.355059  0.821964     0.234904
smote_bs1        Gradient Boosting  0.336933  0.821698     0.078912
smote_bs2        Gradient Boosting  0.340910  0.821636     0.100125
smote_tomek      Gradient Boosting  0.347268  0.821383     0.081840
smote            Gradient Boosting  0.344699  0.818798     0.083547
tabddpm_identity AdaBoost           0.336278  0.818791     0.245026
adasyn           Random Forest      0.301639  0.818391     0.062014
                 Gradient Boosting  0.339183  0.815890     0.089026
smote_enn        AdaBoost           0.342788  0.815325     0.242187
tabddpm_bgm      AdaBoost           0.327993  0.814214     0.235645
smote_tomek      AdaBoost           0.329682  0.809689     0.243305
smote_bs1        AdaBoost           0.323186  0.808338     0.241669
smote_bs2        AdaBoost           0.325775  0.807613     0.242992
adasyn           AdaBoost           0.318681  0.803322     0.244079
smote            AdaBoost           0.326516  0.800849     0.243516
tabddpm_bgm      Random Forest      0.317963  0.795032     0.053583
tabddpm_identity Random Forest      0.248803  0.782873     0.130914
nearmiss         Random Forest      0.322053  0.750976     0.477892
                 Gradient Boosting  0.218863  0.744820     0.411491
                 AdaBoost           0.118109  0.688322     0.275609

Sorted by Brier-Score:
metric                                AUC-PR   AUC-ROC  Brier-Score
resample_method  classifier
nearmiss         Random Forest      0.322053  0.750976     0.477892
                 Gradient Boosting  0.218863  0.744820     0.411491
                 AdaBoost           0.118109  0.688322     0.275609
rus              AdaBoost           0.346518  0.825738     0.246070
ros              AdaBoost           0.356996  0.825673     0.246057
tabddpm_identity AdaBoost           0.336278  0.818791     0.245026
adasyn           AdaBoost           0.318681  0.803322     0.244079
smote            AdaBoost           0.326516  0.800849     0.243516
smote_tomek      AdaBoost           0.329682  0.809689     0.243305
smote_bs2        AdaBoost           0.325775  0.807613     0.242992
smote_enn        AdaBoost           0.342788  0.815325     0.242187
smote_bs1        AdaBoost           0.323186  0.808338     0.241669
tabddpm_bgm      AdaBoost           0.327993  0.814214     0.235645
enn              AdaBoost           0.368129  0.825312     0.235237
tomek            AdaBoost           0.353659  0.825039     0.234946
cwgan            AdaBoost           0.355059  0.821964     0.234904
rus              Random Forest      0.359229  0.848064     0.151527
                 Gradient Boosting  0.375477  0.838991     0.147161
ros              Gradient Boosting  0.374420  0.830456     0.146957
tabddpm_identity Random Forest      0.248803  0.782873     0.130914
                 Gradient Boosting  0.334725  0.827308     0.116552
smote_bs2        Gradient Boosting  0.340910  0.821636     0.100125
smote_enn        Gradient Boosting  0.357769  0.828603     0.091772
adasyn           Gradient Boosting  0.339183  0.815890     0.089026
smote            Gradient Boosting  0.344699  0.818798     0.083547
smote_tomek      Gradient Boosting  0.347268  0.821383     0.081840
smote_bs1        Gradient Boosting  0.336933  0.821698     0.078912
smote_enn        Random Forest      0.354440  0.840615     0.067961
adasyn           Random Forest      0.301639  0.818391     0.062014
smote            Random Forest      0.305791  0.824649     0.060796
smote_tomek      Random Forest      0.306641  0.825735     0.060382
smote_bs2        Random Forest      0.302794  0.835216     0.058719
smote_bs1        Random Forest      0.315039  0.831935     0.057528
ros              Random Forest      0.317711  0.822828     0.055993
enn              Random Forest      0.366399  0.840301     0.055119
tabddpm_bgm      Random Forest      0.317963  0.795032     0.053583
enn              Gradient Boosting  0.380785  0.830812     0.053193
tomek            Random Forest      0.360056  0.837585     0.051650
cwgan            Random Forest      0.360671  0.830824     0.051512
                 Gradient Boosting  0.375113  0.828563     0.050849
tomek            Gradient Boosting  0.385126  0.830312     0.050311
tabddpm_bgm      Gradient Boosting  0.355200  0.826865     0.049964

================================================================================

Finished python src\main.py at: Fri 12/13/2024  7:43:08.92
All tasks completed at: Fri 12/13/2024  7:43:08.93
Press any key to continue . . .
Starting script at: Thu 12/26/2024 10:36:47.35
german_data dataset preparation
train set:  (600, 7) (600, 13) (600,)
val set:  (200, 7) (200, 13) (200,)
test set:  (200, 7) (200, 13) (200,)
pakdd_data dataset preparation
train set:  (30000, 8) (30000, 25) (30000,)
val set:  (10000, 8) (10000, 25) (10000,)
test set:  (10000, 8) (10000, 25) (10000,)
hmeq_data dataset preparation
train set:  (3576, 10) (3576, 2) (3576,)
val set:  (1192, 10) (1192, 2) (1192,)
test set:  (1192, 10) (1192, 2) (1192,)
taiwan dataset preparation
train set:  (18000, 14) (18000, 9) (18000,)
val set:  (6000, 14) (6000, 9) (6000,)
test set:  (6000, 14) (6000, 9) (6000,)
gmsc_data dataset preparation
train set:  (90000, 10) (90000, 0) (90000,)
val set:  (30000, 10) (30000, 0) (30000,)
test set:  (30000, 10) (30000, 0) (30000,)
Finished data prep at: Thu 12/26/2024 10:37:01.19
######################### myTabddpm (resnet_iden): uci_german #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 61}
resnet
Step 500/1000 MLoss: 1.0665 GLoss: 1.001 Sum: 2.0675
Step 1000/1000 MLoss: 1.026 GLoss: 0.977 Sum: 2.003
Found files in Experiments\tabddpm\res_identity\uci_german:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Discrete cols: [0, 2, 3, 5, 6]
Saving Synthetic Data at:  Experiments/tabddpm/res_identity/uci_german
Num shape:  (1055, 7)
Elapsed time: 0:02:52
Finished uci_german (resnet_iden) at: Thu 12/26/2024 10:40:04.13
######################### myTabddpm (resnet_iden): uci_taiwan #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 91}
resnet
Step 500/1000 MLoss: 1.2991 GLoss: 1.0123 Sum: 2.3114
Step 1000/1000 MLoss: 1.1149 GLoss: 1.0012 Sum: 2.1161000000000003
Found files in Experiments\tabddpm\res_identity\uci_taiwan:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/res_identity/uci_taiwan
Num shape:  (60209, 14)
Elapsed time: 0:21:54
Finished uci_taiwan (resnet_iden) at: Thu 12/26/2024 11:02:03.45
######################### myTabddpm (resnet_iden): hmeq #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 18}
resnet
Step 500/1000 MLoss: 1.0611 GLoss: 0.9983 Sum: 2.0594
Step 1000/1000 MLoss: 1.0232 GLoss: 0.9601 Sum: 1.9833
Found files in Experiments\tabddpm\res_identity\hmeq:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Discrete cols: [4, 5, 7]
Saving Synthetic Data at:  Experiments/tabddpm/res_identity/hmeq
Num shape:  (15067, 10)
Elapsed time: 0:03:05
Finished hmeq (resnet_iden) at: Thu 12/26/2024 11:05:14.35
######################### myTabddpm (resnet_iden): gmsc #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 10}
resnet
Step 500/1000 MLoss: 0.0 GLoss: 0.937 Sum: 0.937
Step 1000/1000 MLoss: 0.0 GLoss: 0.9194 Sum: 0.9194
Found files in Experiments\tabddpm\res_identity\gmsc:
['config.toml', 'loss.csv', 'model.pt', 'model_ema.pt']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: [2, 6, 7, 8, 9]
Saving Synthetic Data at:  Experiments/tabddpm/res_identity/gmsc
Num shape:  (1544386, 10)
Elapsed time: 2:10:03
Finished gmsc (resnet_iden) at: Thu 12/26/2024 13:15:23.32
######################### myTabddpm (resnet_iden): pakdd #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 181}
resnet
Found files in Experiments\tabddpm\res_identity\pakdd:
['config.toml', 'loss.csv', 'model.pt', 'model_ema.pt']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: [5, 6, 7]
Saving Synthetic Data at:  Experiments/tabddpm/res_identity/pakdd
Num shape:  (74187, 8)
Elapsed time: 0:41:50
Finished pakdd (resnet_iden) at: Thu 12/26/2024 13:57:28.32
######################### myTabddpm (resnet_bgm): uci_german #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 85}
resnet
Step 500/1000 MLoss: 1.0843 GLoss: 0.9977 Sum: 2.082
Step 1000/1000 MLoss: 1.0281 GLoss: 0.9873 Sum: 2.0154
Found files in Experiments\tabddpm\res_bgm\uci_german:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/res_bgm/uci_german
Num shape:  (1055, 7)
Elapsed time: 0:03:42
Finished uci_german (resnet_bgm) at: Thu 12/26/2024 14:01:15.37
######################### myTabddpm (resnet_bgm): uci_taiwan #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 200}
resnet
Step 500/1000 MLoss: 1.5349 GLoss: 1.0198 Sum: 2.5547
Step 1000/1000 MLoss: 1.3555 GLoss: 0.9883 Sum: 2.3438
Found files in Experiments\tabddpm\res_bgm\uci_taiwan:
['config.toml', 'loss.csv', 'model.pt', 'model_ema.pt']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/res_bgm/uci_taiwan
Num shape:  (60209, 14)
Elapsed time: 0:47:21
Finished uci_taiwan (resnet_bgm) at: Thu 12/26/2024 14:48:42.10
######################### myTabddpm (resnet_bgm): hmeq #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 81}
resnet
Step 500/1000 MLoss: 1.476 GLoss: 0.9755 Sum: 2.4515000000000002
Step 1000/1000 MLoss: 1.4293 GLoss: 0.9355 Sum: 2.3648
Found files in Experiments\tabddpm\res_bgm\hmeq:
['config.toml', 'loss.csv', 'model.pt', 'model_ema.pt']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/res_bgm/hmeq
Num shape:  (15067, 10)
Elapsed time: 0:08:01
Finished hmeq (resnet_bgm) at: Thu 12/26/2024 14:56:48.97
######################### myTabddpm (resnet_bgm): gmsc #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 60}
resnet
Step 500/1000 MLoss: 0.998 GLoss: 1.0027 Sum: 2.0007
Step 1000/1000 MLoss: 0.9172 GLoss: 0.9659 Sum: 1.8831
Found files in Experiments\tabddpm\res_bgm\gmsc:
['config.toml', 'loss.csv', 'model.pt', 'model_ema.pt']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/res_bgm/gmsc
Num shape:  (1544386, 10)
Elapsed time: 5:51:07
Finished gmsc (resnet_bgm) at: Thu 12/26/2024 20:48:01.57
######################### myTabddpm (resnet_bgm): pakdd #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'n_blocks': 2, 'd_main': 3, 'd_hidden': 4, 'dropout_first': 0.25, 'dropout_second': 0.0}, 'd_in': 219}
resnet
Found files in Experiments\tabddpm\res_bgm\pakdd:
['config.toml', 'loss.csv', 'model.pt', 'model_ema.pt']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
resnet
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/res_bgm/pakdd
Num shape:  (74187, 8)
Elapsed time: 1:12:49
Finished pakdd (resnet_bgm) at: Thu 12/26/2024 22:00:56.43
######################### myTabddpm (identity): uci_german #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 61}
mlp
Step 500/1000 MLoss: 0.8974 GLoss: 0.9615 Sum: 1.8589
Step 1000/1000 MLoss: 0.8748 GLoss: 0.8125 Sum: 1.6873
Found files in Experiments\tabddpm\identity\uci_german:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Discrete cols: [0, 2, 3, 5, 6]
Saving Synthetic Data at:  Experiments/tabddpm/identity/uci_german
Num shape:  (1055, 7)
Elapsed time: 0:02:54
Finished uci_german (identity) at: Thu 12/26/2024 22:03:56.47
######################### myTabddpm (identity): uci_taiwan #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 91}
mlp
Step 500/1000 MLoss: 0.701 GLoss: 0.8895 Sum: 1.5905
Step 1000/1000 MLoss: 0.6655 GLoss: 0.7005 Sum: 1.366
Found files in Experiments\tabddpm\identity\uci_taiwan:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/identity/uci_taiwan
Num shape:  (60209, 14)
Elapsed time: 0:22:29
Finished uci_taiwan (identity) at: Thu 12/26/2024 22:26:30.66
######################### myTabddpm (identity): hmeq #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 18}
mlp
Step 500/1000 MLoss: 0.9508 GLoss: 0.7167 Sum: 1.6675
Step 1000/1000 MLoss: 0.9459 GLoss: 0.6465 Sum: 1.5924
Found files in Experiments\tabddpm\identity\hmeq:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Discrete cols: [4, 5, 7]
Saving Synthetic Data at:  Experiments/tabddpm/identity/hmeq
Num shape:  (15067, 10)
Elapsed time: 0:03:10
Finished hmeq (identity) at: Thu 12/26/2024 22:29:46.14
######################### myTabddpm (identity): gmsc #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 10}
mlp
Step 500/1000 MLoss: 0.0 GLoss: 0.276 Sum: 0.276
Step 1000/1000 MLoss: 0.0 GLoss: 0.2592 Sum: 0.2592
Found files in Experiments\tabddpm\identity\gmsc:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: [2, 6, 7, 8, 9]
Saving Synthetic Data at:  Experiments/tabddpm/identity/gmsc
Num shape:  (1544386, 10)
Elapsed time: 2:03:37
Finished gmsc (identity) at: Fri 12/27/2024  0:33:28.41
######################### myTabddpm (identity): pakdd #########################
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 181}
mlp
Found files in Experiments\tabddpm\identity\pakdd:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  identity
Fitting processor
Saved processor state to:  outputs\processor_state\processor_identity.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: [5, 6, 7]
Saving Synthetic Data at:  Experiments/tabddpm/identity/pakdd
Num shape:  (74187, 8)
Elapsed time: 0:44:37
Finished pakdd (identity) at: Fri 12/27/2024  1:18:11.35
######################### myTabddpm (bgm): uci_german #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 85}
mlp
Step 500/1000 MLoss: 0.8869 GLoss: 0.9723 Sum: 1.8592
Step 1000/1000 MLoss: 0.8227 GLoss: 0.814 Sum: 1.6366999999999998
Found files in Experiments\tabddpm\bgm\uci_german:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/uci_german
Num shape:  (1055, 7)
Elapsed time: 0:03:49
Finished uci_german (bgm) at: Fri 12/27/2024  1:22:06.20
######################### myTabddpm (bgm): uci_taiwan #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 200}
mlp
Step 500/1000 MLoss: 1.0527 GLoss: 0.9639 Sum: 2.0166
Step 1000/1000 MLoss: 1.0295 GLoss: 0.8524 Sum: 1.8819000000000001
Found files in Experiments\tabddpm\bgm\uci_taiwan:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/uci_taiwan
Num shape:  (60209, 14)
Elapsed time: 0:47:51
Finished uci_taiwan (bgm) at: Fri 12/27/2024  2:10:02.17
######################### myTabddpm (bgm): hmeq #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 81}
mlp
Step 500/1000 MLoss: 1.2927 GLoss: 0.8805 Sum: 2.1732
Step 1000/1000 MLoss: 1.2526 GLoss: 0.7047 Sum: 1.9573
Found files in Experiments\tabddpm\bgm\hmeq:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/hmeq
Num shape:  (15067, 10)
Elapsed time: 0:08:13
Finished hmeq (bgm) at: Fri 12/27/2024  2:18:21.02
######################### myTabddpm (bgm): gmsc #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 60}
mlp
Step 500/1000 MLoss: 0.7992 GLoss: 0.8864 Sum: 1.6856
Step 1000/1000 MLoss: 0.7465 GLoss: 0.7891 Sum: 1.5356
Found files in Experiments\tabddpm\bgm\gmsc:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/gmsc
Num shape:  (1544386, 10)
Elapsed time: 5:39:48
Finished gmsc (bgm) at: Fri 12/27/2024  7:58:14.62
######################### myTabddpm (bgm): pakdd #########################
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
Done

{'num_classes': 2, 'is_y_cond': True, 'rtdl_params': {'d_layers': [256, 256], 'dropout': 0.0}, 'd_in': 219}
mlp
Found files in Experiments\tabddpm\bgm\pakdd:
['config.toml', 'info.json', 'loss.csv', 'model.pt', 'model_ema.pt', 'X_cat_train.npy', 'X_cat_unnorm.npy', 'X_num_train.npy', 'X_num_unnorm.npy', 'y_train.npy']
Saved model to outputs folder
Selected tabular processor:  bgm
Fitting processor
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:277: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.
  warnings.warn(
C:\Users\ASUS\Documents\Extra Courses\ML-DL\env\lib\site-packages\sklearn\mixture\_base.py:146: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.
  .fit(X)
Saved processor state to:  outputs\processor_state\processor_bgm.pkl
mlp
C:\Users\ASUS\Documents\University\MSc\__Thesis__\__Project__\score-deep\src\myTabddpm\sample.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(model_path, map_location="cpu")
Model Loaded Successfully!
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Sample timestep    0
Discrete cols: []
Saving Synthetic Data at:  Experiments/tabddpm/bgm/pakdd
Num shape:  (74187, 8)
Elapsed time: 0:54:38
Finished pakdd (bgm) at: Fri 12/27/2024  8:52:59.79
######################### PYTHON SRC/MAIN.PY #########################
########## uci_german ##########
DEBUG:root:Dataloader: Loading uci_german
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: uci_german. Returning data.
########## uci_taiwan ##########
DEBUG:root:Dataloader: Loading uci_taiwan
DEBUG:root:Dataloader: Loaded available datasets.
       LIMIT_BAL    SEX EDUCATION MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  ... BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month
ID                                                                                  ...
1          20000  cat_2     cat_3    cat_2   24  cat_5  cat_5  cat_2  cat_2  cat_1  ...         0          0          0         0       689         0         0         0         0                           1
2         120000  cat_2     cat_3    cat_3   26  cat_2  cat_5  cat_3  cat_3  cat_3  ...      3272       3455       3261         0      1000      1000      1000         0      2000                           1
3          90000  cat_2     cat_3    cat_3   34  cat_3  cat_3  cat_3  cat_3  cat_3  ...     14331      14948      15549      1518      1500      1000      1000      1000      5000                           0
4          50000  cat_2     cat_3    cat_2   37  cat_3  cat_3  cat_3  cat_3  cat_3  ...     28314      28959      29547      2000      2019      1200      1100      1069      1000                           0
5          50000  cat_1     cat_3    cat_2   57  cat_2  cat_3  cat_2  cat_3  cat_3  ...     20940      19146      19131      2000     36681     10000      9000       689       679                           0
...          ...    ...       ...      ...  ...    ...    ...    ...    ...    ...  ...       ...        ...        ...       ...       ...       ...       ...       ...       ...                         ...
29996     220000  cat_1     cat_4    cat_2   39  cat_3  cat_3  cat_3  cat_3  cat_3  ...     88004      31237      15980      8500     20000      5003      3047      5000      1000                           0
29997     150000  cat_1     cat_4    cat_3   43  cat_2  cat_2  cat_2  cat_2  cat_3  ...      8979       5190          0      1837      3526      8998       129         0         0                           0
29998      30000  cat_1     cat_3    cat_3   37  cat_7  cat_6  cat_5  cat_2  cat_3  ...     20878      20582      19357         0         0     22000      4200      2000      3100                           1
29999      80000  cat_1     cat_4    cat_2   41  cat_4  cat_2  cat_3  cat_3  cat_3  ...     52774      11855      48944     85900      3409      1178      1926     52964      1804                           1
30000      50000  cat_1     cat_3    cat_2   46  cat_3  cat_3  cat_3  cat_3  cat_3  ...     36535      32428      15313      2078      1800      1430      1000      1000      1000                           1

[30000 rows x 24 columns]
INFO:root:Dataloader: Loaded dataset: uci_taiwan. Returning data.
########## pakdd ##########
DEBUG:root:Dataloader: Loading pakdd
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: pakdd. Returning data.
########## hmeq ##########
DEBUG:root:Dataloader: Loading hmeq
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: hmeq. Returning data.
########## gmsc ##########
DEBUG:root:Dataloader: Loading gmsc
DEBUG:root:Dataloader: Loaded available datasets.
INFO:root:Dataloader: Loaded dataset: gmsc. Returning data.
###########################################
###########################################
########### TRADITIONAL METHODS ###########
###########################################
###########################################
############### Dataset: uci_german, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1569
AUC-ROC: 0.7856
AUC-PR: 0.5637

############### Dataset: uci_german, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2418
AUC-ROC: 0.7625
AUC-PR: 0.4646

############### Dataset: uci_german, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1721
AUC-ROC: 0.7853
AUC-PR: 0.5456

############### Dataset: uci_german, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1811
AUC-ROC: 0.8020
AUC-PR: 0.6198

############### Dataset: uci_german, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2423
AUC-ROC: 0.7765
AUC-PR: 0.4412

############### Dataset: uci_german, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1859
AUC-ROC: 0.7859
AUC-PR: 0.4860

############### Dataset: uci_german, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2155
AUC-ROC: 0.6897
AUC-PR: 0.4301

############### Dataset: uci_german, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2438
AUC-ROC: 0.7079
AUC-PR: 0.4059

############### Dataset: uci_german, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2241
AUC-ROC: 0.6965
AUC-PR: 0.4348

############### Dataset: uci_german, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1999
AUC-ROC: 0.7853
AUC-PR: 0.5978

############### Dataset: uci_german, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2434
AUC-ROC: 0.7713
AUC-PR: 0.4440

############### Dataset: uci_german, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2177
AUC-ROC: 0.7775
AUC-PR: 0.5728

############### Dataset: uci_german, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1624
AUC-ROC: 0.7622
AUC-PR: 0.5401

############### Dataset: uci_german, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2398
AUC-ROC: 0.7734
AUC-PR: 0.4745

############### Dataset: uci_german, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1695
AUC-ROC: 0.7749
AUC-PR: 0.4645

############### Dataset: uci_german, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1568
AUC-ROC: 0.7916
AUC-PR: 0.5809

############### Dataset: uci_german, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.7859
AUC-PR: 0.4892

############### Dataset: uci_german, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1558
AUC-ROC: 0.8046
AUC-PR: 0.5629

############### Dataset: uci_german, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1524
AUC-ROC: 0.8004
AUC-PR: 0.6410

############### Dataset: uci_german, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2407
AUC-ROC: 0.7890
AUC-PR: 0.5031

############### Dataset: uci_german, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1578
AUC-ROC: 0.7989
AUC-PR: 0.5457

############### Dataset: uci_german, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1519
AUC-ROC: 0.8085
AUC-PR: 0.6025

############### Dataset: uci_german, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2404
AUC-ROC: 0.7620
AUC-PR: 0.4606

############### Dataset: uci_german, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1568
AUC-ROC: 0.8015
AUC-PR: 0.5237

############### Dataset: uci_german, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2043
AUC-ROC: 0.7911
AUC-PR: 0.5714

############### Dataset: uci_german, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2364
AUC-ROC: 0.7963
AUC-PR: 0.5324

############### Dataset: uci_german, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2417
AUC-ROC: 0.7885
AUC-PR: 0.5242

############### Dataset: uci_german, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1516
AUC-ROC: 0.8012
AUC-PR: 0.6394

############### Dataset: uci_german, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2406
AUC-ROC: 0.7957
AUC-PR: 0.4980

############### Dataset: uci_german, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1544
AUC-ROC: 0.7994
AUC-PR: 0.5772

############### Dataset: uci_german, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1489
AUC-ROC: 0.8121
AUC-PR: 0.6482

############### Dataset: uci_german, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2394
AUC-ROC: 0.7874
AUC-PR: 0.5093

############### Dataset: uci_german, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1567
AUC-ROC: 0.7931
AUC-PR: 0.5186

############### Dataset: uci_taiwan, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1358
AUC-ROC: 0.7786
AUC-PR: 0.5664

############### Dataset: uci_taiwan, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2470
AUC-ROC: 0.7929
AUC-PR: 0.5775

############### Dataset: uci_taiwan, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1711
AUC-ROC: 0.7986
AUC-PR: 0.5900

############### Dataset: uci_taiwan, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1804
AUC-ROC: 0.7869
AUC-PR: 0.5721

############### Dataset: uci_taiwan, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2469
AUC-ROC: 0.7934
AUC-PR: 0.5767

############### Dataset: uci_taiwan, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1733
AUC-ROC: 0.7992
AUC-PR: 0.5860

############### Dataset: uci_taiwan, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.3932
AUC-ROC: 0.6336
AUC-PR: 0.3352

############### Dataset: uci_taiwan, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2603
AUC-ROC: 0.6382
AUC-PR: 0.2960

############### Dataset: uci_taiwan, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.3404
AUC-ROC: 0.6587
AUC-PR: 0.3145

############### Dataset: uci_taiwan, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1559
AUC-ROC: 0.7860
AUC-PR: 0.5880

############### Dataset: uci_taiwan, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2436
AUC-ROC: 0.8009
AUC-PR: 0.5853

############### Dataset: uci_taiwan, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1470
AUC-ROC: 0.8021
AUC-PR: 0.5866

############### Dataset: uci_taiwan, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1331
AUC-ROC: 0.7831
AUC-PR: 0.5716

############### Dataset: uci_taiwan, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2436
AUC-ROC: 0.7951
AUC-PR: 0.5783

############### Dataset: uci_taiwan, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1279
AUC-ROC: 0.8012
AUC-PR: 0.5905

############### Dataset: uci_taiwan, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1502
AUC-ROC: 0.7675
AUC-PR: 0.5493

############### Dataset: uci_taiwan, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2467
AUC-ROC: 0.7713
AUC-PR: 0.5633

############### Dataset: uci_taiwan, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1577
AUC-ROC: 0.7862
AUC-PR: 0.5850

############### Dataset: uci_taiwan, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1537
AUC-ROC: 0.7670
AUC-PR: 0.5180

############### Dataset: uci_taiwan, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2474
AUC-ROC: 0.7592
AUC-PR: 0.5440

############### Dataset: uci_taiwan, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1698
AUC-ROC: 0.7836
AUC-PR: 0.5733

############### Dataset: uci_taiwan, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1506
AUC-ROC: 0.7733
AUC-PR: 0.5290

############### Dataset: uci_taiwan, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2475
AUC-ROC: 0.7806
AUC-PR: 0.5558

############### Dataset: uci_taiwan, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1731
AUC-ROC: 0.7876
AUC-PR: 0.5650

############### Dataset: uci_taiwan, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1726
AUC-ROC: 0.7749
AUC-PR: 0.5625

############### Dataset: uci_taiwan, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2453
AUC-ROC: 0.7813
AUC-PR: 0.5700

############### Dataset: uci_taiwan, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1789
AUC-ROC: 0.7891
AUC-PR: 0.5828

############### Dataset: uci_taiwan, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1516
AUC-ROC: 0.7630
AUC-PR: 0.5411

############### Dataset: uci_taiwan, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2465
AUC-ROC: 0.7646
AUC-PR: 0.5633

############### Dataset: uci_taiwan, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1576
AUC-ROC: 0.7854
AUC-PR: 0.5811

############### Dataset: uci_taiwan, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1546
AUC-ROC: 0.7614
AUC-PR: 0.5399

############### Dataset: uci_taiwan, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2473
AUC-ROC: 0.7601
AUC-PR: 0.5499

############### Dataset: uci_taiwan, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1677
AUC-ROC: 0.7803
AUC-PR: 0.5795

############### Dataset: pakdd, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1928
AUC-ROC: 0.6253
AUC-PR: 0.3474

############### Dataset: pakdd, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2494
AUC-ROC: 0.6398
AUC-PR: 0.3539

############### Dataset: pakdd, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2333
AUC-ROC: 0.6529
AUC-PR: 0.3671

############### Dataset: pakdd, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2409
AUC-ROC: 0.6247
AUC-PR: 0.3446

############### Dataset: pakdd, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2494
AUC-ROC: 0.6391
AUC-PR: 0.3528

############### Dataset: pakdd, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2346
AUC-ROC: 0.6525
AUC-PR: 0.3622

############### Dataset: pakdd, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.3210
AUC-ROC: 0.5483
AUC-PR: 0.2844

############### Dataset: pakdd, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2526
AUC-ROC: 0.5534
AUC-PR: 0.2812

############### Dataset: pakdd, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.3065
AUC-ROC: 0.5569
AUC-PR: 0.2893

############### Dataset: pakdd, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2367
AUC-ROC: 0.6366
AUC-PR: 0.3554

############### Dataset: pakdd, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2488
AUC-ROC: 0.6348
AUC-PR: 0.3512

############### Dataset: pakdd, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2194
AUC-ROC: 0.6473
AUC-PR: 0.3606

############### Dataset: pakdd, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1885
AUC-ROC: 0.6244
AUC-PR: 0.3499

############### Dataset: pakdd, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2471
AUC-ROC: 0.6361
AUC-PR: 0.3467

############### Dataset: pakdd, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1828
AUC-ROC: 0.6513
AUC-PR: 0.3661

############### Dataset: pakdd, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1909
AUC-ROC: 0.6186
AUC-PR: 0.3426

############### Dataset: pakdd, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2486
AUC-ROC: 0.6012
AUC-PR: 0.3290

############### Dataset: pakdd, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1922
AUC-ROC: 0.6351
AUC-PR: 0.3529

############### Dataset: pakdd, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1900
AUC-ROC: 0.6218
AUC-PR: 0.3458

############### Dataset: pakdd, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2485
AUC-ROC: 0.6114
AUC-PR: 0.3411

############### Dataset: pakdd, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1918
AUC-ROC: 0.6318
AUC-PR: 0.3502

############### Dataset: pakdd, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1915
AUC-ROC: 0.6232
AUC-PR: 0.3447

############### Dataset: pakdd, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2488
AUC-ROC: 0.6018
AUC-PR: 0.3259

############### Dataset: pakdd, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1956
AUC-ROC: 0.6294
AUC-PR: 0.3510

############### Dataset: pakdd, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2671
AUC-ROC: 0.6353
AUC-PR: 0.3554

############### Dataset: pakdd, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2503
AUC-ROC: 0.6221
AUC-PR: 0.3379

############### Dataset: pakdd, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2781
AUC-ROC: 0.6447
AUC-PR: 0.3671

############### Dataset: pakdd, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1901
AUC-ROC: 0.6207
AUC-PR: 0.3455

############### Dataset: pakdd, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2485
AUC-ROC: 0.6015
AUC-PR: 0.3285

############### Dataset: pakdd, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1915
AUC-ROC: 0.6330
AUC-PR: 0.3490

############### Dataset: pakdd, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1911
AUC-ROC: 0.6162
AUC-PR: 0.3446

############### Dataset: pakdd, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2487
AUC-ROC: 0.6049
AUC-PR: 0.3295

############### Dataset: pakdd, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1932
AUC-ROC: 0.6311
AUC-PR: 0.3484

############### Dataset: hmeq, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0574
AUC-ROC: 0.9697
AUC-PR: 0.9007

############### Dataset: hmeq, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2369
AUC-ROC: 0.9186
AUC-PR: 0.8294

############### Dataset: hmeq, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0920
AUC-ROC: 0.9207
AUC-PR: 0.8356

############### Dataset: hmeq, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0912
AUC-ROC: 0.9486
AUC-PR: 0.8532

############### Dataset: hmeq, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2371
AUC-ROC: 0.9162
AUC-PR: 0.8058

############### Dataset: hmeq, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0962
AUC-ROC: 0.9252
AUC-PR: 0.8354

############### Dataset: hmeq, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.2608
AUC-ROC: 0.8410
AUC-PR: 0.5857

############### Dataset: hmeq, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2557
AUC-ROC: 0.8027
AUC-PR: 0.4805

############### Dataset: hmeq, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.2311
AUC-ROC: 0.8537
AUC-PR: 0.6489

############### Dataset: hmeq, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0567
AUC-ROC: 0.9728
AUC-PR: 0.9166

############### Dataset: hmeq, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2328
AUC-ROC: 0.9110
AUC-PR: 0.8160

############### Dataset: hmeq, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0719
AUC-ROC: 0.9243
AUC-PR: 0.8419

############### Dataset: hmeq, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0553
AUC-ROC: 0.9733
AUC-PR: 0.9203

############### Dataset: hmeq, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2329
AUC-ROC: 0.9118
AUC-PR: 0.8180

############### Dataset: hmeq, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0711
AUC-ROC: 0.9196
AUC-PR: 0.8429

############### Dataset: hmeq, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0514
AUC-ROC: 0.9796
AUC-PR: 0.9340

############### Dataset: hmeq, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.9018
AUC-PR: 0.8018

############### Dataset: hmeq, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0862
AUC-ROC: 0.9191
AUC-PR: 0.8287

############### Dataset: hmeq, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0544
AUC-ROC: 0.9748
AUC-PR: 0.9217

############### Dataset: hmeq, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2396
AUC-ROC: 0.8913
AUC-PR: 0.7782

############### Dataset: hmeq, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0913
AUC-ROC: 0.9169
AUC-PR: 0.8187

############### Dataset: hmeq, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0516
AUC-ROC: 0.9791
AUC-PR: 0.9350

############### Dataset: hmeq, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2390
AUC-ROC: 0.8927
AUC-PR: 0.7677

############### Dataset: hmeq, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0948
AUC-ROC: 0.9199
AUC-PR: 0.8227

############### Dataset: hmeq, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0549
AUC-ROC: 0.9736
AUC-PR: 0.9095

############### Dataset: hmeq, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.8909
AUC-PR: 0.7867

############### Dataset: hmeq, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0850
AUC-ROC: 0.9157
AUC-PR: 0.8292

############### Dataset: hmeq, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0531
AUC-ROC: 0.9775
AUC-PR: 0.9277

############### Dataset: hmeq, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2387
AUC-ROC: 0.8960
AUC-PR: 0.7968

############### Dataset: hmeq, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0873
AUC-ROC: 0.9173
AUC-PR: 0.8288

############### Dataset: hmeq, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0527
AUC-ROC: 0.9786
AUC-PR: 0.9274

############### Dataset: hmeq, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2393
AUC-ROC: 0.8959
AUC-PR: 0.7875

############### Dataset: hmeq, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0874
AUC-ROC: 0.9206
AUC-PR: 0.8301

############### Dataset: gmsc, resampling method: ros, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0560
AUC-ROC: 0.8228
AUC-PR: 0.3177

############### Dataset: gmsc, resampling method: ros, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2461
AUC-ROC: 0.8257
AUC-PR: 0.3570

############### Dataset: gmsc, resampling method: ros, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1470
AUC-ROC: 0.8305
AUC-PR: 0.3744

############### Dataset: gmsc, resampling method: rus, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.1515
AUC-ROC: 0.8481
AUC-PR: 0.3592

############### Dataset: gmsc, resampling method: rus, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2461
AUC-ROC: 0.8257
AUC-PR: 0.3465

############### Dataset: gmsc, resampling method: rus, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1472
AUC-ROC: 0.8390
AUC-PR: 0.3755

############### Dataset: gmsc, resampling method: nearmiss, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.4779
AUC-ROC: 0.7510
AUC-PR: 0.3221

############### Dataset: gmsc, resampling method: nearmiss, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2756
AUC-ROC: 0.6883
AUC-PR: 0.1181

############### Dataset: gmsc, resampling method: nearmiss, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.4115
AUC-ROC: 0.7448
AUC-PR: 0.2189

############### Dataset: gmsc, resampling method: enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0551
AUC-ROC: 0.8403
AUC-PR: 0.3664

############### Dataset: gmsc, resampling method: enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2352
AUC-ROC: 0.8253
AUC-PR: 0.3681

############### Dataset: gmsc, resampling method: enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0532
AUC-ROC: 0.8308
AUC-PR: 0.3808

############### Dataset: gmsc, resampling method: tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0517
AUC-ROC: 0.8376
AUC-PR: 0.3601

############### Dataset: gmsc, resampling method: tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2349
AUC-ROC: 0.8250
AUC-PR: 0.3537

############### Dataset: gmsc, resampling method: tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0503
AUC-ROC: 0.8303
AUC-PR: 0.3851

############### Dataset: gmsc, resampling method: smote, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0608
AUC-ROC: 0.8246
AUC-PR: 0.3058

############### Dataset: gmsc, resampling method: smote, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2435
AUC-ROC: 0.8008
AUC-PR: 0.3265

############### Dataset: gmsc, resampling method: smote, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0835
AUC-ROC: 0.8188
AUC-PR: 0.3447

############### Dataset: gmsc, resampling method: smote_bs1, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0575
AUC-ROC: 0.8319
AUC-PR: 0.3150

############### Dataset: gmsc, resampling method: smote_bs1, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2417
AUC-ROC: 0.8083
AUC-PR: 0.3232

############### Dataset: gmsc, resampling method: smote_bs1, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0789
AUC-ROC: 0.8217
AUC-PR: 0.3369

############### Dataset: gmsc, resampling method: smote_bs2, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0587
AUC-ROC: 0.8352
AUC-PR: 0.3028

############### Dataset: gmsc, resampling method: smote_bs2, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2430
AUC-ROC: 0.8076
AUC-PR: 0.3258

############### Dataset: gmsc, resampling method: smote_bs2, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.1001
AUC-ROC: 0.8216
AUC-PR: 0.3409

############### Dataset: gmsc, resampling method: smote_enn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0680
AUC-ROC: 0.8406
AUC-PR: 0.3544

############### Dataset: gmsc, resampling method: smote_enn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2422
AUC-ROC: 0.8153
AUC-PR: 0.3428

############### Dataset: gmsc, resampling method: smote_enn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0918
AUC-ROC: 0.8286
AUC-PR: 0.3578

############### Dataset: gmsc, resampling method: smote_tomek, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0604
AUC-ROC: 0.8257
AUC-PR: 0.3066

############### Dataset: gmsc, resampling method: smote_tomek, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2433
AUC-ROC: 0.8097
AUC-PR: 0.3297

############### Dataset: gmsc, resampling method: smote_tomek, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0818
AUC-ROC: 0.8214
AUC-PR: 0.3473

############### Dataset: gmsc, resampling method: adasyn, classifier: Random Forest ###############
Test Set Evaluation:
Brier Score: 0.0620
AUC-ROC: 0.8184
AUC-PR: 0.3016

############### Dataset: gmsc, resampling method: adasyn, classifier: AdaBoost ###############
Test Set Evaluation:
Brier Score: 0.2441
AUC-ROC: 0.8033
AUC-PR: 0.3187

############### Dataset: gmsc, resampling method: adasyn, classifier: Gradient Boosting ###############
Test Set Evaluation:
Brier Score: 0.0890
AUC-ROC: 0.8159
AUC-PR: 0.3392

###########################################
###########################################
################## CWGAN ##################
###########################################
###########################################
############# uci_german #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.8333 AUC: 0.8933 BCE: 0.3867
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 15 iters per epoch to reach target of 4500.
[ 2500/4500] LG:0.612 LD:0.590 D:0.661 GP:0.005 AC: 0.475 RMSEAVG:0.088 NUM:0.207 SynTraiAuc:0.560 RFAcc:0.900
[ 4500/4500] LG:0.573 LD:0.442 D:0.493 GP:0.003 AC: 0.463 RMSEAVG:0.096 NUM:0.156 SynTraiAuc:0.560 RFAcc:0.913
INFO:root:Finished training after 4500/4500.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1575
AUC-ROC: 0.7817
AUC-PR: 0.5705

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2448
AUC-ROC: 0.7557
AUC-PR: 0.4182

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1592
AUC-ROC: 0.7859
AUC-PR: 0.5867

############# uci_taiwan #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.8291 AUC: 0.8034 BCE: 0.4093
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 422 iters per epoch to reach target of 126600.
[ 2500/126600] LG:1.296 LD:0.967 D:1.052 GP:0.006 AC: 0.347 RMSEAVG:0.040 NUM:0.018 SynTraiAuc:0.551 RFAcc:0.951
[ 5000/126600] LG:1.403 LD:1.107 D:1.165 GP:0.004 AC: 0.319 RMSEAVG:0.048 NUM:0.020 SynTraiAuc:0.735 RFAcc:0.943
[ 7500/126600] LG:1.309 LD:0.849 D:0.938 GP:0.006 AC: 0.325 RMSEAVG:0.051 NUM:0.016 SynTraiAuc:0.735 RFAcc:0.924
[10000/126600] LG:1.360 LD:0.586 D:0.641 GP:0.004 AC: 0.338 RMSEAVG:0.053 NUM:0.015 SynTraiAuc:0.757 RFAcc:0.894
[12500/126600] LG:1.594 LD:0.547 D:0.596 GP:0.003 AC: 0.327 RMSEAVG:0.039 NUM:0.019 SynTraiAuc:0.757 RFAcc:0.911
[15000/126600] LG:1.932 LD:0.574 D:0.619 GP:0.003 AC: 0.314 RMSEAVG:0.032 NUM:0.022 SynTraiAuc:0.733 RFAcc:0.891
[17500/126600] LG:2.146 LD:0.634 D:0.697 GP:0.004 AC: 0.316 RMSEAVG:0.028 NUM:0.016 SynTraiAuc:0.733 RFAcc:0.936
[20000/126600] LG:2.375 LD:0.637 D:0.695 GP:0.004 AC: 0.324 RMSEAVG:0.038 NUM:0.022 SynTraiAuc:0.758 RFAcc:0.929
[22500/126600] LG:2.631 LD:0.592 D:0.650 GP:0.004 AC: 0.328 RMSEAVG:0.035 NUM:0.018 SynTraiAuc:0.758 RFAcc:0.914
[25000/126600] LG:2.720 LD:0.691 D:0.744 GP:0.004 AC: 0.316 RMSEAVG:0.030 NUM:0.024 SynTraiAuc:0.766 RFAcc:0.875
[27500/126600] LG:2.751 LD:0.645 D:0.725 GP:0.005 AC: 0.333 RMSEAVG:0.032 NUM:0.024 SynTraiAuc:0.766 RFAcc:0.873
[30000/126600] LG:2.826 LD:0.772 D:0.803 GP:0.002 AC: 0.319 RMSEAVG:0.044 NUM:0.017 SynTraiAuc:0.737 RFAcc:0.881
[32500/126600] LG:3.081 LD:0.523 D:0.572 GP:0.003 AC: 0.324 RMSEAVG:0.043 NUM:0.022 SynTraiAuc:0.737 RFAcc:0.966
[35000/126600] LG:3.193 LD:0.658 D:0.736 GP:0.005 AC: 0.317 RMSEAVG:0.037 NUM:0.015 SynTraiAuc:0.734 RFAcc:0.855
[37500/126600] LG:3.215 LD:0.644 D:0.701 GP:0.004 AC: 0.321 RMSEAVG:0.037 NUM:0.025 SynTraiAuc:0.734 RFAcc:0.920
[40000/126600] LG:3.388 LD:0.471 D:0.501 GP:0.002 AC: 0.312 RMSEAVG:0.030 NUM:0.016 SynTraiAuc:0.742 RFAcc:0.929
[42500/126600] LG:3.539 LD:0.609 D:0.640 GP:0.002 AC: 0.316 RMSEAVG:0.041 NUM:0.019 SynTraiAuc:0.742 RFAcc:0.940
[45000/126600] LG:3.592 LD:0.555 D:0.590 GP:0.002 AC: 0.313 RMSEAVG:0.032 NUM:0.017 SynTraiAuc:0.763 RFAcc:0.874
[47500/126600] LG:3.669 LD:0.520 D:0.596 GP:0.005 AC: 0.309 RMSEAVG:0.032 NUM:0.018 SynTraiAuc:0.763 RFAcc:0.916
[50000/126600] LG:3.755 LD:0.388 D:0.421 GP:0.002 AC: 0.314 RMSEAVG:0.029 NUM:0.015 SynTraiAuc:0.733 RFAcc:0.873
[52500/126600] LG:3.744 LD:0.507 D:0.542 GP:0.002 AC: 0.311 RMSEAVG:0.035 NUM:0.024 SynTraiAuc:0.733 RFAcc:0.843
[55000/126600] LG:3.851 LD:0.607 D:0.653 GP:0.003 AC: 0.325 RMSEAVG:0.027 NUM:0.016 SynTraiAuc:0.748 RFAcc:0.971
[57500/126600] LG:3.885 LD:0.392 D:0.441 GP:0.003 AC: 0.313 RMSEAVG:0.034 NUM:0.026 SynTraiAuc:0.748 RFAcc:0.949
[60000/126600] LG:3.991 LD:0.316 D:0.369 GP:0.004 AC: 0.315 RMSEAVG:0.030 NUM:0.016 SynTraiAuc:0.733 RFAcc:0.936
[62500/126600] LG:4.064 LD:0.563 D:0.605 GP:0.003 AC: 0.319 RMSEAVG:0.026 NUM:0.020 SynTraiAuc:0.733 RFAcc:0.900
[65000/126600] LG:4.102 LD:0.491 D:0.524 GP:0.002 AC: 0.303 RMSEAVG:0.028 NUM:0.016 SynTraiAuc:0.734 RFAcc:0.921
[67500/126600] LG:4.081 LD:0.340 D:0.412 GP:0.005 AC: 0.322 RMSEAVG:0.029 NUM:0.022 SynTraiAuc:0.734 RFAcc:0.953
[70000/126600] LG:4.060 LD:0.508 D:0.543 GP:0.002 AC: 0.316 RMSEAVG:0.027 NUM:0.018 SynTraiAuc:0.747 RFAcc:0.900
[72500/126600] LG:4.106 LD:0.274 D:0.310 GP:0.002 AC: 0.326 RMSEAVG:0.034 NUM:0.015 SynTraiAuc:0.747 RFAcc:0.851
[75000/126600] LG:4.161 LD:0.393 D:0.435 GP:0.003 AC: 0.318 RMSEAVG:0.031 NUM:0.020 SynTraiAuc:0.735 RFAcc:0.929
[77500/126600] LG:4.091 LD:0.392 D:0.426 GP:0.002 AC: 0.334 RMSEAVG:0.026 NUM:0.014 SynTraiAuc:0.735 RFAcc:0.945
[80000/126600] LG:4.301 LD:0.429 D:0.473 GP:0.003 AC: 0.325 RMSEAVG:0.025 NUM:0.016 SynTraiAuc:0.722 RFAcc:0.881
[82500/126600] LG:4.225 LD:0.300 D:0.336 GP:0.002 AC: 0.315 RMSEAVG:0.029 NUM:0.025 SynTraiAuc:0.722 RFAcc:0.919
[85000/126600] LG:4.349 LD:0.283 D:0.307 GP:0.002 AC: 0.323 RMSEAVG:0.029 NUM:0.024 SynTraiAuc:0.741 RFAcc:0.968
[87500/126600] LG:4.336 LD:0.372 D:0.416 GP:0.003 AC: 0.312 RMSEAVG:0.023 NUM:0.024 SynTraiAuc:0.741 RFAcc:0.959
[90000/126600] LG:4.367 LD:0.445 D:0.482 GP:0.002 AC: 0.324 RMSEAVG:0.021 NUM:0.023 SynTraiAuc:0.746 RFAcc:0.941
[92500/126600] LG:4.406 LD:0.417 D:0.455 GP:0.003 AC: 0.315 RMSEAVG:0.023 NUM:0.022 SynTraiAuc:0.746 RFAcc:0.970
[95000/126600] LG:4.403 LD:0.261 D:0.293 GP:0.002 AC: 0.310 RMSEAVG:0.033 NUM:0.031 SynTraiAuc:0.728 RFAcc:0.955
[97500/126600] LG:4.469 LD:0.236 D:0.272 GP:0.002 AC: 0.326 RMSEAVG:0.026 NUM:0.022 SynTraiAuc:0.728 RFAcc:0.891
[100000/126600] LG:4.416 LD:0.265 D:0.295 GP:0.002 AC: 0.316 RMSEAVG:0.028 NUM:0.017 SynTraiAuc:0.728 RFAcc:0.910
[102500/126600] LG:4.469 LD:0.310 D:0.331 GP:0.001 AC: 0.310 RMSEAVG:0.026 NUM:0.014 SynTraiAuc:0.728 RFAcc:0.926
[105000/126600] LG:4.465 LD:0.280 D:0.302 GP:0.001 AC: 0.325 RMSEAVG:0.023 NUM:0.027 SynTraiAuc:0.750 RFAcc:0.986
[107500/126600] LG:4.446 LD:0.308 D:0.336 GP:0.002 AC: 0.316 RMSEAVG:0.026 NUM:0.014 SynTraiAuc:0.750 RFAcc:0.900
[110000/126600] LG:4.326 LD:0.257 D:0.285 GP:0.002 AC: 0.324 RMSEAVG:0.026 NUM:0.019 SynTraiAuc:0.721 RFAcc:0.906
[112500/126600] LG:4.295 LD:0.169 D:0.219 GP:0.003 AC: 0.324 RMSEAVG:0.025 NUM:0.025 SynTraiAuc:0.721 RFAcc:0.950
[115000/126600] LG:4.415 LD:0.260 D:0.282 GP:0.001 AC: 0.326 RMSEAVG:0.030 NUM:0.027 SynTraiAuc:0.740 RFAcc:0.985
[117500/126600] LG:4.298 LD:0.211 D:0.239 GP:0.002 AC: 0.328 RMSEAVG:0.025 NUM:0.027 SynTraiAuc:0.740 RFAcc:0.919
[120000/126600] LG:4.419 LD:0.282 D:0.301 GP:0.001 AC: 0.331 RMSEAVG:0.028 NUM:0.024 SynTraiAuc:0.701 RFAcc:0.932
[122500/126600] LG:4.409 LD:0.235 D:0.281 GP:0.003 AC: 0.316 RMSEAVG:0.026 NUM:0.025 SynTraiAuc:0.701 RFAcc:0.941
[125000/126600] LG:4.391 LD:0.267 D:0.298 GP:0.002 AC: 0.314 RMSEAVG:0.023 NUM:0.022 SynTraiAuc:0.758 RFAcc:0.917
[126600/126600] LG:4.438 LD:0.312 D:0.332 GP:0.001 AC: 0.323 RMSEAVG:0.024 NUM:0.030 SynTraiAuc:0.758 RFAcc:0.953
INFO:root:Finished training after 126600/126600.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1328
AUC-ROC: 0.7761
AUC-PR: 0.5704

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2435
AUC-ROC: 0.7924
AUC-PR: 0.5788

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1286
AUC-ROC: 0.8020
AUC-PR: 0.5899

############# pakdd #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.7664 AUC: 0.7501 BCE: 0.4927
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 704 iters per epoch to reach target of 211200.
[ 2500/211200] LG:0.138 LD:0.978 D:1.021 GP:0.003 AC: 0.770 RMSEAVG:0.049 NUM:0.044 SynTraiAuc:0.517 RFAcc:0.986
[ 5000/211200] LG:0.380 LD:1.354 D:1.416 GP:0.004 AC: 0.418 RMSEAVG:0.040 NUM:0.017 SynTraiAuc:0.562 RFAcc:0.975
[ 7500/211200] LG:0.491 LD:1.336 D:1.399 GP:0.004 AC: 0.392 RMSEAVG:0.026 NUM:0.021 SynTraiAuc:0.562 RFAcc:0.966
[10000/211200] LG:0.749 LD:1.413 D:1.503 GP:0.006 AC: 0.366 RMSEAVG:0.024 NUM:0.017 SynTraiAuc:0.577 RFAcc:0.948
[12500/211200] LG:0.918 LD:1.183 D:1.278 GP:0.006 AC: 0.351 RMSEAVG:0.018 NUM:0.023 SynTraiAuc:0.577 RFAcc:0.963
[15000/211200] LG:1.003 LD:1.226 D:1.310 GP:0.006 AC: 0.361 RMSEAVG:0.018 NUM:0.040 SynTraiAuc:0.571 RFAcc:0.930
[17500/211200] LG:0.974 LD:1.304 D:1.377 GP:0.005 AC: 0.341 RMSEAVG:0.020 NUM:0.016 SynTraiAuc:0.571 RFAcc:0.966
[20000/211200] LG:1.056 LD:1.097 D:1.169 GP:0.005 AC: 0.375 RMSEAVG:0.016 NUM:0.024 SynTraiAuc:0.568 RFAcc:0.943
[22500/211200] LG:0.971 LD:1.135 D:1.215 GP:0.005 AC: 0.338 RMSEAVG:0.014 NUM:0.022 SynTraiAuc:0.568 RFAcc:0.927
[25000/211200] LG:1.073 LD:1.241 D:1.315 GP:0.005 AC: 0.372 RMSEAVG:0.014 NUM:0.023 SynTraiAuc:0.568 RFAcc:0.973
[27500/211200] LG:1.150 LD:1.284 D:1.372 GP:0.006 AC: 0.337 RMSEAVG:0.014 NUM:0.024 SynTraiAuc:0.568 RFAcc:0.939
[30000/211200] LG:1.186 LD:1.220 D:1.272 GP:0.003 AC: 0.341 RMSEAVG:0.016 NUM:0.020 SynTraiAuc:0.575 RFAcc:0.930
[32500/211200] LG:1.126 LD:1.262 D:1.364 GP:0.007 AC: 0.384 RMSEAVG:0.013 NUM:0.008 SynTraiAuc:0.575 RFAcc:0.990
[35000/211200] LG:1.084 LD:1.056 D:1.119 GP:0.004 AC: 0.384 RMSEAVG:0.017 NUM:0.027 SynTraiAuc:0.560 RFAcc:0.975
[37500/211200] LG:1.179 LD:1.109 D:1.223 GP:0.008 AC: 0.360 RMSEAVG:0.017 NUM:0.046 SynTraiAuc:0.560 RFAcc:0.961
[40000/211200] LG:1.107 LD:0.972 D:1.080 GP:0.007 AC: 0.346 RMSEAVG:0.017 NUM:0.021 SynTraiAuc:0.570 RFAcc:0.959
[42500/211200] LG:1.232 LD:0.986 D:1.136 GP:0.010 AC: 0.349 RMSEAVG:0.020 NUM:0.035 SynTraiAuc:0.570 RFAcc:0.932
[45000/211200] LG:1.199 LD:0.979 D:1.044 GP:0.004 AC: 0.458 RMSEAVG:0.017 NUM:0.044 SynTraiAuc:0.528 RFAcc:0.984
[47500/211200] LG:1.161 LD:0.871 D:0.957 GP:0.006 AC: 0.370 RMSEAVG:0.021 NUM:0.014 SynTraiAuc:0.528 RFAcc:0.958
[50000/211200] LG:1.222 LD:0.877 D:0.955 GP:0.005 AC: 0.338 RMSEAVG:0.021 NUM:0.032 SynTraiAuc:0.533 RFAcc:0.861
[52500/211200] LG:1.190 LD:0.926 D:0.991 GP:0.004 AC: 0.378 RMSEAVG:0.016 NUM:0.024 SynTraiAuc:0.533 RFAcc:0.973
[55000/211200] LG:1.275 LD:0.883 D:0.945 GP:0.004 AC: 0.381 RMSEAVG:0.012 NUM:0.020 SynTraiAuc:0.537 RFAcc:0.989
[57500/211200] LG:1.135 LD:0.846 D:0.958 GP:0.007 AC: 0.343 RMSEAVG:0.011 NUM:0.030 SynTraiAuc:0.537 RFAcc:0.970
[60000/211200] LG:1.302 LD:0.898 D:0.999 GP:0.007 AC: 0.355 RMSEAVG:0.013 NUM:0.033 SynTraiAuc:0.547 RFAcc:0.995
[62500/211200] LG:1.325 LD:0.700 D:0.762 GP:0.004 AC: 0.358 RMSEAVG:0.012 NUM:0.031 SynTraiAuc:0.547 RFAcc:0.993
[65000/211200] LG:1.412 LD:0.682 D:0.775 GP:0.006 AC: 0.364 RMSEAVG:0.013 NUM:0.026 SynTraiAuc:0.561 RFAcc:0.988
[67500/211200] LG:1.505 LD:0.764 D:0.860 GP:0.006 AC: 0.358 RMSEAVG:0.015 NUM:0.044 SynTraiAuc:0.561 RFAcc:0.894
[70000/211200] LG:1.509 LD:0.682 D:0.746 GP:0.004 AC: 0.358 RMSEAVG:0.012 NUM:0.021 SynTraiAuc:0.559 RFAcc:0.975
[72500/211200] LG:1.547 LD:0.734 D:0.801 GP:0.004 AC: 0.364 RMSEAVG:0.016 NUM:0.040 SynTraiAuc:0.559 RFAcc:0.993
[75000/211200] LG:1.519 LD:0.691 D:0.766 GP:0.005 AC: 0.336 RMSEAVG:0.012 NUM:0.034 SynTraiAuc:0.549 RFAcc:0.979
[77500/211200] LG:1.596 LD:0.705 D:0.751 GP:0.003 AC: 0.402 RMSEAVG:0.013 NUM:0.032 SynTraiAuc:0.549 RFAcc:0.991
[80000/211200] LG:1.475 LD:0.521 D:0.561 GP:0.003 AC: 0.361 RMSEAVG:0.013 NUM:0.023 SynTraiAuc:0.550 RFAcc:0.990
[82500/211200] LG:1.618 LD:0.818 D:0.846 GP:0.002 AC: 0.333 RMSEAVG:0.012 NUM:0.039 SynTraiAuc:0.550 RFAcc:0.984
[85000/211200] LG:1.692 LD:0.619 D:0.691 GP:0.005 AC: 0.351 RMSEAVG:0.014 NUM:0.037 SynTraiAuc:0.559 RFAcc:0.978
[87500/211200] LG:1.535 LD:0.625 D:0.724 GP:0.007 AC: 0.340 RMSEAVG:0.012 NUM:0.041 SynTraiAuc:0.559 RFAcc:0.998
[90000/211200] LG:1.616 LD:0.628 D:0.673 GP:0.003 AC: 0.351 RMSEAVG:0.016 NUM:0.031 SynTraiAuc:0.563 RFAcc:0.969
[92500/211200] LG:1.643 LD:0.522 D:0.566 GP:0.003 AC: 0.373 RMSEAVG:0.013 NUM:0.028 SynTraiAuc:0.563 RFAcc:0.995
[95000/211200] LG:1.554 LD:0.552 D:0.585 GP:0.002 AC: 0.380 RMSEAVG:0.013 NUM:0.033 SynTraiAuc:0.556 RFAcc:0.969
[97500/211200] LG:1.572 LD:0.682 D:0.743 GP:0.004 AC: 0.356 RMSEAVG:0.013 NUM:0.034 SynTraiAuc:0.556 RFAcc:0.993
[100000/211200] LG:1.547 LD:0.611 D:0.694 GP:0.006 AC: 0.371 RMSEAVG:0.013 NUM:0.042 SynTraiAuc:0.547 RFAcc:0.996
[102500/211200] LG:1.538 LD:0.575 D:0.611 GP:0.002 AC: 0.375 RMSEAVG:0.011 NUM:0.030 SynTraiAuc:0.547 RFAcc:0.950
[105000/211200] LG:1.432 LD:0.414 D:0.466 GP:0.003 AC: 0.365 RMSEAVG:0.014 NUM:0.046 SynTraiAuc:0.542 RFAcc:0.985
[107500/211200] LG:1.604 LD:0.712 D:0.744 GP:0.002 AC: 0.374 RMSEAVG:0.014 NUM:0.030 SynTraiAuc:0.542 RFAcc:0.988
[110000/211200] LG:1.452 LD:0.594 D:0.627 GP:0.002 AC: 0.370 RMSEAVG:0.013 NUM:0.040 SynTraiAuc:0.556 RFAcc:0.990
[112500/211200] LG:1.473 LD:0.554 D:0.637 GP:0.006 AC: 0.375 RMSEAVG:0.013 NUM:0.036 SynTraiAuc:0.556 RFAcc:0.960
[115000/211200] LG:1.514 LD:0.502 D:0.557 GP:0.004 AC: 0.358 RMSEAVG:0.015 NUM:0.050 SynTraiAuc:0.537 RFAcc:0.826
[117500/211200] LG:1.490 LD:0.521 D:0.582 GP:0.004 AC: 0.391 RMSEAVG:0.014 NUM:0.041 SynTraiAuc:0.537 RFAcc:0.969
[120000/211200] LG:1.583 LD:0.537 D:0.586 GP:0.003 AC: 0.394 RMSEAVG:0.013 NUM:0.029 SynTraiAuc:0.562 RFAcc:0.994
[122500/211200] LG:1.459 LD:0.528 D:0.574 GP:0.003 AC: 0.371 RMSEAVG:0.015 NUM:0.041 SynTraiAuc:0.562 RFAcc:0.990
[125000/211200] LG:1.498 LD:0.485 D:0.564 GP:0.005 AC: 0.358 RMSEAVG:0.014 NUM:0.032 SynTraiAuc:0.537 RFAcc:0.989
[127500/211200] LG:1.616 LD:0.545 D:0.590 GP:0.003 AC: 0.386 RMSEAVG:0.013 NUM:0.027 SynTraiAuc:0.537 RFAcc:0.991
[130000/211200] LG:1.539 LD:0.390 D:0.419 GP:0.002 AC: 0.364 RMSEAVG:0.016 NUM:0.025 SynTraiAuc:0.551 RFAcc:0.990
[132500/211200] LG:1.592 LD:0.415 D:0.463 GP:0.003 AC: 0.436 RMSEAVG:0.015 NUM:0.034 SynTraiAuc:0.551 RFAcc:0.945
[135000/211200] LG:1.607 LD:0.632 D:0.674 GP:0.003 AC: 0.339 RMSEAVG:0.013 NUM:0.032 SynTraiAuc:0.549 RFAcc:0.939
[137500/211200] LG:1.586 LD:0.564 D:0.599 GP:0.002 AC: 0.358 RMSEAVG:0.015 NUM:0.036 SynTraiAuc:0.549 RFAcc:0.969
[140000/211200] LG:1.539 LD:0.479 D:0.534 GP:0.004 AC: 0.375 RMSEAVG:0.013 NUM:0.032 SynTraiAuc:0.572 RFAcc:0.979
[142500/211200] LG:1.599 LD:0.566 D:0.604 GP:0.003 AC: 0.357 RMSEAVG:0.015 NUM:0.037 SynTraiAuc:0.572 RFAcc:0.990
[145000/211200] LG:1.654 LD:0.537 D:0.572 GP:0.002 AC: 0.357 RMSEAVG:0.013 NUM:0.034 SynTraiAuc:0.551 RFAcc:0.980
[147500/211200] LG:1.676 LD:0.499 D:0.549 GP:0.003 AC: 0.345 RMSEAVG:0.012 NUM:0.034 SynTraiAuc:0.551 RFAcc:1.000
[150000/211200] LG:1.744 LD:0.487 D:0.524 GP:0.003 AC: 0.352 RMSEAVG:0.013 NUM:0.046 SynTraiAuc:0.539 RFAcc:0.966
[152500/211200] LG:1.742 LD:0.473 D:0.526 GP:0.004 AC: 0.392 RMSEAVG:0.015 NUM:0.036 SynTraiAuc:0.539 RFAcc:0.999
[155000/211200] LG:1.657 LD:0.517 D:0.561 GP:0.003 AC: 0.339 RMSEAVG:0.013 NUM:0.037 SynTraiAuc:0.537 RFAcc:0.998
[157500/211200] LG:1.815 LD:0.476 D:0.511 GP:0.002 AC: 0.355 RMSEAVG:0.014 NUM:0.042 SynTraiAuc:0.537 RFAcc:0.999
[160000/211200] LG:1.784 LD:0.458 D:0.523 GP:0.004 AC: 0.373 RMSEAVG:0.012 NUM:0.031 SynTraiAuc:0.539 RFAcc:0.991
[162500/211200] LG:1.838 LD:0.444 D:0.537 GP:0.006 AC: 0.395 RMSEAVG:0.012 NUM:0.049 SynTraiAuc:0.539 RFAcc:0.999
[165000/211200] LG:1.941 LD:0.436 D:0.490 GP:0.004 AC: 0.358 RMSEAVG:0.011 NUM:0.036 SynTraiAuc:0.533 RFAcc:0.990
[167500/211200] LG:1.965 LD:0.472 D:0.537 GP:0.004 AC: 0.368 RMSEAVG:0.012 NUM:0.036 SynTraiAuc:0.533 RFAcc:0.980
[170000/211200] LG:1.899 LD:0.534 D:0.577 GP:0.003 AC: 0.395 RMSEAVG:0.010 NUM:0.036 SynTraiAuc:0.547 RFAcc:0.998
[172500/211200] LG:1.987 LD:0.369 D:0.400 GP:0.002 AC: 0.358 RMSEAVG:0.010 NUM:0.034 SynTraiAuc:0.547 RFAcc:0.994
[175000/211200] LG:1.945 LD:0.412 D:0.456 GP:0.003 AC: 0.360 RMSEAVG:0.013 NUM:0.049 SynTraiAuc:0.545 RFAcc:0.961
[177500/211200] LG:1.961 LD:0.436 D:0.494 GP:0.004 AC: 0.382 RMSEAVG:0.013 NUM:0.048 SynTraiAuc:0.545 RFAcc:0.995
[180000/211200] LG:1.921 LD:0.442 D:0.488 GP:0.003 AC: 0.360 RMSEAVG:0.010 NUM:0.033 SynTraiAuc:0.552 RFAcc:0.897
[182500/211200] LG:1.940 LD:0.533 D:0.583 GP:0.003 AC: 0.378 RMSEAVG:0.012 NUM:0.040 SynTraiAuc:0.552 RFAcc:0.994
[185000/211200] LG:1.918 LD:0.411 D:0.467 GP:0.004 AC: 0.345 RMSEAVG:0.015 NUM:0.037 SynTraiAuc:0.560 RFAcc:0.994
[187500/211200] LG:1.883 LD:0.419 D:0.454 GP:0.002 AC: 0.357 RMSEAVG:0.010 NUM:0.039 SynTraiAuc:0.560 RFAcc:0.996
[190000/211200] LG:1.978 LD:0.467 D:0.507 GP:0.003 AC: 0.348 RMSEAVG:0.014 NUM:0.038 SynTraiAuc:0.559 RFAcc:0.954
[192500/211200] LG:1.854 LD:0.459 D:0.491 GP:0.002 AC: 0.372 RMSEAVG:0.013 NUM:0.054 SynTraiAuc:0.559 RFAcc:0.991
[195000/211200] LG:1.918 LD:0.555 D:0.597 GP:0.003 AC: 0.339 RMSEAVG:0.013 NUM:0.042 SynTraiAuc:0.551 RFAcc:0.922
[197500/211200] LG:1.856 LD:0.382 D:0.432 GP:0.003 AC: 0.331 RMSEAVG:0.013 NUM:0.040 SynTraiAuc:0.551 RFAcc:0.974
[200000/211200] LG:1.894 LD:0.414 D:0.495 GP:0.005 AC: 0.355 RMSEAVG:0.012 NUM:0.035 SynTraiAuc:0.547 RFAcc:0.998
[202500/211200] LG:1.878 LD:0.476 D:0.505 GP:0.002 AC: 0.386 RMSEAVG:0.013 NUM:0.043 SynTraiAuc:0.547 RFAcc:0.995
[205000/211200] LG:1.883 LD:0.445 D:0.473 GP:0.002 AC: 0.330 RMSEAVG:0.013 NUM:0.043 SynTraiAuc:0.557 RFAcc:0.988
[207500/211200] LG:1.919 LD:0.426 D:0.478 GP:0.004 AC: 0.338 RMSEAVG:0.012 NUM:0.035 SynTraiAuc:0.557 RFAcc:0.996
[210000/211200] LG:1.979 LD:0.423 D:0.472 GP:0.003 AC: 0.349 RMSEAVG:0.014 NUM:0.031 SynTraiAuc:0.553 RFAcc:0.989
[211200/211200] LG:1.960 LD:0.330 D:0.337 GP:0.000 AC: 0.365 RMSEAVG:0.014 NUM:0.031 SynTraiAuc:0.553 RFAcc:0.989
INFO:root:Finished training after 211200/211200.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1874
AUC-ROC: 0.6208
AUC-PR: 0.3479

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2469
AUC-ROC: 0.6360
AUC-PR: 0.3481

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1831
AUC-ROC: 0.6496
AUC-PR: 0.3674

############# hmeq #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.8904 AUC: 0.9100 BCE: 0.2804
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 84 iters per epoch to reach target of 25200.
[ 2500/25200] LG:0.511 LD:0.251 D:0.289 GP:0.003 AC: 0.327 RMSEAVG:0.079 NUM:0.033 SynTraiAuc:0.515 RFAcc:0.806
[ 5000/25200] LG:0.503 LD:0.227 D:0.254 GP:0.002 AC: 0.396 RMSEAVG:0.047 NUM:0.021 SynTraiAuc:0.787 RFAcc:0.738
[ 7500/25200] LG:0.489 LD:0.278 D:0.300 GP:0.001 AC: 0.384 RMSEAVG:0.028 NUM:0.018 SynTraiAuc:0.787 RFAcc:0.756
[10000/25200] LG:0.373 LD:0.255 D:0.288 GP:0.002 AC: 0.367 RMSEAVG:0.025 NUM:0.018 SynTraiAuc:0.803 RFAcc:0.767
[12500/25200] LG:0.375 LD:0.292 D:0.307 GP:0.001 AC: 0.308 RMSEAVG:0.028 NUM:0.020 SynTraiAuc:0.803 RFAcc:0.828
[15000/25200] LG:0.319 LD:0.232 D:0.256 GP:0.002 AC: 0.354 RMSEAVG:0.022 NUM:0.015 SynTraiAuc:0.793 RFAcc:0.749
[17500/25200] LG:0.187 LD:0.256 D:0.269 GP:0.001 AC: 0.364 RMSEAVG:0.024 NUM:0.019 SynTraiAuc:0.793 RFAcc:0.777
[20000/25200] LG:0.119 LD:0.220 D:0.230 GP:0.001 AC: 0.518 RMSEAVG:0.024 NUM:0.023 SynTraiAuc:0.752 RFAcc:0.726
[22500/25200] LG:0.131 LD:0.226 D:0.237 GP:0.001 AC: 0.554 RMSEAVG:0.035 NUM:0.016 SynTraiAuc:0.752 RFAcc:0.729
[25000/25200] LG:0.109 LD:0.201 D:0.216 GP:0.001 AC: 0.590 RMSEAVG:0.024 NUM:0.018 SynTraiAuc:0.776 RFAcc:0.685
[25200/25200] LG:0.061 LD:0.182 D:0.197 GP:0.001 AC: 0.467 RMSEAVG:0.024 NUM:0.018 SynTraiAuc:0.776 RFAcc:0.685
INFO:root:Finished training after 25200/25200.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0513
AUC-ROC: 0.9782
AUC-PR: 0.9361

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2356
AUC-ROC: 0.8839
AUC-PR: 0.7787

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0757
AUC-ROC: 0.9059
AUC-PR: 0.8235

############# gmsc #############
DEBUG:root:GAN initilisation finished.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no netG during init. Initialising now.
DEBUG:root:GAN got no auxillary classifier during init, but use_aux_classifier_loss is True. Initialising now.
DEBUG:root:Using an auxiliary classifier loss. Starting pretraining.
INFO:root:Finished training auxiliary classifier. ACC: 0.9363 AUC: 0.8349 BCE: 0.1876
INFO:root:Pretrained the aux classifier. Proceeding to training GAN or aux teacher if used.
INFO:root:Starting training. Expecting to train for 300 epochs at 2110 iters per epoch to reach target of 633000.
[ 2500/633000] LG:-0.604 LD:0.010 D:0.021 GP:0.001 AC: 0.448 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.570 RFAcc:0.943
[ 5000/633000] LG:-0.500 LD:0.013 D:0.016 GP:0.000 AC: 0.303 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.687 RFAcc:0.969
[ 7500/633000] LG:-0.417 LD:0.006 D:0.013 GP:0.000 AC: 0.321 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.687 RFAcc:0.956
[10000/633000] LG:-0.383 LD:0.012 D:0.015 GP:0.000 AC: 0.310 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.761 RFAcc:0.907
[12500/633000] LG:-0.327 LD:0.023 D:0.029 GP:0.000 AC: 0.317 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.761 RFAcc:0.963
[15000/633000] LG:-0.250 LD:0.006 D:0.011 GP:0.000 AC: 0.333 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.790 RFAcc:0.953
[17500/633000] LG:-0.220 LD:0.003 D:0.005 GP:0.000 AC: 0.413 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.790 RFAcc:0.911
[20000/633000] LG:-0.254 LD:0.005 D:0.008 GP:0.000 AC: 0.350 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.774 RFAcc:0.959
[22500/633000] LG:-0.267 LD:-0.006 D:0.005 GP:0.001 AC: 0.344 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.774 RFAcc:0.949
[25000/633000] LG:-0.246 LD:0.001 D:0.003 GP:0.000 AC: 0.317 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.781 RFAcc:0.964
[27500/633000] LG:-0.195 LD:0.023 D:0.026 GP:0.000 AC: 0.350 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.781 RFAcc:0.958
[30000/633000] LG:-0.199 LD:-0.004 D:0.002 GP:0.000 AC: 0.347 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.769 RFAcc:0.950
[32500/633000] LG:-0.244 LD:-0.000 D:0.001 GP:0.000 AC: 0.337 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.769 RFAcc:0.910
[35000/633000] LG:-0.191 LD:0.021 D:0.025 GP:0.000 AC: 0.334 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.765 RFAcc:0.941
[37500/633000] LG:-0.195 LD:-0.004 D:0.000 GP:0.000 AC: 0.337 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.765 RFAcc:0.932
[40000/633000] LG:-0.262 LD:0.000 D:0.002 GP:0.000 AC: 0.307 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.760 RFAcc:0.964
[42500/633000] LG:-0.251 LD:-0.002 D:-0.001 GP:0.000 AC: 0.317 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.760 RFAcc:0.949
[45000/633000] LG:-0.188 LD:-0.001 D:0.001 GP:0.000 AC: 0.347 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.761 RFAcc:0.969
[47500/633000] LG:-0.282 LD:0.000 D:0.001 GP:0.000 AC: 0.317 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.761 RFAcc:0.876
[50000/633000] LG:-0.233 LD:-0.003 D:0.001 GP:0.000 AC: 0.320 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.794 RFAcc:0.896
[52500/633000] LG:-0.233 LD:0.001 D:0.003 GP:0.000 AC: 0.321 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.794 RFAcc:0.970
[55000/633000] LG:-0.230 LD:0.001 D:0.003 GP:0.000 AC: 0.320 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.714 RFAcc:0.953
[57500/633000] LG:-0.248 LD:0.002 D:0.003 GP:0.000 AC: 0.328 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.714 RFAcc:0.951
[60000/633000] LG:-0.273 LD:0.001 D:0.002 GP:0.000 AC: 0.303 RMSEAVG:0.004 NUM:0.004 SynTraiAuc:0.780 RFAcc:0.961
[62500/633000] LG:-0.284 LD:0.001 D:0.002 GP:0.000 AC: 0.301 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.780 RFAcc:0.935
[65000/633000] LG:-0.216 LD:-0.002 D:0.002 GP:0.000 AC: 0.317 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.764 RFAcc:0.950
[67500/633000] LG:-0.237 LD:-0.003 D:-0.002 GP:0.000 AC: 0.315 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.764 RFAcc:0.934
[70000/633000] LG:-0.240 LD:-0.001 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.004 NUM:0.004 SynTraiAuc:0.768 RFAcc:0.926
[72500/633000] LG:-0.233 LD:-0.001 D:0.002 GP:0.000 AC: 0.312 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.768 RFAcc:0.960
[75000/633000] LG:-0.253 LD:0.001 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.772 RFAcc:0.979
[77500/633000] LG:-0.230 LD:0.002 D:0.003 GP:0.000 AC: 0.317 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.772 RFAcc:0.911
[80000/633000] LG:-0.234 LD:0.023 D:0.024 GP:0.000 AC: 0.323 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.729 RFAcc:0.927
[82500/633000] LG:-0.247 LD:-0.002 D:-0.001 GP:0.000 AC: 0.304 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.729 RFAcc:0.966
[85000/633000] LG:-0.184 LD:-0.001 D:0.002 GP:0.000 AC: 0.327 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.789 RFAcc:0.932
[87500/633000] LG:-0.221 LD:0.001 D:0.002 GP:0.000 AC: 0.313 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.789 RFAcc:0.944
[90000/633000] LG:-0.256 LD:-0.011 D:-0.003 GP:0.001 AC: 0.300 RMSEAVG:0.004 NUM:0.004 SynTraiAuc:0.789 RFAcc:0.906
[92500/633000] LG:-0.184 LD:0.022 D:0.027 GP:0.000 AC: 0.320 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.789 RFAcc:0.934
[95000/633000] LG:-0.219 LD:0.000 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.792 RFAcc:0.961
[97500/633000] LG:-0.143 LD:0.027 D:0.028 GP:0.000 AC: 0.362 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.792 RFAcc:0.954
[100000/633000] LG:-0.172 LD:0.023 D:0.027 GP:0.000 AC: 0.305 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.790 RFAcc:0.941
[102500/633000] LG:-0.149 LD:-0.004 D:-0.002 GP:0.000 AC: 0.338 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.790 RFAcc:0.920
[105000/633000] LG:-0.100 LD:-0.003 D:-0.000 GP:0.000 AC: 0.328 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.751 RFAcc:0.940
[107500/633000] LG:-0.134 LD:0.002 D:0.003 GP:0.000 AC: 0.330 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.751 RFAcc:0.954
[110000/633000] LG:-0.037 LD:-0.007 D:-0.001 GP:0.000 AC: 0.406 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.779 RFAcc:0.890
[112500/633000] LG:-0.123 LD:-0.001 D:0.000 GP:0.000 AC: 0.339 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.779 RFAcc:0.899
[115000/633000] LG:-0.074 LD:-0.002 D:0.000 GP:0.000 AC: 0.353 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.763 RFAcc:0.939
[117500/633000] LG:-0.087 LD:-0.005 D:-0.003 GP:0.000 AC: 0.320 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.763 RFAcc:0.925
[120000/633000] LG:-0.057 LD:-0.013 D:-0.002 GP:0.001 AC: 0.379 RMSEAVG:0.003 NUM:0.003 SynTraiAuc:0.757 RFAcc:0.956
[122500/633000] LG:-0.061 LD:-0.002 D:-0.001 GP:0.000 AC: 0.394 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.757 RFAcc:0.946
[125000/633000] LG:-0.020 LD:-0.001 D:0.003 GP:0.000 AC: 0.350 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.547 RFAcc:0.983
[127500/633000] LG:-0.007 LD:-0.005 D:-0.002 GP:0.000 AC: 0.327 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.547 RFAcc:0.960
[130000/633000] LG:0.148 LD:-0.002 D:-0.000 GP:0.000 AC: 0.366 RMSEAVG:0.026 NUM:0.026 SynTraiAuc:0.679 RFAcc:0.954
[132500/633000] LG:0.036 LD:-0.005 D:-0.002 GP:0.000 AC: 0.782 RMSEAVG:0.077 NUM:0.077 SynTraiAuc:0.679 RFAcc:0.925
[135000/633000] LG:0.196 LD:-0.007 D:0.003 GP:0.001 AC: 1.060 RMSEAVG:0.143 NUM:0.143 SynTraiAuc:0.520 RFAcc:0.924
[137500/633000] LG:0.087 LD:-0.001 D:0.000 GP:0.000 AC: 0.312 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.520 RFAcc:0.969
[140000/633000] LG:0.167 LD:-0.008 D:-0.001 GP:0.000 AC: 0.379 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.786 RFAcc:0.929
[142500/633000] LG:0.036 LD:-0.003 D:0.001 GP:0.000 AC: 0.359 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.786 RFAcc:0.929
[145000/633000] LG:0.081 LD:0.001 D:0.003 GP:0.000 AC: 0.350 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.599 RFAcc:0.940
[147500/633000] LG:0.155 LD:0.013 D:0.015 GP:0.000 AC: 0.303 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.599 RFAcc:0.955
[150000/633000] LG:0.177 LD:-0.000 D:0.000 GP:0.000 AC: 0.313 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.738 RFAcc:0.954
[152500/633000] LG:0.143 LD:0.000 D:0.001 GP:0.000 AC: 0.318 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.738 RFAcc:0.963
[155000/633000] LG:0.146 LD:-0.002 D:-0.001 GP:0.000 AC: 0.321 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.765 RFAcc:0.926
[157500/633000] LG:0.197 LD:-0.001 D:0.002 GP:0.000 AC: 0.355 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.765 RFAcc:0.946
[160000/633000] LG:0.274 LD:-0.003 D:-0.001 GP:0.000 AC: 0.338 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.759 RFAcc:0.954
[162500/633000] LG:0.271 LD:-0.000 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.759 RFAcc:0.963
[165000/633000] LG:0.340 LD:0.024 D:0.026 GP:0.000 AC: 0.317 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.783 RFAcc:0.970
[167500/633000] LG:0.316 LD:-0.003 D:0.001 GP:0.000 AC: 0.338 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.783 RFAcc:0.934
[170000/633000] LG:0.202 LD:-0.002 D:-0.001 GP:0.000 AC: 0.338 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.773 RFAcc:0.887
[172500/633000] LG:0.304 LD:0.017 D:0.026 GP:0.001 AC: 0.314 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.773 RFAcc:0.973
[175000/633000] LG:0.244 LD:-0.002 D:-0.000 GP:0.000 AC: 0.311 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.788 RFAcc:0.935
[177500/633000] LG:0.268 LD:0.004 D:0.005 GP:0.000 AC: 0.304 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.788 RFAcc:0.964
[180000/633000] LG:0.272 LD:-0.002 D:-0.001 GP:0.000 AC: 0.316 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.800 RFAcc:0.924
[182500/633000] LG:0.249 LD:-0.010 D:-0.003 GP:0.000 AC: 0.322 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.800 RFAcc:0.986
[185000/633000] LG:0.298 LD:0.002 D:0.002 GP:0.000 AC: 0.354 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.771 RFAcc:0.931
[187500/633000] LG:0.241 LD:-0.000 D:0.000 GP:0.000 AC: 0.328 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.771 RFAcc:0.950
[190000/633000] LG:0.199 LD:-0.000 D:0.000 GP:0.000 AC: 0.330 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.762 RFAcc:0.984
[192500/633000] LG:0.181 LD:-0.000 D:0.000 GP:0.000 AC: 0.322 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.762 RFAcc:0.958
[195000/633000] LG:0.160 LD:-0.001 D:-0.000 GP:0.000 AC: 0.318 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.801 RFAcc:0.924
[197500/633000] LG:0.193 LD:0.019 D:0.022 GP:0.000 AC: 0.333 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.801 RFAcc:0.929
[200000/633000] LG:0.197 LD:-0.004 D:-0.003 GP:0.000 AC: 0.337 RMSEAVG:0.034 NUM:0.034 SynTraiAuc:0.759 RFAcc:0.974
[202500/633000] LG:0.237 LD:-0.001 D:-0.001 GP:0.000 AC: 0.344 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.759 RFAcc:0.927
[205000/633000] LG:0.164 LD:-0.000 D:0.000 GP:0.000 AC: 0.377 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.797 RFAcc:0.964
[207500/633000] LG:0.179 LD:0.000 D:0.001 GP:0.000 AC: 0.312 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.797 RFAcc:0.936
[210000/633000] LG:0.139 LD:-0.000 D:0.001 GP:0.000 AC: 0.317 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.766 RFAcc:0.902
[212500/633000] LG:0.186 LD:-0.003 D:-0.002 GP:0.000 AC: 0.311 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.766 RFAcc:0.954
[215000/633000] LG:0.231 LD:0.000 D:0.002 GP:0.000 AC: 0.342 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.735 RFAcc:0.978
[217500/633000] LG:0.231 LD:-0.006 D:0.001 GP:0.000 AC: 0.307 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.735 RFAcc:0.976
[220000/633000] LG:0.182 LD:-0.003 D:-0.000 GP:0.000 AC: 0.315 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.794 RFAcc:0.973
[222500/633000] LG:0.273 LD:-0.001 D:-0.001 GP:0.000 AC: 0.385 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.794 RFAcc:0.964
[225000/633000] LG:0.256 LD:0.001 D:0.002 GP:0.000 AC: 0.380 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.769 RFAcc:0.917
[227500/633000] LG:0.156 LD:0.000 D:0.003 GP:0.000 AC: 0.335 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.769 RFAcc:0.919
[230000/633000] LG:0.149 LD:-0.000 D:0.000 GP:0.000 AC: 0.336 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.776 RFAcc:0.975
[232500/633000] LG:0.249 LD:-0.001 D:-0.000 GP:0.000 AC: 0.327 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.776 RFAcc:0.939
[235000/633000] LG:0.240 LD:0.000 D:0.001 GP:0.000 AC: 0.343 RMSEAVG:0.030 NUM:0.030 SynTraiAuc:0.764 RFAcc:0.936
[237500/633000] LG:0.249 LD:-0.006 D:-0.001 GP:0.000 AC: 0.321 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.764 RFAcc:0.966
[240000/633000] LG:0.152 LD:-0.002 D:-0.000 GP:0.000 AC: 0.354 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.780 RFAcc:0.890
[242500/633000] LG:0.147 LD:-0.001 D:0.000 GP:0.000 AC: 0.332 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.780 RFAcc:0.950
[245000/633000] LG:0.216 LD:-0.001 D:0.000 GP:0.000 AC: 0.311 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.797 RFAcc:0.949
[247500/633000] LG:0.251 LD:0.000 D:0.001 GP:0.000 AC: 0.314 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.797 RFAcc:0.922
[250000/633000] LG:0.263 LD:0.000 D:0.000 GP:0.000 AC: 0.318 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.776 RFAcc:0.910
[252500/633000] LG:0.291 LD:0.000 D:0.001 GP:0.000 AC: 0.326 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.776 RFAcc:0.959
[255000/633000] LG:0.293 LD:-0.001 D:-0.000 GP:0.000 AC: 0.343 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.765 RFAcc:0.904
[257500/633000] LG:0.359 LD:-0.007 D:0.001 GP:0.000 AC: 0.307 RMSEAVG:0.004 NUM:0.004 SynTraiAuc:0.765 RFAcc:0.964
[260000/633000] LG:0.289 LD:0.001 D:0.002 GP:0.000 AC: 0.323 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.792 RFAcc:0.914
[262500/633000] LG:0.354 LD:-0.000 D:0.000 GP:0.000 AC: 0.325 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.792 RFAcc:0.955
[265000/633000] LG:0.391 LD:-0.004 D:-0.001 GP:0.000 AC: 0.313 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.759 RFAcc:0.927
[267500/633000] LG:0.344 LD:-0.008 D:0.000 GP:0.001 AC: 0.311 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.759 RFAcc:0.948
[270000/633000] LG:0.320 LD:0.003 D:0.003 GP:0.000 AC: 0.307 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.803 RFAcc:0.926
[272500/633000] LG:0.362 LD:-0.018 D:-0.017 GP:0.000 AC: 0.307 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.803 RFAcc:0.984
[275000/633000] LG:0.349 LD:-0.001 D:0.000 GP:0.000 AC: 0.317 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.787 RFAcc:0.968
[277500/633000] LG:0.354 LD:-0.002 D:-0.001 GP:0.000 AC: 0.329 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.787 RFAcc:0.944
[280000/633000] LG:0.249 LD:-0.002 D:-0.001 GP:0.000 AC: 0.324 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.774 RFAcc:0.980
[282500/633000] LG:0.280 LD:-0.002 D:-0.001 GP:0.000 AC: 0.342 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.774 RFAcc:0.880
[285000/633000] LG:0.374 LD:-0.000 D:0.001 GP:0.000 AC: 0.318 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.777 RFAcc:0.959
[287500/633000] LG:0.175 LD:-0.001 D:0.002 GP:0.000 AC: 0.337 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.777 RFAcc:0.950
[290000/633000] LG:0.285 LD:-0.006 D:0.002 GP:0.001 AC: 0.317 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.792 RFAcc:0.945
[292500/633000] LG:0.203 LD:-0.000 D:0.001 GP:0.000 AC: 0.326 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.792 RFAcc:0.965
[295000/633000] LG:0.247 LD:-0.000 D:0.001 GP:0.000 AC: 0.378 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.811 RFAcc:0.959
[297500/633000] LG:0.253 LD:0.004 D:0.005 GP:0.000 AC: 0.329 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.811 RFAcc:0.881
[300000/633000] LG:0.237 LD:0.017 D:0.020 GP:0.000 AC: 0.324 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.761 RFAcc:0.970
[302500/633000] LG:0.277 LD:-0.005 D:-0.001 GP:0.000 AC: 0.308 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.761 RFAcc:0.956
[305000/633000] LG:0.241 LD:-0.001 D:0.000 GP:0.000 AC: 0.306 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.773 RFAcc:0.939
[307500/633000] LG:0.220 LD:0.014 D:0.014 GP:0.000 AC: 0.319 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.773 RFAcc:0.951
[310000/633000] LG:0.265 LD:0.013 D:0.014 GP:0.000 AC: 0.311 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.767 RFAcc:0.936
[312500/633000] LG:0.238 LD:-0.004 D:-0.001 GP:0.000 AC: 0.359 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.767 RFAcc:0.943
[315000/633000] LG:0.241 LD:-0.002 D:-0.001 GP:0.000 AC: 0.324 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.810 RFAcc:0.897
[317500/633000] LG:0.264 LD:-0.003 D:-0.002 GP:0.000 AC: 0.325 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.810 RFAcc:0.920
[320000/633000] LG:0.259 LD:0.001 D:0.001 GP:0.000 AC: 0.324 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.779 RFAcc:0.955
[322500/633000] LG:0.275 LD:-0.010 D:-0.001 GP:0.001 AC: 0.329 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.779 RFAcc:0.960
[325000/633000] LG:0.208 LD:-0.023 D:-0.002 GP:0.001 AC: 0.321 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.791 RFAcc:0.954
[327500/633000] LG:0.217 LD:-0.002 D:-0.000 GP:0.000 AC: 0.312 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.791 RFAcc:0.938
[330000/633000] LG:0.237 LD:-0.000 D:0.001 GP:0.000 AC: 0.318 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.783 RFAcc:0.988
[332500/633000] LG:0.184 LD:-0.001 D:0.001 GP:0.000 AC: 0.300 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.783 RFAcc:0.981
[335000/633000] LG:0.248 LD:0.001 D:0.001 GP:0.000 AC: 0.324 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.789 RFAcc:0.978
[337500/633000] LG:0.235 LD:-0.001 D:0.000 GP:0.000 AC: 0.329 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.789 RFAcc:0.958
[340000/633000] LG:0.232 LD:-0.002 D:0.004 GP:0.000 AC: 0.329 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.776 RFAcc:0.924
[342500/633000] LG:0.245 LD:-0.001 D:0.000 GP:0.000 AC: 0.318 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.776 RFAcc:0.975
[345000/633000] LG:0.220 LD:-0.002 D:-0.001 GP:0.000 AC: 0.329 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.764 RFAcc:0.931
[347500/633000] LG:0.231 LD:-0.000 D:0.000 GP:0.000 AC: 0.316 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.764 RFAcc:0.966
[350000/633000] LG:0.239 LD:0.000 D:0.001 GP:0.000 AC: 0.311 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.796 RFAcc:0.939
[352500/633000] LG:0.271 LD:-0.001 D:-0.000 GP:0.000 AC: 0.323 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.796 RFAcc:0.906
[355000/633000] LG:0.309 LD:0.007 D:0.008 GP:0.000 AC: 0.325 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.783 RFAcc:0.956
[357500/633000] LG:0.220 LD:-0.002 D:0.000 GP:0.000 AC: 0.300 RMSEAVG:0.004 NUM:0.004 SynTraiAuc:0.783 RFAcc:0.959
[360000/633000] LG:0.295 LD:-0.001 D:0.000 GP:0.000 AC: 0.315 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.772 RFAcc:0.959
[362500/633000] LG:0.248 LD:0.001 D:0.001 GP:0.000 AC: 0.319 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.772 RFAcc:0.960
[365000/633000] LG:0.238 LD:-0.003 D:-0.001 GP:0.000 AC: 0.311 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.785 RFAcc:0.985
[367500/633000] LG:0.193 LD:0.013 D:0.014 GP:0.000 AC: 0.331 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.785 RFAcc:0.896
[370000/633000] LG:0.248 LD:0.002 D:0.002 GP:0.000 AC: 0.330 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.766 RFAcc:0.968
[372500/633000] LG:0.247 LD:-0.004 D:-0.004 GP:0.000 AC: 0.322 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.766 RFAcc:0.914
[375000/633000] LG:0.259 LD:-0.002 D:-0.001 GP:0.000 AC: 0.312 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.781 RFAcc:0.973
[377500/633000] LG:0.212 LD:0.010 D:0.012 GP:0.000 AC: 0.313 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.781 RFAcc:0.944
[380000/633000] LG:0.261 LD:0.001 D:0.001 GP:0.000 AC: 0.331 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.763 RFAcc:0.926
[382500/633000] LG:0.306 LD:0.002 D:0.002 GP:0.000 AC: 0.321 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.763 RFAcc:0.938
[385000/633000] LG:0.292 LD:0.017 D:0.017 GP:0.000 AC: 0.318 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.787 RFAcc:0.961
[387500/633000] LG:0.208 LD:-0.004 D:-0.004 GP:0.000 AC: 0.337 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.787 RFAcc:0.984
[390000/633000] LG:0.149 LD:0.001 D:0.002 GP:0.000 AC: 0.353 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.792 RFAcc:0.936
[392500/633000] LG:0.246 LD:0.001 D:0.001 GP:0.000 AC: 0.323 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.792 RFAcc:0.979
[395000/633000] LG:0.210 LD:-0.000 D:0.000 GP:0.000 AC: 0.319 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.785 RFAcc:0.953
[397500/633000] LG:0.289 LD:0.001 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.785 RFAcc:0.941
[400000/633000] LG:0.353 LD:0.003 D:0.003 GP:0.000 AC: 0.322 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.791 RFAcc:0.964
[402500/633000] LG:0.434 LD:0.001 D:0.002 GP:0.000 AC: 0.318 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.791 RFAcc:0.955
[405000/633000] LG:0.352 LD:0.014 D:0.015 GP:0.000 AC: 0.314 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.786 RFAcc:0.895
[407500/633000] LG:0.337 LD:0.003 D:0.007 GP:0.000 AC: 0.306 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.786 RFAcc:0.934
[410000/633000] LG:0.259 LD:-0.001 D:-0.001 GP:0.000 AC: 0.305 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.758 RFAcc:0.965
[412500/633000] LG:0.286 LD:-0.003 D:-0.000 GP:0.000 AC: 0.333 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.758 RFAcc:0.882
[415000/633000] LG:0.266 LD:-0.005 D:-0.002 GP:0.000 AC: 0.339 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.805 RFAcc:0.939
[417500/633000] LG:0.357 LD:-0.001 D:-0.000 GP:0.000 AC: 0.326 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.805 RFAcc:0.871
[420000/633000] LG:0.318 LD:-0.005 D:-0.001 GP:0.000 AC: 0.308 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.793 RFAcc:0.951
[422500/633000] LG:0.245 LD:0.002 D:0.002 GP:0.000 AC: 0.338 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.793 RFAcc:0.946
[425000/633000] LG:0.371 LD:-0.001 D:0.000 GP:0.000 AC: 0.325 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.800 RFAcc:0.919
[427500/633000] LG:0.347 LD:-0.003 D:-0.001 GP:0.000 AC: 0.313 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.800 RFAcc:0.934
[430000/633000] LG:0.312 LD:0.001 D:0.001 GP:0.000 AC: 0.332 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.785 RFAcc:0.914
[432500/633000] LG:0.224 LD:-0.002 D:-0.001 GP:0.000 AC: 0.327 RMSEAVG:0.029 NUM:0.029 SynTraiAuc:0.785 RFAcc:0.901
[435000/633000] LG:0.221 LD:0.001 D:0.001 GP:0.000 AC: 0.327 RMSEAVG:0.029 NUM:0.029 SynTraiAuc:0.789 RFAcc:0.925
[437500/633000] LG:0.264 LD:0.015 D:0.015 GP:0.000 AC: 0.313 RMSEAVG:0.021 NUM:0.021 SynTraiAuc:0.789 RFAcc:0.895
[440000/633000] LG:0.373 LD:-0.001 D:0.001 GP:0.000 AC: 0.321 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.789 RFAcc:0.909
[442500/633000] LG:0.405 LD:0.003 D:0.003 GP:0.000 AC: 0.316 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.789 RFAcc:0.959
[445000/633000] LG:0.468 LD:-0.004 D:0.002 GP:0.000 AC: 0.317 RMSEAVG:0.022 NUM:0.022 SynTraiAuc:0.750 RFAcc:0.922
[447500/633000] LG:0.615 LD:-0.030 D:-0.002 GP:0.002 AC: 0.300 RMSEAVG:0.082 NUM:0.082 SynTraiAuc:0.750 RFAcc:1.000
[450000/633000] LG:0.583 LD:-0.003 D:-0.001 GP:0.000 AC: 0.305 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.806 RFAcc:0.965
[452500/633000] LG:0.553 LD:-0.001 D:0.001 GP:0.000 AC: 0.302 RMSEAVG:0.024 NUM:0.024 SynTraiAuc:0.806 RFAcc:0.929
[455000/633000] LG:0.611 LD:0.015 D:0.015 GP:0.000 AC: 0.309 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.760 RFAcc:0.924
[457500/633000] LG:0.520 LD:-0.002 D:-0.001 GP:0.000 AC: 0.305 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.760 RFAcc:0.869
[460000/633000] LG:0.640 LD:0.000 D:0.000 GP:0.000 AC: 0.306 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.716 RFAcc:0.938
[462500/633000] LG:0.509 LD:0.000 D:0.001 GP:0.000 AC: 0.316 RMSEAVG:0.028 NUM:0.028 SynTraiAuc:0.716 RFAcc:0.959
[465000/633000] LG:0.448 LD:0.001 D:0.002 GP:0.000 AC: 0.321 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.756 RFAcc:0.899
[467500/633000] LG:0.484 LD:-0.003 D:-0.002 GP:0.000 AC: 0.307 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.756 RFAcc:0.910
[470000/633000] LG:0.558 LD:-0.000 D:0.003 GP:0.000 AC: 0.307 RMSEAVG:0.025 NUM:0.025 SynTraiAuc:0.749 RFAcc:0.885
[472500/633000] LG:0.422 LD:0.002 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.749 RFAcc:0.979
[475000/633000] LG:0.544 LD:0.000 D:0.001 GP:0.000 AC: 0.306 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.772 RFAcc:0.949
[477500/633000] LG:0.359 LD:0.012 D:0.012 GP:0.000 AC: 0.321 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.772 RFAcc:0.921
[480000/633000] LG:0.394 LD:-0.001 D:-0.001 GP:0.000 AC: 0.322 RMSEAVG:0.028 NUM:0.028 SynTraiAuc:0.750 RFAcc:0.899
[482500/633000] LG:0.404 LD:-0.001 D:-0.000 GP:0.000 AC: 0.313 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.750 RFAcc:0.938
[485000/633000] LG:0.469 LD:0.001 D:0.001 GP:0.000 AC: 0.307 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.773 RFAcc:0.917
[487500/633000] LG:0.359 LD:0.014 D:0.014 GP:0.000 AC: 0.321 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.773 RFAcc:0.887
[490000/633000] LG:0.471 LD:-0.001 D:0.001 GP:0.000 AC: 0.308 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.773 RFAcc:0.940
[492500/633000] LG:0.539 LD:0.002 D:0.002 GP:0.000 AC: 0.320 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.773 RFAcc:0.946
[495000/633000] LG:0.403 LD:0.000 D:0.000 GP:0.000 AC: 0.310 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.768 RFAcc:0.956
[497500/633000] LG:0.457 LD:-0.001 D:0.001 GP:0.000 AC: 0.313 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.768 RFAcc:0.926
[500000/633000] LG:0.510 LD:0.000 D:0.002 GP:0.000 AC: 0.309 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.776 RFAcc:0.968
[502500/633000] LG:0.379 LD:0.010 D:0.011 GP:0.000 AC: 0.320 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.776 RFAcc:0.956
[505000/633000] LG:0.495 LD:-0.001 D:-0.000 GP:0.000 AC: 0.307 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.778 RFAcc:0.906
[507500/633000] LG:0.430 LD:0.000 D:0.001 GP:0.000 AC: 0.314 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.778 RFAcc:0.959
[510000/633000] LG:0.477 LD:0.001 D:0.001 GP:0.000 AC: 0.306 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.782 RFAcc:0.955
[512500/633000] LG:0.473 LD:-0.002 D:-0.001 GP:0.000 AC: 0.316 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.782 RFAcc:0.934
[515000/633000] LG:0.363 LD:0.008 D:0.009 GP:0.000 AC: 0.309 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.792 RFAcc:0.932
[517500/633000] LG:0.482 LD:-0.001 D:-0.000 GP:0.000 AC: 0.308 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.792 RFAcc:0.975
[520000/633000] LG:0.413 LD:0.010 D:0.011 GP:0.000 AC: 0.329 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.779 RFAcc:0.927
[522500/633000] LG:0.492 LD:0.001 D:0.002 GP:0.000 AC: 0.313 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.779 RFAcc:0.914
[525000/633000] LG:0.408 LD:-0.001 D:0.000 GP:0.000 AC: 0.320 RMSEAVG:0.018 NUM:0.018 SynTraiAuc:0.731 RFAcc:0.959
[527500/633000] LG:0.488 LD:-0.001 D:-0.001 GP:0.000 AC: 0.307 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.731 RFAcc:0.948
[530000/633000] LG:0.468 LD:-0.011 D:-0.010 GP:0.000 AC: 0.308 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.807 RFAcc:0.958
[532500/633000] LG:0.471 LD:0.003 D:0.003 GP:0.000 AC: 0.319 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.807 RFAcc:0.910
[535000/633000] LG:0.487 LD:-0.002 D:0.000 GP:0.000 AC: 0.306 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.748 RFAcc:0.953
[537500/633000] LG:0.498 LD:0.001 D:0.001 GP:0.000 AC: 0.305 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.748 RFAcc:0.941
[540000/633000] LG:0.417 LD:0.004 D:0.004 GP:0.000 AC: 0.316 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.754 RFAcc:0.945
[542500/633000] LG:0.509 LD:0.000 D:0.000 GP:0.000 AC: 0.308 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.754 RFAcc:0.945
[545000/633000] LG:0.473 LD:0.001 D:0.001 GP:0.000 AC: 0.313 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.764 RFAcc:0.916
[547500/633000] LG:0.541 LD:0.002 D:0.003 GP:0.000 AC: 0.304 RMSEAVG:0.019 NUM:0.019 SynTraiAuc:0.764 RFAcc:0.911
[550000/633000] LG:0.449 LD:0.001 D:0.001 GP:0.000 AC: 0.324 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.769 RFAcc:0.891
[552500/633000] LG:0.529 LD:-0.002 D:-0.000 GP:0.000 AC: 0.312 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.769 RFAcc:0.946
[555000/633000] LG:0.552 LD:0.003 D:0.004 GP:0.000 AC: 0.306 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.777 RFAcc:0.956
[557500/633000] LG:0.496 LD:0.001 D:0.003 GP:0.000 AC: 0.309 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.777 RFAcc:0.974
[560000/633000] LG:0.378 LD:0.003 D:0.003 GP:0.000 AC: 0.318 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.800 RFAcc:0.951
[562500/633000] LG:0.471 LD:0.001 D:0.002 GP:0.000 AC: 0.331 RMSEAVG:0.017 NUM:0.017 SynTraiAuc:0.800 RFAcc:0.931
[565000/633000] LG:0.365 LD:0.002 D:0.003 GP:0.000 AC: 0.328 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.767 RFAcc:0.964
[567500/633000] LG:0.476 LD:0.000 D:0.000 GP:0.000 AC: 0.313 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.767 RFAcc:0.876
[570000/633000] LG:0.569 LD:0.000 D:0.001 GP:0.000 AC: 0.306 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.774 RFAcc:0.961
[572500/633000] LG:0.493 LD:-0.000 D:-0.000 GP:0.000 AC: 0.304 RMSEAVG:0.015 NUM:0.015 SynTraiAuc:0.774 RFAcc:0.944
[575000/633000] LG:0.385 LD:0.012 D:0.016 GP:0.000 AC: 0.320 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.796 RFAcc:0.968
[577500/633000] LG:0.364 LD:0.002 D:0.002 GP:0.000 AC: 0.315 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.796 RFAcc:0.941
[580000/633000] LG:0.471 LD:0.001 D:0.001 GP:0.000 AC: 0.304 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.754 RFAcc:0.955
[582500/633000] LG:0.517 LD:0.000 D:0.001 GP:0.000 AC: 0.307 RMSEAVG:0.014 NUM:0.014 SynTraiAuc:0.754 RFAcc:0.940
[585000/633000] LG:0.499 LD:0.003 D:0.003 GP:0.000 AC: 0.311 RMSEAVG:0.016 NUM:0.016 SynTraiAuc:0.749 RFAcc:0.938
[587500/633000] LG:0.558 LD:-0.009 D:-0.008 GP:0.000 AC: 0.311 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.749 RFAcc:0.946
[590000/633000] LG:0.414 LD:-0.000 D:0.000 GP:0.000 AC: 0.317 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.763 RFAcc:0.949
[592500/633000] LG:0.539 LD:0.002 D:0.002 GP:0.000 AC: 0.319 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.763 RFAcc:0.920
[595000/633000] LG:0.526 LD:-0.000 D:0.000 GP:0.000 AC: 0.304 RMSEAVG:0.004 NUM:0.004 SynTraiAuc:0.786 RFAcc:0.985
[597500/633000] LG:0.587 LD:0.001 D:0.001 GP:0.000 AC: 0.305 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.786 RFAcc:0.968
[600000/633000] LG:0.551 LD:0.002 D:0.002 GP:0.000 AC: 0.307 RMSEAVG:0.023 NUM:0.023 SynTraiAuc:0.766 RFAcc:0.960
[602500/633000] LG:0.615 LD:0.015 D:0.016 GP:0.000 AC: 0.313 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.766 RFAcc:0.930
[605000/633000] LG:0.482 LD:0.000 D:0.000 GP:0.000 AC: 0.307 RMSEAVG:0.007 NUM:0.007 SynTraiAuc:0.789 RFAcc:0.941
[607500/633000] LG:0.661 LD:0.007 D:0.010 GP:0.000 AC: 0.307 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.789 RFAcc:0.981
[610000/633000] LG:0.604 LD:-0.002 D:-0.002 GP:0.000 AC: 0.311 RMSEAVG:0.009 NUM:0.009 SynTraiAuc:0.758 RFAcc:0.949
[612500/633000] LG:0.572 LD:0.002 D:0.002 GP:0.000 AC: 0.316 RMSEAVG:0.013 NUM:0.013 SynTraiAuc:0.758 RFAcc:0.955
[615000/633000] LG:0.640 LD:0.016 D:0.018 GP:0.000 AC: 0.311 RMSEAVG:0.012 NUM:0.012 SynTraiAuc:0.772 RFAcc:0.925
[617500/633000] LG:0.627 LD:0.016 D:0.017 GP:0.000 AC: 0.306 RMSEAVG:0.005 NUM:0.005 SynTraiAuc:0.772 RFAcc:0.980
[620000/633000] LG:0.625 LD:-0.001 D:-0.001 GP:0.000 AC: 0.329 RMSEAVG:0.020 NUM:0.020 SynTraiAuc:0.752 RFAcc:0.948
[622500/633000] LG:0.707 LD:0.017 D:0.022 GP:0.000 AC: 0.309 RMSEAVG:0.011 NUM:0.011 SynTraiAuc:0.752 RFAcc:0.938
[625000/633000] LG:0.646 LD:0.003 D:0.003 GP:0.000 AC: 0.309 RMSEAVG:0.006 NUM:0.006 SynTraiAuc:0.755 RFAcc:0.934
[627500/633000] LG:0.626 LD:0.002 D:0.003 GP:0.000 AC: 0.306 RMSEAVG:0.010 NUM:0.010 SynTraiAuc:0.755 RFAcc:0.935
[630000/633000] LG:0.703 LD:0.003 D:0.003 GP:0.000 AC: 0.307 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.799 RFAcc:0.944
[632500/633000] LG:0.663 LD:0.000 D:0.001 GP:0.000 AC: 0.305 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.799 RFAcc:0.949
[633000/633000] LG:0.663 LD:0.000 D:0.001 GP:0.000 AC: 0.310 RMSEAVG:0.008 NUM:0.008 SynTraiAuc:0.799 RFAcc:0.949
INFO:root:Finished training after 633000/633000.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.0516
AUC-ROC: 0.8322
AUC-PR: 0.3576

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2349
AUC-ROC: 0.8228
AUC-PR: 0.3496

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.0512
AUC-ROC: 0.8294
AUC-PR: 0.3770

###########################################
###########################################
################# TABDDPM #################
###########################################
###########################################
[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 800 samples, Labels: {0: 559, 1: 241}
- Generated data: 1055 samples, Labels: {0: 740, 1: 315}
- Combined training data: 1115 samples, Labels: {0: 559, 1: 556}
Training set merged, preprocessed, and saved in Merged/uci_german.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1599
AUC-ROC: 0.8085
AUC-PR: 0.6587

Results for Random Forest:
 Brier-Score: 0.1599, AUC-ROC: 0.8085, AUC-PR: 0.6587

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2438
AUC-ROC: 0.8303
AUC-PR: 0.6207

Results for AdaBoost:
 Brier-Score: 0.2438, AUC-ROC: 0.8303, AUC-PR: 0.6207

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1540
AUC-ROC: 0.8071
AUC-PR: 0.6393

Results for Gradient Boosting:
 Brier-Score: 0.1540, AUC-ROC: 0.8071, AUC-PR: 0.6393

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 800 samples, Labels: {0: 559, 1: 241}
- Generated data: 1055 samples, Labels: {0: 752, 1: 303}
- Combined training data: 1103 samples, Labels: {0: 559, 1: 544}
Training set merged, preprocessed, and saved in Merged/uci_german.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1617
AUC-ROC: 0.7920
AUC-PR: 0.6468

Results for Random Forest:
 Brier-Score: 0.1617, AUC-ROC: 0.7920, AUC-PR: 0.6468

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2431
AUC-ROC: 0.7980
AUC-PR: 0.6219

Results for AdaBoost:
 Brier-Score: 0.2431, AUC-ROC: 0.7980, AUC-PR: 0.6219

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1585
AUC-ROC: 0.8114
AUC-PR: 0.6217

Results for Gradient Boosting:
 Brier-Score: 0.1585, AUC-ROC: 0.8114, AUC-PR: 0.6217

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 800 samples, Labels: {0: 559, 1: 241}
- Generated data: 1055 samples, Labels: {0: 737, 1: 318}
- Combined training data: 1118 samples, Labels: {0: 559, 1: 559}
Training set merged, preprocessed, and saved in Merged/uci_german.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1588
AUC-ROC: 0.8078
AUC-PR: 0.6465

Results for Random Forest:
 Brier-Score: 0.1588, AUC-ROC: 0.8078, AUC-PR: 0.6465

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2438
AUC-ROC: 0.8119
AUC-PR: 0.5936

Results for AdaBoost:
 Brier-Score: 0.2438, AUC-ROC: 0.8119, AUC-PR: 0.5936

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1434
AUC-ROC: 0.8436
AUC-PR: 0.6838

Results for Gradient Boosting:
 Brier-Score: 0.1434, AUC-ROC: 0.8436, AUC-PR: 0.6838

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 800 samples, Labels: {0: 559, 1: 241}
- Generated data: 1055 samples, Labels: {0: 713, 1: 342}
- Combined training data: 1142 samples, Labels: {1: 583, 0: 559}
Training set merged, preprocessed, and saved in Merged/uci_german.
Training and evaluating Random Forest on uci_german dataset...
Brier Score: 0.1672
AUC-ROC: 0.7937
AUC-PR: 0.5546

Results for Random Forest:
 Brier-Score: 0.1672, AUC-ROC: 0.7937, AUC-PR: 0.5546

Training and evaluating AdaBoost on uci_german dataset...
Brier Score: 0.2436
AUC-ROC: 0.7954
AUC-PR: 0.5584

Results for AdaBoost:
 Brier-Score: 0.2436, AUC-ROC: 0.7954, AUC-PR: 0.5584

Training and evaluating Gradient Boosting on uci_german dataset...
Brier Score: 0.1608
AUC-ROC: 0.8098
AUC-PR: 0.5779

Results for Gradient Boosting:
 Brier-Score: 0.1608, AUC-ROC: 0.8098, AUC-PR: 0.5779

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 24000 samples, Labels: {0: 18677, 1: 5323}
- Generated data: 60209 samples, Labels: {0: 46883, 1: 13326}
- Combined training data: 37326 samples, Labels: {0: 18677, 1: 18649}
Training set merged, preprocessed, and saved in Merged/uci_taiwan.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1390
AUC-ROC: 0.7589
AUC-PR: 0.5219

Results for Random Forest:
 Brier-Score: 0.1390, AUC-ROC: 0.7589, AUC-PR: 0.5219

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2437
AUC-ROC: 0.7738
AUC-PR: 0.5269

Results for AdaBoost:
 Brier-Score: 0.2437, AUC-ROC: 0.7738, AUC-PR: 0.5269

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1350
AUC-ROC: 0.7776
AUC-PR: 0.5373

Results for Gradient Boosting:
 Brier-Score: 0.1350, AUC-ROC: 0.7776, AUC-PR: 0.5373

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 24000 samples, Labels: {0: 18677, 1: 5323}
- Generated data: 60209 samples, Labels: {0: 46709, 1: 13500}
- Combined training data: 37500 samples, Labels: {1: 18823, 0: 18677}
Training set merged, preprocessed, and saved in Merged/uci_taiwan.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1400
AUC-ROC: 0.7539
AUC-PR: 0.5202

Results for Random Forest:
 Brier-Score: 0.1400, AUC-ROC: 0.7539, AUC-PR: 0.5202

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2438
AUC-ROC: 0.7775
AUC-PR: 0.5280

Results for AdaBoost:
 Brier-Score: 0.2438, AUC-ROC: 0.7775, AUC-PR: 0.5280

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1353
AUC-ROC: 0.7766
AUC-PR: 0.5372

Results for Gradient Boosting:
 Brier-Score: 0.1353, AUC-ROC: 0.7766, AUC-PR: 0.5372

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 24000 samples, Labels: {0: 18677, 1: 5323}
- Generated data: 60209 samples, Labels: {0: 46853, 1: 13356}
- Combined training data: 37356 samples, Labels: {1: 18679, 0: 18677}
Training set merged, preprocessed, and saved in Merged/uci_taiwan.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1393
AUC-ROC: 0.7552
AUC-PR: 0.5228

Results for Random Forest:
 Brier-Score: 0.1393, AUC-ROC: 0.7552, AUC-PR: 0.5228

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2437
AUC-ROC: 0.7785
AUC-PR: 0.5298

Results for AdaBoost:
 Brier-Score: 0.2437, AUC-ROC: 0.7785, AUC-PR: 0.5298

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1349
AUC-ROC: 0.7790
AUC-PR: 0.5392

Results for Gradient Boosting:
 Brier-Score: 0.1349, AUC-ROC: 0.7790, AUC-PR: 0.5392

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 24000 samples, Labels: {0: 18677, 1: 5323}
- Generated data: 60209 samples, Labels: {0: 46909, 1: 13300}
- Combined training data: 37300 samples, Labels: {0: 18677, 1: 18623}
Training set merged, preprocessed, and saved in Merged/uci_taiwan.
Training and evaluating Random Forest on uci_taiwan dataset...
Brier Score: 0.1391
AUC-ROC: 0.7571
AUC-PR: 0.5256

Results for Random Forest:
 Brier-Score: 0.1391, AUC-ROC: 0.7571, AUC-PR: 0.5256

Training and evaluating AdaBoost on uci_taiwan dataset...
Brier Score: 0.2438
AUC-ROC: 0.7671
AUC-PR: 0.5212

Results for AdaBoost:
 Brier-Score: 0.2438, AUC-ROC: 0.7671, AUC-PR: 0.5212

Training and evaluating Gradient Boosting on uci_taiwan dataset...
Brier Score: 0.1354
AUC-ROC: 0.7731
AUC-PR: 0.5389

Results for Gradient Boosting:
 Brier-Score: 0.1354, AUC-ROC: 0.7731, AUC-PR: 0.5389

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 40000 samples, Labels: {0: 29623, 1: 10377}
- Generated data: 74187 samples, Labels: {0: 54961, 1: 19226}
- Combined training data: 59226 samples, Labels: {0: 29623, 1: 29603}
Training set merged, preprocessed, and saved in Merged/pakdd.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1928
AUC-ROC: 0.6022
AUC-PR: 0.3408

Results for Random Forest:
 Brier-Score: 0.1928, AUC-ROC: 0.6022, AUC-PR: 0.3408

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2471
AUC-ROC: 0.6211
AUC-PR: 0.3516

Results for AdaBoost:
 Brier-Score: 0.2471, AUC-ROC: 0.6211, AUC-PR: 0.3516

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1877
AUC-ROC: 0.6301
AUC-PR: 0.3692

Results for Gradient Boosting:
 Brier-Score: 0.1877, AUC-ROC: 0.6301, AUC-PR: 0.3692

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 40000 samples, Labels: {0: 29623, 1: 10377}
- Generated data: 74187 samples, Labels: {0: 54910, 1: 19277}
- Combined training data: 59277 samples, Labels: {1: 29654, 0: 29623}
Training set merged, preprocessed, and saved in Merged/pakdd.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1927
AUC-ROC: 0.6020
AUC-PR: 0.3419

Results for Random Forest:
 Brier-Score: 0.1927, AUC-ROC: 0.6020, AUC-PR: 0.3419

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2471
AUC-ROC: 0.6131
AUC-PR: 0.3417

Results for AdaBoost:
 Brier-Score: 0.2471, AUC-ROC: 0.6131, AUC-PR: 0.3417

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1882
AUC-ROC: 0.6259
AUC-PR: 0.3593

Results for Gradient Boosting:
 Brier-Score: 0.1882, AUC-ROC: 0.6259, AUC-PR: 0.3593

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 40000 samples, Labels: {0: 29623, 1: 10377}
- Generated data: 74187 samples, Labels: {0: 54697, 1: 19490}
- Combined training data: 59490 samples, Labels: {1: 29867, 0: 29623}
Training set merged, preprocessed, and saved in Merged/pakdd.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1918
AUC-ROC: 0.6055
AUC-PR: 0.3492

Results for Random Forest:
 Brier-Score: 0.1918, AUC-ROC: 0.6055, AUC-PR: 0.3492

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2471
AUC-ROC: 0.6154
AUC-PR: 0.3467

Results for AdaBoost:
 Brier-Score: 0.2471, AUC-ROC: 0.6154, AUC-PR: 0.3467

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1874
AUC-ROC: 0.6334
AUC-PR: 0.3721

Results for Gradient Boosting:
 Brier-Score: 0.1874, AUC-ROC: 0.6334, AUC-PR: 0.3721

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 40000 samples, Labels: {0: 29623, 1: 10377}
- Generated data: 74187 samples, Labels: {0: 55020, 1: 19167}
- Combined training data: 59167 samples, Labels: {0: 29623, 1: 29544}
Training set merged, preprocessed, and saved in Merged/pakdd.
Training and evaluating Random Forest on pakdd dataset...
Brier Score: 0.1920
AUC-ROC: 0.6058
AUC-PR: 0.3488

Results for Random Forest:
 Brier-Score: 0.1920, AUC-ROC: 0.6058, AUC-PR: 0.3488

Training and evaluating AdaBoost on pakdd dataset...
Brier Score: 0.2472
AUC-ROC: 0.6123
AUC-PR: 0.3385

Results for AdaBoost:
 Brier-Score: 0.2472, AUC-ROC: 0.6123, AUC-PR: 0.3385

Training and evaluating Gradient Boosting on pakdd dataset...
Brier Score: 0.1889
AUC-ROC: 0.6236
AUC-PR: 0.3496

Results for Gradient Boosting:
 Brier-Score: 0.1889, AUC-ROC: 0.6236, AUC-PR: 0.3496

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 4768 samples, Labels: {0: 3844, 1: 924}
- Generated data: 15067 samples, Labels: {0: 12232, 1: 2835}
- Combined training data: 7603 samples, Labels: {0: 3844, 1: 3759}
Training set merged, preprocessed, and saved in Merged/hmeq.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0665
AUC-ROC: 0.9639
AUC-PR: 0.8855

Results for Random Forest:
 Brier-Score: 0.0665, AUC-ROC: 0.9639, AUC-PR: 0.8855

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2330
AUC-ROC: 0.9134
AUC-PR: 0.8210

Results for AdaBoost:
 Brier-Score: 0.2330, AUC-ROC: 0.9134, AUC-PR: 0.8210

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0761
AUC-ROC: 0.9326
AUC-PR: 0.8495

Results for Gradient Boosting:
 Brier-Score: 0.0761, AUC-ROC: 0.9326, AUC-PR: 0.8495

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 4768 samples, Labels: {0: 3844, 1: 924}
- Generated data: 15067 samples, Labels: {0: 12301, 1: 2766}
- Combined training data: 7534 samples, Labels: {0: 3844, 1: 3690}
Training set merged, preprocessed, and saved in Merged/hmeq.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0663
AUC-ROC: 0.9646
AUC-PR: 0.8813

Results for Random Forest:
 Brier-Score: 0.0663, AUC-ROC: 0.9646, AUC-PR: 0.8813

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2341
AUC-ROC: 0.8992
AUC-PR: 0.7948

Results for AdaBoost:
 Brier-Score: 0.2341, AUC-ROC: 0.8992, AUC-PR: 0.7948

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0787
AUC-ROC: 0.9296
AUC-PR: 0.8412

Results for Gradient Boosting:
 Brier-Score: 0.0787, AUC-ROC: 0.9296, AUC-PR: 0.8412

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 4768 samples, Labels: {0: 3844, 1: 924}
- Generated data: 15067 samples, Labels: {0: 12233, 1: 2834}
- Combined training data: 7602 samples, Labels: {0: 3844, 1: 3758}
Training set merged, preprocessed, and saved in Merged/hmeq.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0688
AUC-ROC: 0.9607
AUC-PR: 0.8728

Results for Random Forest:
 Brier-Score: 0.0688, AUC-ROC: 0.9607, AUC-PR: 0.8728

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2327
AUC-ROC: 0.9168
AUC-PR: 0.8204

Results for AdaBoost:
 Brier-Score: 0.2327, AUC-ROC: 0.9168, AUC-PR: 0.8204

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0756
AUC-ROC: 0.9359
AUC-PR: 0.8502

Results for Gradient Boosting:
 Brier-Score: 0.0756, AUC-ROC: 0.9359, AUC-PR: 0.8502

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 4768 samples, Labels: {0: 3844, 1: 924}
- Generated data: 15067 samples, Labels: {0: 12182, 1: 2885}
- Combined training data: 7653 samples, Labels: {0: 3844, 1: 3809}
Training set merged, preprocessed, and saved in Merged/hmeq.
Training and evaluating Random Forest on hmeq dataset...
Brier Score: 0.0654
AUC-ROC: 0.9644
AUC-PR: 0.8884

Results for Random Forest:
 Brier-Score: 0.0654, AUC-ROC: 0.9644, AUC-PR: 0.8884

Training and evaluating AdaBoost on hmeq dataset...
Brier Score: 0.2341
AUC-ROC: 0.8986
AUC-PR: 0.7937

Results for AdaBoost:
 Brier-Score: 0.2341, AUC-ROC: 0.8986, AUC-PR: 0.7937

Training and evaluating Gradient Boosting on hmeq dataset...
Brier Score: 0.0792
AUC-ROC: 0.9234
AUC-PR: 0.8386

Results for Gradient Boosting:
 Brier-Score: 0.0792, AUC-ROC: 0.9234, AUC-PR: 0.8386

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 120000 samples, Labels: {0: 111930, 1: 8070}
- Generated data: 1544386 samples, Labels: {0: 1441031, 1: 103355}
- Combined training data: 223355 samples, Labels: {0: 111930, 1: 111425}
Training set merged, preprocessed, and saved in Merged/gmsc.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.1309
AUC-ROC: 0.7829
AUC-PR: 0.2488

Results for Random Forest:
 Brier-Score: 0.1309, AUC-ROC: 0.7829, AUC-PR: 0.2488

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2450
AUC-ROC: 0.8188
AUC-PR: 0.3363

Results for AdaBoost:
 Brier-Score: 0.2450, AUC-ROC: 0.8188, AUC-PR: 0.3363

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.1166
AUC-ROC: 0.8273
AUC-PR: 0.3347

Results for Gradient Boosting:
 Brier-Score: 0.1166, AUC-ROC: 0.8273, AUC-PR: 0.3347

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 120000 samples, Labels: {0: 111930, 1: 8070}
- Generated data: 1544386 samples, Labels: {0: 1441816, 1: 102570}
- Combined training data: 222570 samples, Labels: {0: 111930, 1: 110640}
Training set merged, preprocessed, and saved in Merged/gmsc.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.0536
AUC-ROC: 0.7950
AUC-PR: 0.3180

Results for Random Forest:
 Brier-Score: 0.0536, AUC-ROC: 0.7950, AUC-PR: 0.3180

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2356
AUC-ROC: 0.8142
AUC-PR: 0.3280

Results for AdaBoost:
 Brier-Score: 0.2356, AUC-ROC: 0.8142, AUC-PR: 0.3280

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.0500
AUC-ROC: 0.8269
AUC-PR: 0.3552

Results for Gradient Boosting:
 Brier-Score: 0.0500, AUC-ROC: 0.8269, AUC-PR: 0.3552

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 120000 samples, Labels: {0: 111930, 1: 8070}
- Generated data: 1544386 samples, Labels: {0: 1441738, 1: 102648}
- Combined training data: 222648 samples, Labels: {0: 111930, 1: 110718}
Training set merged, preprocessed, and saved in Merged/gmsc.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.0536
AUC-ROC: 0.7948
AUC-PR: 0.3108

Results for Random Forest:
 Brier-Score: 0.0536, AUC-ROC: 0.7948, AUC-PR: 0.3108

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2348
AUC-ROC: 0.8319
AUC-PR: 0.3368

Results for AdaBoost:
 Brier-Score: 0.2348, AUC-ROC: 0.8319, AUC-PR: 0.3368

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.0496
AUC-ROC: 0.8384
AUC-PR: 0.3630

Results for Gradient Boosting:
 Brier-Score: 0.0496, AUC-ROC: 0.8384, AUC-PR: 0.3630

[INFO] Data merged successfully (training set includes validation set):
- Original data (training + validation): 120000 samples, Labels: {0: 111930, 1: 8070}
- Generated data: 1544386 samples, Labels: {0: 1441557, 1: 102829}
- Combined training data: 222829 samples, Labels: {0: 111930, 1: 110899}
Training set merged, preprocessed, and saved in Merged/gmsc.
Training and evaluating Random Forest on gmsc dataset...
Brier Score: 0.0539
AUC-ROC: 0.7922
AUC-PR: 0.3166

Results for Random Forest:
 Brier-Score: 0.0539, AUC-ROC: 0.7922, AUC-PR: 0.3166

Training and evaluating AdaBoost on gmsc dataset...
Brier Score: 0.2355
AUC-ROC: 0.8221
AUC-PR: 0.3383

Results for AdaBoost:
 Brier-Score: 0.2355, AUC-ROC: 0.8221, AUC-PR: 0.3383

Training and evaluating Gradient Boosting on gmsc dataset...
Brier Score: 0.0499
AUC-ROC: 0.8267
AUC-PR: 0.3545

Results for Gradient Boosting:
 Brier-Score: 0.0499, AUC-ROC: 0.8267, AUC-PR: 0.3545

Complete Results DataFrame:
        dataset  resample_method         classifier       metric     value
0    uci_german              ros      Random Forest  Brier-Score  0.156852
1    uci_german              ros      Random Forest      AUC-ROC  0.785603
2    uci_german              ros      Random Forest       AUC-PR  0.563668
3    uci_german              ros           AdaBoost  Brier-Score  0.241799
4    uci_german              ros           AdaBoost      AUC-ROC  0.762474
..          ...              ...                ...          ...       ...
715        gmsc  tabddpm_res_bgm           AdaBoost      AUC-ROC  0.822108
716        gmsc  tabddpm_res_bgm           AdaBoost       AUC-PR  0.338276
717        gmsc  tabddpm_res_bgm  Gradient Boosting  Brier-Score  0.049880
718        gmsc  tabddpm_res_bgm  Gradient Boosting      AUC-ROC  0.826676
719        gmsc  tabddpm_res_bgm  Gradient Boosting       AUC-PR  0.354492

[720 rows x 5 columns]

================================================================================


ANALYSIS FOR DATASET: uci_german
==================================================

All Results:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
adasyn               AdaBoost           0.509310  0.787422     0.239379
                     Gradient Boosting  0.518638  0.793139     0.156741
                     Random Forest      0.648193  0.812110     0.148913
cwgan                AdaBoost           0.418181  0.755717     0.244799
                     Gradient Boosting  0.586654  0.785863     0.159176
                     Random Forest      0.570500  0.781705     0.157503
enn                  AdaBoost           0.443985  0.771310     0.243384
                     Gradient Boosting  0.572804  0.777547     0.217696
                     Random Forest      0.597760  0.785343     0.199940
nearmiss             AdaBoost           0.405866  0.707900     0.243842
                     Gradient Boosting  0.434809  0.696466     0.224133
                     Random Forest      0.430121  0.689709     0.215533
ros                  AdaBoost           0.464614  0.762474     0.241799
                     Gradient Boosting  0.545642  0.785343     0.172145
                     Random Forest      0.563668  0.785603     0.156852
rus                  AdaBoost           0.441203  0.776507     0.242263
                     Gradient Boosting  0.485986  0.785863     0.185932
                     Random Forest      0.619757  0.801975     0.181109
smote                AdaBoost           0.489220  0.785863     0.238692
                     Gradient Boosting  0.562884  0.804574     0.155846
                     Random Forest      0.580915  0.791580     0.156839
smote_bs1            AdaBoost           0.503087  0.788981     0.240741
                     Gradient Boosting  0.545742  0.798857     0.157834
                     Random Forest      0.641044  0.800416     0.152443
smote_bs2            AdaBoost           0.460603  0.761954     0.240370
                     Gradient Boosting  0.523710  0.801455     0.156831
                     Random Forest      0.602453  0.808472     0.151907
smote_enn            AdaBoost           0.532423  0.796258     0.236447
                     Gradient Boosting  0.524243  0.788462     0.241737
                     Random Forest      0.571405  0.791060     0.204302
smote_tomek          AdaBoost           0.498032  0.795738     0.240586
                     Gradient Boosting  0.577250  0.799376     0.154447
                     Random Forest      0.639356  0.801195     0.151624
tabddpm_bgm          AdaBoost           0.621904  0.797993     0.243076
                     Gradient Boosting  0.621707  0.811396     0.158547
                     Random Forest      0.646803  0.791982     0.161737
tabddpm_identity     AdaBoost           0.620721  0.830268     0.243807
                     Gradient Boosting  0.639277  0.807068     0.154043
                     Random Forest      0.658690  0.808511     0.159947
tabddpm_res_bgm      AdaBoost           0.558370  0.795408     0.243563
                     Gradient Boosting  0.577872  0.809833     0.160765
                     Random Forest      0.554583  0.793665     0.167163
tabddpm_res_identity AdaBoost           0.593609  0.811876     0.243807
                     Gradient Boosting  0.683768  0.843611     0.143417
                     Random Forest      0.646510  0.807850     0.158831
tomek                AdaBoost           0.474484  0.773389     0.239779
                     Gradient Boosting  0.464456  0.774948     0.169526
                     Random Forest      0.540138  0.762214     0.162409

Sorted by AUC-PR:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
tabddpm_res_identity Gradient Boosting  0.683768  0.843611     0.143417
tabddpm_identity     Random Forest      0.658690  0.808511     0.159947
adasyn               Random Forest      0.648193  0.812110     0.148913
tabddpm_bgm          Random Forest      0.646803  0.791982     0.161737
tabddpm_res_identity Random Forest      0.646510  0.807850     0.158831
smote_bs1            Random Forest      0.641044  0.800416     0.152443
smote_tomek          Random Forest      0.639356  0.801195     0.151624
tabddpm_identity     Gradient Boosting  0.639277  0.807068     0.154043
tabddpm_bgm          AdaBoost           0.621904  0.797993     0.243076
                     Gradient Boosting  0.621707  0.811396     0.158547
tabddpm_identity     AdaBoost           0.620721  0.830268     0.243807
rus                  Random Forest      0.619757  0.801975     0.181109
smote_bs2            Random Forest      0.602453  0.808472     0.151907
enn                  Random Forest      0.597760  0.785343     0.199940
tabddpm_res_identity AdaBoost           0.593609  0.811876     0.243807
cwgan                Gradient Boosting  0.586654  0.785863     0.159176
smote                Random Forest      0.580915  0.791580     0.156839
tabddpm_res_bgm      Gradient Boosting  0.577872  0.809833     0.160765
smote_tomek          Gradient Boosting  0.577250  0.799376     0.154447
enn                  Gradient Boosting  0.572804  0.777547     0.217696
smote_enn            Random Forest      0.571405  0.791060     0.204302
cwgan                Random Forest      0.570500  0.781705     0.157503
ros                  Random Forest      0.563668  0.785603     0.156852
smote                Gradient Boosting  0.562884  0.804574     0.155846
tabddpm_res_bgm      AdaBoost           0.558370  0.795408     0.243563
                     Random Forest      0.554583  0.793665     0.167163
smote_bs1            Gradient Boosting  0.545742  0.798857     0.157834
ros                  Gradient Boosting  0.545642  0.785343     0.172145
tomek                Random Forest      0.540138  0.762214     0.162409
smote_enn            AdaBoost           0.532423  0.796258     0.236447
                     Gradient Boosting  0.524243  0.788462     0.241737
smote_bs2            Gradient Boosting  0.523710  0.801455     0.156831
adasyn               Gradient Boosting  0.518638  0.793139     0.156741
                     AdaBoost           0.509310  0.787422     0.239379
smote_bs1            AdaBoost           0.503087  0.788981     0.240741
smote_tomek          AdaBoost           0.498032  0.795738     0.240586
smote                AdaBoost           0.489220  0.785863     0.238692
rus                  Gradient Boosting  0.485986  0.785863     0.185932
tomek                AdaBoost           0.474484  0.773389     0.239779
ros                  AdaBoost           0.464614  0.762474     0.241799
tomek                Gradient Boosting  0.464456  0.774948     0.169526
smote_bs2            AdaBoost           0.460603  0.761954     0.240370
enn                  AdaBoost           0.443985  0.771310     0.243384
rus                  AdaBoost           0.441203  0.776507     0.242263
nearmiss             Gradient Boosting  0.434809  0.696466     0.224133
                     Random Forest      0.430121  0.689709     0.215533
cwgan                AdaBoost           0.418181  0.755717     0.244799
nearmiss             AdaBoost           0.405866  0.707900     0.243842

Sorted by AUC-ROC:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
tabddpm_res_identity Gradient Boosting  0.683768  0.843611     0.143417
tabddpm_identity     AdaBoost           0.620721  0.830268     0.243807
adasyn               Random Forest      0.648193  0.812110     0.148913
tabddpm_res_identity AdaBoost           0.593609  0.811876     0.243807
tabddpm_bgm          Gradient Boosting  0.621707  0.811396     0.158547
tabddpm_res_bgm      Gradient Boosting  0.577872  0.809833     0.160765
tabddpm_identity     Random Forest      0.658690  0.808511     0.159947
smote_bs2            Random Forest      0.602453  0.808472     0.151907
tabddpm_res_identity Random Forest      0.646510  0.807850     0.158831
tabddpm_identity     Gradient Boosting  0.639277  0.807068     0.154043
smote                Gradient Boosting  0.562884  0.804574     0.155846
rus                  Random Forest      0.619757  0.801975     0.181109
smote_bs2            Gradient Boosting  0.523710  0.801455     0.156831
smote_tomek          Random Forest      0.639356  0.801195     0.151624
smote_bs1            Random Forest      0.641044  0.800416     0.152443
smote_tomek          Gradient Boosting  0.577250  0.799376     0.154447
smote_bs1            Gradient Boosting  0.545742  0.798857     0.157834
tabddpm_bgm          AdaBoost           0.621904  0.797993     0.243076
smote_enn            AdaBoost           0.532423  0.796258     0.236447
smote_tomek          AdaBoost           0.498032  0.795738     0.240586
tabddpm_res_bgm      AdaBoost           0.558370  0.795408     0.243563
                     Random Forest      0.554583  0.793665     0.167163
adasyn               Gradient Boosting  0.518638  0.793139     0.156741
tabddpm_bgm          Random Forest      0.646803  0.791982     0.161737
smote                Random Forest      0.580915  0.791580     0.156839
smote_enn            Random Forest      0.571405  0.791060     0.204302
smote_bs1            AdaBoost           0.503087  0.788981     0.240741
smote_enn            Gradient Boosting  0.524243  0.788462     0.241737
adasyn               AdaBoost           0.509310  0.787422     0.239379
smote                AdaBoost           0.489220  0.785863     0.238692
rus                  Gradient Boosting  0.485986  0.785863     0.185932
cwgan                Gradient Boosting  0.586654  0.785863     0.159176
ros                  Random Forest      0.563668  0.785603     0.156852
                     Gradient Boosting  0.545642  0.785343     0.172145
enn                  Random Forest      0.597760  0.785343     0.199940
cwgan                Random Forest      0.570500  0.781705     0.157503
enn                  Gradient Boosting  0.572804  0.777547     0.217696
rus                  AdaBoost           0.441203  0.776507     0.242263
tomek                Gradient Boosting  0.464456  0.774948     0.169526
                     AdaBoost           0.474484  0.773389     0.239779
enn                  AdaBoost           0.443985  0.771310     0.243384
ros                  AdaBoost           0.464614  0.762474     0.241799
tomek                Random Forest      0.540138  0.762214     0.162409
smote_bs2            AdaBoost           0.460603  0.761954     0.240370
cwgan                AdaBoost           0.418181  0.755717     0.244799
nearmiss             AdaBoost           0.405866  0.707900     0.243842
                     Gradient Boosting  0.434809  0.696466     0.224133
                     Random Forest      0.430121  0.689709     0.215533

Sorted by Brier-Score:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
cwgan                AdaBoost           0.418181  0.755717     0.244799
nearmiss             AdaBoost           0.405866  0.707900     0.243842
tabddpm_res_identity AdaBoost           0.593609  0.811876     0.243807
tabddpm_identity     AdaBoost           0.620721  0.830268     0.243807
tabddpm_res_bgm      AdaBoost           0.558370  0.795408     0.243563
enn                  AdaBoost           0.443985  0.771310     0.243384
tabddpm_bgm          AdaBoost           0.621904  0.797993     0.243076
rus                  AdaBoost           0.441203  0.776507     0.242263
ros                  AdaBoost           0.464614  0.762474     0.241799
smote_enn            Gradient Boosting  0.524243  0.788462     0.241737
smote_bs1            AdaBoost           0.503087  0.788981     0.240741
smote_tomek          AdaBoost           0.498032  0.795738     0.240586
smote_bs2            AdaBoost           0.460603  0.761954     0.240370
tomek                AdaBoost           0.474484  0.773389     0.239779
adasyn               AdaBoost           0.509310  0.787422     0.239379
smote                AdaBoost           0.489220  0.785863     0.238692
smote_enn            AdaBoost           0.532423  0.796258     0.236447
nearmiss             Gradient Boosting  0.434809  0.696466     0.224133
enn                  Gradient Boosting  0.572804  0.777547     0.217696
nearmiss             Random Forest      0.430121  0.689709     0.215533
smote_enn            Random Forest      0.571405  0.791060     0.204302
enn                  Random Forest      0.597760  0.785343     0.199940
rus                  Gradient Boosting  0.485986  0.785863     0.185932
                     Random Forest      0.619757  0.801975     0.181109
ros                  Gradient Boosting  0.545642  0.785343     0.172145
tomek                Gradient Boosting  0.464456  0.774948     0.169526
tabddpm_res_bgm      Random Forest      0.554583  0.793665     0.167163
tomek                Random Forest      0.540138  0.762214     0.162409
tabddpm_bgm          Random Forest      0.646803  0.791982     0.161737
tabddpm_res_bgm      Gradient Boosting  0.577872  0.809833     0.160765
tabddpm_identity     Random Forest      0.658690  0.808511     0.159947
cwgan                Gradient Boosting  0.586654  0.785863     0.159176
tabddpm_res_identity Random Forest      0.646510  0.807850     0.158831
tabddpm_bgm          Gradient Boosting  0.621707  0.811396     0.158547
smote_bs1            Gradient Boosting  0.545742  0.798857     0.157834
cwgan                Random Forest      0.570500  0.781705     0.157503
ros                  Random Forest      0.563668  0.785603     0.156852
smote                Random Forest      0.580915  0.791580     0.156839
smote_bs2            Gradient Boosting  0.523710  0.801455     0.156831
adasyn               Gradient Boosting  0.518638  0.793139     0.156741
smote                Gradient Boosting  0.562884  0.804574     0.155846
smote_tomek          Gradient Boosting  0.577250  0.799376     0.154447
tabddpm_identity     Gradient Boosting  0.639277  0.807068     0.154043
smote_bs1            Random Forest      0.641044  0.800416     0.152443
smote_bs2            Random Forest      0.602453  0.808472     0.151907
smote_tomek          Random Forest      0.639356  0.801195     0.151624
adasyn               Random Forest      0.648193  0.812110     0.148913
tabddpm_res_identity Gradient Boosting  0.683768  0.843611     0.143417

================================================================================


ANALYSIS FOR DATASET: uci_taiwan
==================================================

All Results:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
adasyn               AdaBoost           0.549939  0.760056     0.247338
                     Gradient Boosting  0.579500  0.780291     0.167737
                     Random Forest      0.539947  0.761406     0.154567
cwgan                AdaBoost           0.578833  0.792448     0.243458
                     Gradient Boosting  0.589875  0.802004     0.128585
                     Random Forest      0.570423  0.776110     0.132760
enn                  AdaBoost           0.585322  0.800890     0.243640
                     Gradient Boosting  0.586578  0.802124     0.147026
                     Random Forest      0.587964  0.786008     0.155922
nearmiss             AdaBoost           0.295983  0.638243     0.260278
                     Gradient Boosting  0.314471  0.658749     0.340387
                     Random Forest      0.335191  0.633629     0.393159
ros                  AdaBoost           0.577516  0.792857     0.246968
                     Gradient Boosting  0.590013  0.798558     0.171097
                     Random Forest      0.566425  0.778623     0.135838
rus                  AdaBoost           0.576733  0.793379     0.246919
                     Gradient Boosting  0.586022  0.799156     0.173272
                     Random Forest      0.572139  0.786932     0.180427
smote                AdaBoost           0.563264  0.771261     0.246718
                     Gradient Boosting  0.585007  0.786239     0.157707
                     Random Forest      0.549329  0.767539     0.150224
smote_bs1            AdaBoost           0.544023  0.759201     0.247450
                     Gradient Boosting  0.573286  0.783568     0.169800
                     Random Forest      0.518001  0.767028     0.153724
smote_bs2            AdaBoost           0.555773  0.780616     0.247492
                     Gradient Boosting  0.565018  0.787615     0.173067
                     Random Forest      0.529024  0.773336     0.150569
smote_enn            AdaBoost           0.569976  0.781311     0.245323
                     Gradient Boosting  0.582789  0.789103     0.178942
                     Random Forest      0.562460  0.774917     0.172613
smote_tomek          AdaBoost           0.563349  0.764640     0.246503
                     Gradient Boosting  0.581067  0.785447     0.157570
                     Random Forest      0.541150  0.762999     0.151578
tabddpm_bgm          AdaBoost           0.528044  0.777453     0.243848
                     Gradient Boosting  0.537174  0.776633     0.135257
                     Random Forest      0.520188  0.753912     0.139967
tabddpm_identity     AdaBoost           0.526922  0.773764     0.243700
                     Gradient Boosting  0.537294  0.777620     0.134989
                     Random Forest      0.521853  0.758888     0.138955
tabddpm_res_bgm      AdaBoost           0.521219  0.767137     0.243821
                     Gradient Boosting  0.538852  0.773085     0.135387
                     Random Forest      0.525572  0.757122     0.139087
tabddpm_res_identity AdaBoost           0.529826  0.778532     0.243703
                     Gradient Boosting  0.539179  0.779004     0.134852
                     Random Forest      0.522836  0.755217     0.139266
tomek                AdaBoost           0.578309  0.795087     0.243567
                     Gradient Boosting  0.590492  0.801174     0.127933
                     Random Forest      0.571649  0.783089     0.133052

Sorted by AUC-PR:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
tomek                Gradient Boosting  0.590492  0.801174     0.127933
ros                  Gradient Boosting  0.590013  0.798558     0.171097
cwgan                Gradient Boosting  0.589875  0.802004     0.128585
enn                  Random Forest      0.587964  0.786008     0.155922
                     Gradient Boosting  0.586578  0.802124     0.147026
rus                  Gradient Boosting  0.586022  0.799156     0.173272
enn                  AdaBoost           0.585322  0.800890     0.243640
smote                Gradient Boosting  0.585007  0.786239     0.157707
smote_enn            Gradient Boosting  0.582789  0.789103     0.178942
smote_tomek          Gradient Boosting  0.581067  0.785447     0.157570
adasyn               Gradient Boosting  0.579500  0.780291     0.167737
cwgan                AdaBoost           0.578833  0.792448     0.243458
tomek                AdaBoost           0.578309  0.795087     0.243567
ros                  AdaBoost           0.577516  0.792857     0.246968
rus                  AdaBoost           0.576733  0.793379     0.246919
smote_bs1            Gradient Boosting  0.573286  0.783568     0.169800
rus                  Random Forest      0.572139  0.786932     0.180427
tomek                Random Forest      0.571649  0.783089     0.133052
cwgan                Random Forest      0.570423  0.776110     0.132760
smote_enn            AdaBoost           0.569976  0.781311     0.245323
ros                  Random Forest      0.566425  0.778623     0.135838
smote_bs2            Gradient Boosting  0.565018  0.787615     0.173067
smote_tomek          AdaBoost           0.563349  0.764640     0.246503
smote                AdaBoost           0.563264  0.771261     0.246718
smote_enn            Random Forest      0.562460  0.774917     0.172613
smote_bs2            AdaBoost           0.555773  0.780616     0.247492
adasyn               AdaBoost           0.549939  0.760056     0.247338
smote                Random Forest      0.549329  0.767539     0.150224
smote_bs1            AdaBoost           0.544023  0.759201     0.247450
smote_tomek          Random Forest      0.541150  0.762999     0.151578
adasyn               Random Forest      0.539947  0.761406     0.154567
tabddpm_res_identity Gradient Boosting  0.539179  0.779004     0.134852
tabddpm_res_bgm      Gradient Boosting  0.538852  0.773085     0.135387
tabddpm_identity     Gradient Boosting  0.537294  0.777620     0.134989
tabddpm_bgm          Gradient Boosting  0.537174  0.776633     0.135257
tabddpm_res_identity AdaBoost           0.529826  0.778532     0.243703
smote_bs2            Random Forest      0.529024  0.773336     0.150569
tabddpm_bgm          AdaBoost           0.528044  0.777453     0.243848
tabddpm_identity     AdaBoost           0.526922  0.773764     0.243700
tabddpm_res_bgm      Random Forest      0.525572  0.757122     0.139087
tabddpm_res_identity Random Forest      0.522836  0.755217     0.139266
tabddpm_identity     Random Forest      0.521853  0.758888     0.138955
tabddpm_res_bgm      AdaBoost           0.521219  0.767137     0.243821
tabddpm_bgm          Random Forest      0.520188  0.753912     0.139967
smote_bs1            Random Forest      0.518001  0.767028     0.153724
nearmiss             Random Forest      0.335191  0.633629     0.393159
                     Gradient Boosting  0.314471  0.658749     0.340387
                     AdaBoost           0.295983  0.638243     0.260278

Sorted by AUC-ROC:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
enn                  Gradient Boosting  0.586578  0.802124     0.147026
cwgan                Gradient Boosting  0.589875  0.802004     0.128585
tomek                Gradient Boosting  0.590492  0.801174     0.127933
enn                  AdaBoost           0.585322  0.800890     0.243640
rus                  Gradient Boosting  0.586022  0.799156     0.173272
ros                  Gradient Boosting  0.590013  0.798558     0.171097
tomek                AdaBoost           0.578309  0.795087     0.243567
rus                  AdaBoost           0.576733  0.793379     0.246919
ros                  AdaBoost           0.577516  0.792857     0.246968
cwgan                AdaBoost           0.578833  0.792448     0.243458
smote_enn            Gradient Boosting  0.582789  0.789103     0.178942
smote_bs2            Gradient Boosting  0.565018  0.787615     0.173067
rus                  Random Forest      0.572139  0.786932     0.180427
smote                Gradient Boosting  0.585007  0.786239     0.157707
enn                  Random Forest      0.587964  0.786008     0.155922
smote_tomek          Gradient Boosting  0.581067  0.785447     0.157570
smote_bs1            Gradient Boosting  0.573286  0.783568     0.169800
tomek                Random Forest      0.571649  0.783089     0.133052
smote_enn            AdaBoost           0.569976  0.781311     0.245323
smote_bs2            AdaBoost           0.555773  0.780616     0.247492
adasyn               Gradient Boosting  0.579500  0.780291     0.167737
tabddpm_res_identity Gradient Boosting  0.539179  0.779004     0.134852
ros                  Random Forest      0.566425  0.778623     0.135838
tabddpm_res_identity AdaBoost           0.529826  0.778532     0.243703
tabddpm_identity     Gradient Boosting  0.537294  0.777620     0.134989
tabddpm_bgm          AdaBoost           0.528044  0.777453     0.243848
                     Gradient Boosting  0.537174  0.776633     0.135257
cwgan                Random Forest      0.570423  0.776110     0.132760
smote_enn            Random Forest      0.562460  0.774917     0.172613
tabddpm_identity     AdaBoost           0.526922  0.773764     0.243700
smote_bs2            Random Forest      0.529024  0.773336     0.150569
tabddpm_res_bgm      Gradient Boosting  0.538852  0.773085     0.135387
smote                AdaBoost           0.563264  0.771261     0.246718
                     Random Forest      0.549329  0.767539     0.150224
tabddpm_res_bgm      AdaBoost           0.521219  0.767137     0.243821
smote_bs1            Random Forest      0.518001  0.767028     0.153724
smote_tomek          AdaBoost           0.563349  0.764640     0.246503
                     Random Forest      0.541150  0.762999     0.151578
adasyn               Random Forest      0.539947  0.761406     0.154567
                     AdaBoost           0.549939  0.760056     0.247338
smote_bs1            AdaBoost           0.544023  0.759201     0.247450
tabddpm_identity     Random Forest      0.521853  0.758888     0.138955
tabddpm_res_bgm      Random Forest      0.525572  0.757122     0.139087
tabddpm_res_identity Random Forest      0.522836  0.755217     0.139266
tabddpm_bgm          Random Forest      0.520188  0.753912     0.139967
nearmiss             Gradient Boosting  0.314471  0.658749     0.340387
                     AdaBoost           0.295983  0.638243     0.260278
                     Random Forest      0.335191  0.633629     0.393159

Sorted by Brier-Score:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
nearmiss             Random Forest      0.335191  0.633629     0.393159
                     Gradient Boosting  0.314471  0.658749     0.340387
                     AdaBoost           0.295983  0.638243     0.260278
smote_bs2            AdaBoost           0.555773  0.780616     0.247492
smote_bs1            AdaBoost           0.544023  0.759201     0.247450
adasyn               AdaBoost           0.549939  0.760056     0.247338
ros                  AdaBoost           0.577516  0.792857     0.246968
rus                  AdaBoost           0.576733  0.793379     0.246919
smote                AdaBoost           0.563264  0.771261     0.246718
smote_tomek          AdaBoost           0.563349  0.764640     0.246503
smote_enn            AdaBoost           0.569976  0.781311     0.245323
tabddpm_bgm          AdaBoost           0.528044  0.777453     0.243848
tabddpm_res_bgm      AdaBoost           0.521219  0.767137     0.243821
tabddpm_res_identity AdaBoost           0.529826  0.778532     0.243703
tabddpm_identity     AdaBoost           0.526922  0.773764     0.243700
enn                  AdaBoost           0.585322  0.800890     0.243640
tomek                AdaBoost           0.578309  0.795087     0.243567
cwgan                AdaBoost           0.578833  0.792448     0.243458
rus                  Random Forest      0.572139  0.786932     0.180427
smote_enn            Gradient Boosting  0.582789  0.789103     0.178942
rus                  Gradient Boosting  0.586022  0.799156     0.173272
smote_bs2            Gradient Boosting  0.565018  0.787615     0.173067
smote_enn            Random Forest      0.562460  0.774917     0.172613
ros                  Gradient Boosting  0.590013  0.798558     0.171097
smote_bs1            Gradient Boosting  0.573286  0.783568     0.169800
adasyn               Gradient Boosting  0.579500  0.780291     0.167737
smote                Gradient Boosting  0.585007  0.786239     0.157707
smote_tomek          Gradient Boosting  0.581067  0.785447     0.157570
enn                  Random Forest      0.587964  0.786008     0.155922
adasyn               Random Forest      0.539947  0.761406     0.154567
smote_bs1            Random Forest      0.518001  0.767028     0.153724
smote_tomek          Random Forest      0.541150  0.762999     0.151578
smote_bs2            Random Forest      0.529024  0.773336     0.150569
smote                Random Forest      0.549329  0.767539     0.150224
enn                  Gradient Boosting  0.586578  0.802124     0.147026
tabddpm_bgm          Random Forest      0.520188  0.753912     0.139967
tabddpm_res_identity Random Forest      0.522836  0.755217     0.139266
tabddpm_res_bgm      Random Forest      0.525572  0.757122     0.139087
tabddpm_identity     Random Forest      0.521853  0.758888     0.138955
ros                  Random Forest      0.566425  0.778623     0.135838
tabddpm_res_bgm      Gradient Boosting  0.538852  0.773085     0.135387
tabddpm_bgm          Gradient Boosting  0.537174  0.776633     0.135257
tabddpm_identity     Gradient Boosting  0.537294  0.777620     0.134989
tabddpm_res_identity Gradient Boosting  0.539179  0.779004     0.134852
tomek                Random Forest      0.571649  0.783089     0.133052
cwgan                Random Forest      0.570423  0.776110     0.132760
                     Gradient Boosting  0.589875  0.802004     0.128585
tomek                Gradient Boosting  0.590492  0.801174     0.127933

================================================================================


ANALYSIS FOR DATASET: pakdd
==================================================

All Results:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
adasyn               AdaBoost           0.329513  0.604949     0.248661
                     Gradient Boosting  0.348370  0.631064     0.193206
                     Random Forest      0.344634  0.616172     0.191097
cwgan                AdaBoost           0.348115  0.636042     0.246934
                     Gradient Boosting  0.367443  0.649617     0.183091
                     Random Forest      0.347934  0.620836     0.187418
enn                  AdaBoost           0.351188  0.634802     0.248762
                     Gradient Boosting  0.360567  0.647261     0.219432
                     Random Forest      0.355352  0.636638     0.236706
nearmiss             AdaBoost           0.281189  0.553401     0.252630
                     Gradient Boosting  0.289347  0.556893     0.306519
                     Random Forest      0.284431  0.548321     0.320995
ros                  AdaBoost           0.353928  0.639810     0.249410
                     Gradient Boosting  0.367111  0.652908     0.233295
                     Random Forest      0.347424  0.625295     0.192818
rus                  AdaBoost           0.352757  0.639060     0.249390
                     Gradient Boosting  0.362194  0.652481     0.234595
                     Random Forest      0.344642  0.624728     0.240920
smote                AdaBoost           0.329032  0.601221     0.248598
                     Gradient Boosting  0.352901  0.635081     0.192165
                     Random Forest      0.342644  0.618555     0.190855
smote_bs1            AdaBoost           0.341141  0.611425     0.248537
                     Gradient Boosting  0.350235  0.631838     0.191759
                     Random Forest      0.345815  0.621807     0.190027
smote_bs2            AdaBoost           0.325868  0.601831     0.248812
                     Gradient Boosting  0.351021  0.629359     0.195611
                     Random Forest      0.344742  0.623183     0.191537
smote_enn            AdaBoost           0.337868  0.622056     0.250333
                     Gradient Boosting  0.367150  0.644687     0.278120
                     Random Forest      0.355362  0.635259     0.267145
smote_tomek          AdaBoost           0.328541  0.601498     0.248525
                     Gradient Boosting  0.349003  0.632973     0.191462
                     Random Forest      0.345516  0.620719     0.190116
tabddpm_bgm          AdaBoost           0.341749  0.613121     0.247145
                     Gradient Boosting  0.359335  0.625891     0.188181
                     Random Forest      0.341865  0.601992     0.192702
tabddpm_identity     AdaBoost           0.351568  0.621137     0.247101
                     Gradient Boosting  0.369232  0.630076     0.187671
                     Random Forest      0.340785  0.602164     0.192825
tabddpm_res_bgm      AdaBoost           0.338500  0.612308     0.247155
                     Gradient Boosting  0.349631  0.623634     0.188931
                     Random Forest      0.348823  0.605765     0.191968
tabddpm_res_identity AdaBoost           0.346711  0.615376     0.247141
                     Gradient Boosting  0.372150  0.633378     0.187380
                     Random Forest      0.349240  0.605510     0.191789
tomek                AdaBoost           0.346741  0.636121     0.247143
                     Gradient Boosting  0.366080  0.651303     0.182835
                     Random Forest      0.349886  0.624441     0.188486

Sorted by AUC-PR:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
tabddpm_res_identity Gradient Boosting  0.372150  0.633378     0.187380
tabddpm_identity     Gradient Boosting  0.369232  0.630076     0.187671
cwgan                Gradient Boosting  0.367443  0.649617     0.183091
smote_enn            Gradient Boosting  0.367150  0.644687     0.278120
ros                  Gradient Boosting  0.367111  0.652908     0.233295
tomek                Gradient Boosting  0.366080  0.651303     0.182835
rus                  Gradient Boosting  0.362194  0.652481     0.234595
enn                  Gradient Boosting  0.360567  0.647261     0.219432
tabddpm_bgm          Gradient Boosting  0.359335  0.625891     0.188181
smote_enn            Random Forest      0.355362  0.635259     0.267145
enn                  Random Forest      0.355352  0.636638     0.236706
ros                  AdaBoost           0.353928  0.639810     0.249410
smote                Gradient Boosting  0.352901  0.635081     0.192165
rus                  AdaBoost           0.352757  0.639060     0.249390
tabddpm_identity     AdaBoost           0.351568  0.621137     0.247101
enn                  AdaBoost           0.351188  0.634802     0.248762
smote_bs2            Gradient Boosting  0.351021  0.629359     0.195611
smote_bs1            Gradient Boosting  0.350235  0.631838     0.191759
tomek                Random Forest      0.349886  0.624441     0.188486
tabddpm_res_bgm      Gradient Boosting  0.349631  0.623634     0.188931
tabddpm_res_identity Random Forest      0.349240  0.605510     0.191789
smote_tomek          Gradient Boosting  0.349003  0.632973     0.191462
tabddpm_res_bgm      Random Forest      0.348823  0.605765     0.191968
adasyn               Gradient Boosting  0.348370  0.631064     0.193206
cwgan                AdaBoost           0.348115  0.636042     0.246934
                     Random Forest      0.347934  0.620836     0.187418
ros                  Random Forest      0.347424  0.625295     0.192818
tomek                AdaBoost           0.346741  0.636121     0.247143
tabddpm_res_identity AdaBoost           0.346711  0.615376     0.247141
smote_bs1            Random Forest      0.345815  0.621807     0.190027
smote_tomek          Random Forest      0.345516  0.620719     0.190116
smote_bs2            Random Forest      0.344742  0.623183     0.191537
rus                  Random Forest      0.344642  0.624728     0.240920
adasyn               Random Forest      0.344634  0.616172     0.191097
smote                Random Forest      0.342644  0.618555     0.190855
tabddpm_bgm          Random Forest      0.341865  0.601992     0.192702
                     AdaBoost           0.341749  0.613121     0.247145
smote_bs1            AdaBoost           0.341141  0.611425     0.248537
tabddpm_identity     Random Forest      0.340785  0.602164     0.192825
tabddpm_res_bgm      AdaBoost           0.338500  0.612308     0.247155
smote_enn            AdaBoost           0.337868  0.622056     0.250333
adasyn               AdaBoost           0.329513  0.604949     0.248661
smote                AdaBoost           0.329032  0.601221     0.248598
smote_tomek          AdaBoost           0.328541  0.601498     0.248525
smote_bs2            AdaBoost           0.325868  0.601831     0.248812
nearmiss             Gradient Boosting  0.289347  0.556893     0.306519
                     Random Forest      0.284431  0.548321     0.320995
                     AdaBoost           0.281189  0.553401     0.252630

Sorted by AUC-ROC:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
ros                  Gradient Boosting  0.367111  0.652908     0.233295
rus                  Gradient Boosting  0.362194  0.652481     0.234595
tomek                Gradient Boosting  0.366080  0.651303     0.182835
cwgan                Gradient Boosting  0.367443  0.649617     0.183091
enn                  Gradient Boosting  0.360567  0.647261     0.219432
smote_enn            Gradient Boosting  0.367150  0.644687     0.278120
ros                  AdaBoost           0.353928  0.639810     0.249410
rus                  AdaBoost           0.352757  0.639060     0.249390
enn                  Random Forest      0.355352  0.636638     0.236706
tomek                AdaBoost           0.346741  0.636121     0.247143
cwgan                AdaBoost           0.348115  0.636042     0.246934
smote_enn            Random Forest      0.355362  0.635259     0.267145
smote                Gradient Boosting  0.352901  0.635081     0.192165
enn                  AdaBoost           0.351188  0.634802     0.248762
tabddpm_res_identity Gradient Boosting  0.372150  0.633378     0.187380
smote_tomek          Gradient Boosting  0.349003  0.632973     0.191462
smote_bs1            Gradient Boosting  0.350235  0.631838     0.191759
adasyn               Gradient Boosting  0.348370  0.631064     0.193206
tabddpm_identity     Gradient Boosting  0.369232  0.630076     0.187671
smote_bs2            Gradient Boosting  0.351021  0.629359     0.195611
tabddpm_bgm          Gradient Boosting  0.359335  0.625891     0.188181
ros                  Random Forest      0.347424  0.625295     0.192818
rus                  Random Forest      0.344642  0.624728     0.240920
tomek                Random Forest      0.349886  0.624441     0.188486
tabddpm_res_bgm      Gradient Boosting  0.349631  0.623634     0.188931
smote_bs2            Random Forest      0.344742  0.623183     0.191537
smote_enn            AdaBoost           0.337868  0.622056     0.250333
smote_bs1            Random Forest      0.345815  0.621807     0.190027
tabddpm_identity     AdaBoost           0.351568  0.621137     0.247101
cwgan                Random Forest      0.347934  0.620836     0.187418
smote_tomek          Random Forest      0.345516  0.620719     0.190116
smote                Random Forest      0.342644  0.618555     0.190855
adasyn               Random Forest      0.344634  0.616172     0.191097
tabddpm_res_identity AdaBoost           0.346711  0.615376     0.247141
tabddpm_bgm          AdaBoost           0.341749  0.613121     0.247145
tabddpm_res_bgm      AdaBoost           0.338500  0.612308     0.247155
smote_bs1            AdaBoost           0.341141  0.611425     0.248537
tabddpm_res_bgm      Random Forest      0.348823  0.605765     0.191968
tabddpm_res_identity Random Forest      0.349240  0.605510     0.191789
adasyn               AdaBoost           0.329513  0.604949     0.248661
tabddpm_identity     Random Forest      0.340785  0.602164     0.192825
tabddpm_bgm          Random Forest      0.341865  0.601992     0.192702
smote_bs2            AdaBoost           0.325868  0.601831     0.248812
smote_tomek          AdaBoost           0.328541  0.601498     0.248525
smote                AdaBoost           0.329032  0.601221     0.248598
nearmiss             Gradient Boosting  0.289347  0.556893     0.306519
                     AdaBoost           0.281189  0.553401     0.252630
                     Random Forest      0.284431  0.548321     0.320995

Sorted by Brier-Score:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
nearmiss             Random Forest      0.284431  0.548321     0.320995
                     Gradient Boosting  0.289347  0.556893     0.306519
smote_enn            Gradient Boosting  0.367150  0.644687     0.278120
                     Random Forest      0.355362  0.635259     0.267145
nearmiss             AdaBoost           0.281189  0.553401     0.252630
smote_enn            AdaBoost           0.337868  0.622056     0.250333
ros                  AdaBoost           0.353928  0.639810     0.249410
rus                  AdaBoost           0.352757  0.639060     0.249390
smote_bs2            AdaBoost           0.325868  0.601831     0.248812
enn                  AdaBoost           0.351188  0.634802     0.248762
adasyn               AdaBoost           0.329513  0.604949     0.248661
smote                AdaBoost           0.329032  0.601221     0.248598
smote_bs1            AdaBoost           0.341141  0.611425     0.248537
smote_tomek          AdaBoost           0.328541  0.601498     0.248525
tabddpm_res_bgm      AdaBoost           0.338500  0.612308     0.247155
tabddpm_bgm          AdaBoost           0.341749  0.613121     0.247145
tomek                AdaBoost           0.346741  0.636121     0.247143
tabddpm_res_identity AdaBoost           0.346711  0.615376     0.247141
tabddpm_identity     AdaBoost           0.351568  0.621137     0.247101
cwgan                AdaBoost           0.348115  0.636042     0.246934
rus                  Random Forest      0.344642  0.624728     0.240920
enn                  Random Forest      0.355352  0.636638     0.236706
rus                  Gradient Boosting  0.362194  0.652481     0.234595
ros                  Gradient Boosting  0.367111  0.652908     0.233295
enn                  Gradient Boosting  0.360567  0.647261     0.219432
smote_bs2            Gradient Boosting  0.351021  0.629359     0.195611
adasyn               Gradient Boosting  0.348370  0.631064     0.193206
tabddpm_identity     Random Forest      0.340785  0.602164     0.192825
ros                  Random Forest      0.347424  0.625295     0.192818
tabddpm_bgm          Random Forest      0.341865  0.601992     0.192702
smote                Gradient Boosting  0.352901  0.635081     0.192165
tabddpm_res_bgm      Random Forest      0.348823  0.605765     0.191968
tabddpm_res_identity Random Forest      0.349240  0.605510     0.191789
smote_bs1            Gradient Boosting  0.350235  0.631838     0.191759
smote_bs2            Random Forest      0.344742  0.623183     0.191537
smote_tomek          Gradient Boosting  0.349003  0.632973     0.191462
adasyn               Random Forest      0.344634  0.616172     0.191097
smote                Random Forest      0.342644  0.618555     0.190855
smote_tomek          Random Forest      0.345516  0.620719     0.190116
smote_bs1            Random Forest      0.345815  0.621807     0.190027
tabddpm_res_bgm      Gradient Boosting  0.349631  0.623634     0.188931
tomek                Random Forest      0.349886  0.624441     0.188486
tabddpm_bgm          Gradient Boosting  0.359335  0.625891     0.188181
tabddpm_identity     Gradient Boosting  0.369232  0.630076     0.187671
cwgan                Random Forest      0.347934  0.620836     0.187418
tabddpm_res_identity Gradient Boosting  0.372150  0.633378     0.187380
cwgan                Gradient Boosting  0.367443  0.649617     0.183091
tomek                Gradient Boosting  0.366080  0.651303     0.182835

================================================================================


ANALYSIS FOR DATASET: hmeq
==================================================

All Results:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
adasyn               AdaBoost           0.787464  0.895937     0.239297
                     Gradient Boosting  0.830110  0.920592     0.087420
                     Random Forest      0.927409  0.978556     0.052694
cwgan                AdaBoost           0.778741  0.883880     0.235574
                     Gradient Boosting  0.823525  0.905890     0.075741
                     Random Forest      0.936093  0.978191     0.051252
enn                  AdaBoost           0.815958  0.911005     0.232829
                     Gradient Boosting  0.841927  0.924332     0.071879
                     Random Forest      0.916634  0.972788     0.056714
nearmiss             AdaBoost           0.480540  0.802732     0.255737
                     Gradient Boosting  0.648859  0.853728     0.231082
                     Random Forest      0.585694  0.840957     0.260766
ros                  AdaBoost           0.829392  0.918643     0.236899
                     Gradient Boosting  0.835648  0.920661     0.092015
                     Random Forest      0.900732  0.969743     0.057369
rus                  AdaBoost           0.805823  0.916216     0.237071
                     Gradient Boosting  0.835387  0.925176     0.096152
                     Random Forest      0.853232  0.948586     0.091188
smote                AdaBoost           0.801818  0.901836     0.238698
                     Gradient Boosting  0.828723  0.919060     0.086151
                     Random Forest      0.934014  0.979635     0.051378
smote_bs1            AdaBoost           0.778182  0.891283     0.239635
                     Gradient Boosting  0.818741  0.916938     0.091256
                     Random Forest      0.921685  0.974833     0.054418
smote_bs2            AdaBoost           0.767721  0.892736     0.238990
                     Gradient Boosting  0.822701  0.919861     0.094760
                     Random Forest      0.934954  0.979104     0.051630
smote_enn            AdaBoost           0.786747  0.890883     0.238745
                     Gradient Boosting  0.829212  0.915685     0.084983
                     Random Forest      0.909533  0.973554     0.054862
smote_tomek          AdaBoost           0.796829  0.895963     0.238739
                     Gradient Boosting  0.828820  0.917251     0.087283
                     Random Forest      0.927662  0.977512     0.053104
tabddpm_bgm          AdaBoost           0.794839  0.899196     0.234129
                     Gradient Boosting  0.841231  0.929635     0.078671
                     Random Forest      0.881305  0.964627     0.066340
tabddpm_identity     AdaBoost           0.820997  0.913436     0.233030
                     Gradient Boosting  0.849453  0.932643     0.076128
                     Random Forest      0.885545  0.963909     0.066478
tabddpm_res_bgm      AdaBoost           0.793699  0.898612     0.234058
                     Gradient Boosting  0.838556  0.923362     0.079164
                     Random Forest      0.888444  0.964444     0.065447
tabddpm_res_identity AdaBoost           0.820403  0.916774     0.232681
                     Gradient Boosting  0.850166  0.935900     0.075562
                     Random Forest      0.872827  0.960699     0.068846
tomek                AdaBoost           0.818017  0.911779     0.232932
                     Gradient Boosting  0.842925  0.919565     0.071121
                     Random Forest      0.920331  0.973301     0.055338

Sorted by AUC-PR:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
cwgan                Random Forest      0.936093  0.978191     0.051252
smote_bs2            Random Forest      0.934954  0.979104     0.051630
smote                Random Forest      0.934014  0.979635     0.051378
smote_tomek          Random Forest      0.927662  0.977512     0.053104
adasyn               Random Forest      0.927409  0.978556     0.052694
smote_bs1            Random Forest      0.921685  0.974833     0.054418
tomek                Random Forest      0.920331  0.973301     0.055338
enn                  Random Forest      0.916634  0.972788     0.056714
smote_enn            Random Forest      0.909533  0.973554     0.054862
ros                  Random Forest      0.900732  0.969743     0.057369
tabddpm_res_bgm      Random Forest      0.888444  0.964444     0.065447
tabddpm_identity     Random Forest      0.885545  0.963909     0.066478
tabddpm_bgm          Random Forest      0.881305  0.964627     0.066340
tabddpm_res_identity Random Forest      0.872827  0.960699     0.068846
rus                  Random Forest      0.853232  0.948586     0.091188
tabddpm_res_identity Gradient Boosting  0.850166  0.935900     0.075562
tabddpm_identity     Gradient Boosting  0.849453  0.932643     0.076128
tomek                Gradient Boosting  0.842925  0.919565     0.071121
enn                  Gradient Boosting  0.841927  0.924332     0.071879
tabddpm_bgm          Gradient Boosting  0.841231  0.929635     0.078671
tabddpm_res_bgm      Gradient Boosting  0.838556  0.923362     0.079164
ros                  Gradient Boosting  0.835648  0.920661     0.092015
rus                  Gradient Boosting  0.835387  0.925176     0.096152
adasyn               Gradient Boosting  0.830110  0.920592     0.087420
ros                  AdaBoost           0.829392  0.918643     0.236899
smote_enn            Gradient Boosting  0.829212  0.915685     0.084983
smote_tomek          Gradient Boosting  0.828820  0.917251     0.087283
smote                Gradient Boosting  0.828723  0.919060     0.086151
cwgan                Gradient Boosting  0.823525  0.905890     0.075741
smote_bs2            Gradient Boosting  0.822701  0.919861     0.094760
tabddpm_identity     AdaBoost           0.820997  0.913436     0.233030
tabddpm_res_identity AdaBoost           0.820403  0.916774     0.232681
smote_bs1            Gradient Boosting  0.818741  0.916938     0.091256
tomek                AdaBoost           0.818017  0.911779     0.232932
enn                  AdaBoost           0.815958  0.911005     0.232829
rus                  AdaBoost           0.805823  0.916216     0.237071
smote                AdaBoost           0.801818  0.901836     0.238698
smote_tomek          AdaBoost           0.796829  0.895963     0.238739
tabddpm_bgm          AdaBoost           0.794839  0.899196     0.234129
tabddpm_res_bgm      AdaBoost           0.793699  0.898612     0.234058
adasyn               AdaBoost           0.787464  0.895937     0.239297
smote_enn            AdaBoost           0.786747  0.890883     0.238745
cwgan                AdaBoost           0.778741  0.883880     0.235574
smote_bs1            AdaBoost           0.778182  0.891283     0.239635
smote_bs2            AdaBoost           0.767721  0.892736     0.238990
nearmiss             Gradient Boosting  0.648859  0.853728     0.231082
                     Random Forest      0.585694  0.840957     0.260766
                     AdaBoost           0.480540  0.802732     0.255737

Sorted by AUC-ROC:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
smote                Random Forest      0.934014  0.979635     0.051378
smote_bs2            Random Forest      0.934954  0.979104     0.051630
adasyn               Random Forest      0.927409  0.978556     0.052694
cwgan                Random Forest      0.936093  0.978191     0.051252
smote_tomek          Random Forest      0.927662  0.977512     0.053104
smote_bs1            Random Forest      0.921685  0.974833     0.054418
smote_enn            Random Forest      0.909533  0.973554     0.054862
tomek                Random Forest      0.920331  0.973301     0.055338
enn                  Random Forest      0.916634  0.972788     0.056714
ros                  Random Forest      0.900732  0.969743     0.057369
tabddpm_bgm          Random Forest      0.881305  0.964627     0.066340
tabddpm_res_bgm      Random Forest      0.888444  0.964444     0.065447
tabddpm_identity     Random Forest      0.885545  0.963909     0.066478
tabddpm_res_identity Random Forest      0.872827  0.960699     0.068846
rus                  Random Forest      0.853232  0.948586     0.091188
tabddpm_res_identity Gradient Boosting  0.850166  0.935900     0.075562
tabddpm_identity     Gradient Boosting  0.849453  0.932643     0.076128
tabddpm_bgm          Gradient Boosting  0.841231  0.929635     0.078671
rus                  Gradient Boosting  0.835387  0.925176     0.096152
enn                  Gradient Boosting  0.841927  0.924332     0.071879
tabddpm_res_bgm      Gradient Boosting  0.838556  0.923362     0.079164
ros                  Gradient Boosting  0.835648  0.920661     0.092015
adasyn               Gradient Boosting  0.830110  0.920592     0.087420
smote_bs2            Gradient Boosting  0.822701  0.919861     0.094760
tomek                Gradient Boosting  0.842925  0.919565     0.071121
smote                Gradient Boosting  0.828723  0.919060     0.086151
ros                  AdaBoost           0.829392  0.918643     0.236899
smote_tomek          Gradient Boosting  0.828820  0.917251     0.087283
smote_bs1            Gradient Boosting  0.818741  0.916938     0.091256
tabddpm_res_identity AdaBoost           0.820403  0.916774     0.232681
rus                  AdaBoost           0.805823  0.916216     0.237071
smote_enn            Gradient Boosting  0.829212  0.915685     0.084983
tabddpm_identity     AdaBoost           0.820997  0.913436     0.233030
tomek                AdaBoost           0.818017  0.911779     0.232932
enn                  AdaBoost           0.815958  0.911005     0.232829
cwgan                Gradient Boosting  0.823525  0.905890     0.075741
smote                AdaBoost           0.801818  0.901836     0.238698
tabddpm_bgm          AdaBoost           0.794839  0.899196     0.234129
tabddpm_res_bgm      AdaBoost           0.793699  0.898612     0.234058
smote_tomek          AdaBoost           0.796829  0.895963     0.238739
adasyn               AdaBoost           0.787464  0.895937     0.239297
smote_bs2            AdaBoost           0.767721  0.892736     0.238990
smote_bs1            AdaBoost           0.778182  0.891283     0.239635
smote_enn            AdaBoost           0.786747  0.890883     0.238745
cwgan                AdaBoost           0.778741  0.883880     0.235574
nearmiss             Gradient Boosting  0.648859  0.853728     0.231082
                     Random Forest      0.585694  0.840957     0.260766
                     AdaBoost           0.480540  0.802732     0.255737

Sorted by Brier-Score:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
nearmiss             Random Forest      0.585694  0.840957     0.260766
                     AdaBoost           0.480540  0.802732     0.255737
smote_bs1            AdaBoost           0.778182  0.891283     0.239635
adasyn               AdaBoost           0.787464  0.895937     0.239297
smote_bs2            AdaBoost           0.767721  0.892736     0.238990
smote_enn            AdaBoost           0.786747  0.890883     0.238745
smote_tomek          AdaBoost           0.796829  0.895963     0.238739
smote                AdaBoost           0.801818  0.901836     0.238698
rus                  AdaBoost           0.805823  0.916216     0.237071
ros                  AdaBoost           0.829392  0.918643     0.236899
cwgan                AdaBoost           0.778741  0.883880     0.235574
tabddpm_bgm          AdaBoost           0.794839  0.899196     0.234129
tabddpm_res_bgm      AdaBoost           0.793699  0.898612     0.234058
tabddpm_identity     AdaBoost           0.820997  0.913436     0.233030
tomek                AdaBoost           0.818017  0.911779     0.232932
enn                  AdaBoost           0.815958  0.911005     0.232829
tabddpm_res_identity AdaBoost           0.820403  0.916774     0.232681
nearmiss             Gradient Boosting  0.648859  0.853728     0.231082
rus                  Gradient Boosting  0.835387  0.925176     0.096152
smote_bs2            Gradient Boosting  0.822701  0.919861     0.094760
ros                  Gradient Boosting  0.835648  0.920661     0.092015
smote_bs1            Gradient Boosting  0.818741  0.916938     0.091256
rus                  Random Forest      0.853232  0.948586     0.091188
adasyn               Gradient Boosting  0.830110  0.920592     0.087420
smote_tomek          Gradient Boosting  0.828820  0.917251     0.087283
smote                Gradient Boosting  0.828723  0.919060     0.086151
smote_enn            Gradient Boosting  0.829212  0.915685     0.084983
tabddpm_res_bgm      Gradient Boosting  0.838556  0.923362     0.079164
tabddpm_bgm          Gradient Boosting  0.841231  0.929635     0.078671
tabddpm_identity     Gradient Boosting  0.849453  0.932643     0.076128
cwgan                Gradient Boosting  0.823525  0.905890     0.075741
tabddpm_res_identity Gradient Boosting  0.850166  0.935900     0.075562
enn                  Gradient Boosting  0.841927  0.924332     0.071879
tomek                Gradient Boosting  0.842925  0.919565     0.071121
tabddpm_res_identity Random Forest      0.872827  0.960699     0.068846
tabddpm_identity     Random Forest      0.885545  0.963909     0.066478
tabddpm_bgm          Random Forest      0.881305  0.964627     0.066340
tabddpm_res_bgm      Random Forest      0.888444  0.964444     0.065447
ros                  Random Forest      0.900732  0.969743     0.057369
enn                  Random Forest      0.916634  0.972788     0.056714
tomek                Random Forest      0.920331  0.973301     0.055338
smote_enn            Random Forest      0.909533  0.973554     0.054862
smote_bs1            Random Forest      0.921685  0.974833     0.054418
smote_tomek          Random Forest      0.927662  0.977512     0.053104
adasyn               Random Forest      0.927409  0.978556     0.052694
smote_bs2            Random Forest      0.934954  0.979104     0.051630
smote                Random Forest      0.934014  0.979635     0.051378
cwgan                Random Forest      0.936093  0.978191     0.051252

================================================================================


ANALYSIS FOR DATASET: gmsc
==================================================

All Results:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
adasyn               AdaBoost           0.318681  0.803322     0.244079
                     Gradient Boosting  0.339183  0.815890     0.089026
                     Random Forest      0.301639  0.818391     0.062014
cwgan                AdaBoost           0.349580  0.822823     0.234874
                     Gradient Boosting  0.376983  0.829356     0.051193
                     Random Forest      0.357556  0.832151     0.051628
enn                  AdaBoost           0.368129  0.825312     0.235237
                     Gradient Boosting  0.380785  0.830812     0.053193
                     Random Forest      0.366399  0.840301     0.055119
nearmiss             AdaBoost           0.118109  0.688322     0.275609
                     Gradient Boosting  0.218863  0.744820     0.411491
                     Random Forest      0.322053  0.750976     0.477892
ros                  AdaBoost           0.356996  0.825673     0.246057
                     Gradient Boosting  0.374420  0.830456     0.146957
                     Random Forest      0.317711  0.822828     0.055993
rus                  AdaBoost           0.346518  0.825738     0.246070
                     Gradient Boosting  0.375477  0.838991     0.147161
                     Random Forest      0.359229  0.848064     0.151527
smote                AdaBoost           0.326516  0.800849     0.243516
                     Gradient Boosting  0.344699  0.818798     0.083547
                     Random Forest      0.305791  0.824649     0.060796
smote_bs1            AdaBoost           0.323186  0.808338     0.241669
                     Gradient Boosting  0.336933  0.821698     0.078912
                     Random Forest      0.315039  0.831935     0.057528
smote_bs2            AdaBoost           0.325775  0.807613     0.242992
                     Gradient Boosting  0.340910  0.821636     0.100125
                     Random Forest      0.302794  0.835216     0.058719
smote_enn            AdaBoost           0.342788  0.815325     0.242187
                     Gradient Boosting  0.357769  0.828603     0.091772
                     Random Forest      0.354440  0.840615     0.067961
smote_tomek          AdaBoost           0.329682  0.809689     0.243305
                     Gradient Boosting  0.347268  0.821383     0.081840
                     Random Forest      0.306641  0.825735     0.060382
tabddpm_bgm          AdaBoost           0.327993  0.814214     0.235645
                     Gradient Boosting  0.355200  0.826865     0.049964
                     Random Forest      0.317963  0.795032     0.053583
tabddpm_identity     AdaBoost           0.336278  0.818791     0.245026
                     Gradient Boosting  0.334725  0.827308     0.116552
                     Random Forest      0.248803  0.782873     0.130914
tabddpm_res_bgm      AdaBoost           0.338276  0.822108     0.235526
                     Gradient Boosting  0.354492  0.826676     0.049880
                     Random Forest      0.316624  0.792232     0.053902
tabddpm_res_identity AdaBoost           0.336836  0.831913     0.234839
                     Gradient Boosting  0.362983  0.838434     0.049603
                     Random Forest      0.310776  0.794845     0.053633
tomek                AdaBoost           0.353659  0.825039     0.234946
                     Gradient Boosting  0.385126  0.830312     0.050311
                     Random Forest      0.360056  0.837585     0.051650

Sorted by AUC-PR:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
tomek                Gradient Boosting  0.385126  0.830312     0.050311
enn                  Gradient Boosting  0.380785  0.830812     0.053193
cwgan                Gradient Boosting  0.376983  0.829356     0.051193
rus                  Gradient Boosting  0.375477  0.838991     0.147161
ros                  Gradient Boosting  0.374420  0.830456     0.146957
enn                  AdaBoost           0.368129  0.825312     0.235237
                     Random Forest      0.366399  0.840301     0.055119
tabddpm_res_identity Gradient Boosting  0.362983  0.838434     0.049603
tomek                Random Forest      0.360056  0.837585     0.051650
rus                  Random Forest      0.359229  0.848064     0.151527
smote_enn            Gradient Boosting  0.357769  0.828603     0.091772
cwgan                Random Forest      0.357556  0.832151     0.051628
ros                  AdaBoost           0.356996  0.825673     0.246057
tabddpm_bgm          Gradient Boosting  0.355200  0.826865     0.049964
tabddpm_res_bgm      Gradient Boosting  0.354492  0.826676     0.049880
smote_enn            Random Forest      0.354440  0.840615     0.067961
tomek                AdaBoost           0.353659  0.825039     0.234946
cwgan                AdaBoost           0.349580  0.822823     0.234874
smote_tomek          Gradient Boosting  0.347268  0.821383     0.081840
rus                  AdaBoost           0.346518  0.825738     0.246070
smote                Gradient Boosting  0.344699  0.818798     0.083547
smote_enn            AdaBoost           0.342788  0.815325     0.242187
smote_bs2            Gradient Boosting  0.340910  0.821636     0.100125
adasyn               Gradient Boosting  0.339183  0.815890     0.089026
tabddpm_res_bgm      AdaBoost           0.338276  0.822108     0.235526
smote_bs1            Gradient Boosting  0.336933  0.821698     0.078912
tabddpm_res_identity AdaBoost           0.336836  0.831913     0.234839
tabddpm_identity     AdaBoost           0.336278  0.818791     0.245026
                     Gradient Boosting  0.334725  0.827308     0.116552
smote_tomek          AdaBoost           0.329682  0.809689     0.243305
tabddpm_bgm          AdaBoost           0.327993  0.814214     0.235645
smote                AdaBoost           0.326516  0.800849     0.243516
smote_bs2            AdaBoost           0.325775  0.807613     0.242992
smote_bs1            AdaBoost           0.323186  0.808338     0.241669
nearmiss             Random Forest      0.322053  0.750976     0.477892
adasyn               AdaBoost           0.318681  0.803322     0.244079
tabddpm_bgm          Random Forest      0.317963  0.795032     0.053583
ros                  Random Forest      0.317711  0.822828     0.055993
tabddpm_res_bgm      Random Forest      0.316624  0.792232     0.053902
smote_bs1            Random Forest      0.315039  0.831935     0.057528
tabddpm_res_identity Random Forest      0.310776  0.794845     0.053633
smote_tomek          Random Forest      0.306641  0.825735     0.060382
smote                Random Forest      0.305791  0.824649     0.060796
smote_bs2            Random Forest      0.302794  0.835216     0.058719
adasyn               Random Forest      0.301639  0.818391     0.062014
tabddpm_identity     Random Forest      0.248803  0.782873     0.130914
nearmiss             Gradient Boosting  0.218863  0.744820     0.411491
                     AdaBoost           0.118109  0.688322     0.275609

Sorted by AUC-ROC:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
rus                  Random Forest      0.359229  0.848064     0.151527
smote_enn            Random Forest      0.354440  0.840615     0.067961
enn                  Random Forest      0.366399  0.840301     0.055119
rus                  Gradient Boosting  0.375477  0.838991     0.147161
tabddpm_res_identity Gradient Boosting  0.362983  0.838434     0.049603
tomek                Random Forest      0.360056  0.837585     0.051650
smote_bs2            Random Forest      0.302794  0.835216     0.058719
cwgan                Random Forest      0.357556  0.832151     0.051628
smote_bs1            Random Forest      0.315039  0.831935     0.057528
tabddpm_res_identity AdaBoost           0.336836  0.831913     0.234839
enn                  Gradient Boosting  0.380785  0.830812     0.053193
ros                  Gradient Boosting  0.374420  0.830456     0.146957
tomek                Gradient Boosting  0.385126  0.830312     0.050311
cwgan                Gradient Boosting  0.376983  0.829356     0.051193
smote_enn            Gradient Boosting  0.357769  0.828603     0.091772
tabddpm_identity     Gradient Boosting  0.334725  0.827308     0.116552
tabddpm_bgm          Gradient Boosting  0.355200  0.826865     0.049964
tabddpm_res_bgm      Gradient Boosting  0.354492  0.826676     0.049880
rus                  AdaBoost           0.346518  0.825738     0.246070
smote_tomek          Random Forest      0.306641  0.825735     0.060382
ros                  AdaBoost           0.356996  0.825673     0.246057
enn                  AdaBoost           0.368129  0.825312     0.235237
tomek                AdaBoost           0.353659  0.825039     0.234946
smote                Random Forest      0.305791  0.824649     0.060796
ros                  Random Forest      0.317711  0.822828     0.055993
cwgan                AdaBoost           0.349580  0.822823     0.234874
tabddpm_res_bgm      AdaBoost           0.338276  0.822108     0.235526
smote_bs1            Gradient Boosting  0.336933  0.821698     0.078912
smote_bs2            Gradient Boosting  0.340910  0.821636     0.100125
smote_tomek          Gradient Boosting  0.347268  0.821383     0.081840
smote                Gradient Boosting  0.344699  0.818798     0.083547
tabddpm_identity     AdaBoost           0.336278  0.818791     0.245026
adasyn               Random Forest      0.301639  0.818391     0.062014
                     Gradient Boosting  0.339183  0.815890     0.089026
smote_enn            AdaBoost           0.342788  0.815325     0.242187
tabddpm_bgm          AdaBoost           0.327993  0.814214     0.235645
smote_tomek          AdaBoost           0.329682  0.809689     0.243305
smote_bs1            AdaBoost           0.323186  0.808338     0.241669
smote_bs2            AdaBoost           0.325775  0.807613     0.242992
adasyn               AdaBoost           0.318681  0.803322     0.244079
smote                AdaBoost           0.326516  0.800849     0.243516
tabddpm_bgm          Random Forest      0.317963  0.795032     0.053583
tabddpm_res_identity Random Forest      0.310776  0.794845     0.053633
tabddpm_res_bgm      Random Forest      0.316624  0.792232     0.053902
tabddpm_identity     Random Forest      0.248803  0.782873     0.130914
nearmiss             Random Forest      0.322053  0.750976     0.477892
                     Gradient Boosting  0.218863  0.744820     0.411491
                     AdaBoost           0.118109  0.688322     0.275609

Sorted by Brier-Score:
metric                                    AUC-PR   AUC-ROC  Brier-Score
resample_method      classifier
nearmiss             Random Forest      0.322053  0.750976     0.477892
                     Gradient Boosting  0.218863  0.744820     0.411491
                     AdaBoost           0.118109  0.688322     0.275609
rus                  AdaBoost           0.346518  0.825738     0.246070
ros                  AdaBoost           0.356996  0.825673     0.246057
tabddpm_identity     AdaBoost           0.336278  0.818791     0.245026
adasyn               AdaBoost           0.318681  0.803322     0.244079
smote                AdaBoost           0.326516  0.800849     0.243516
smote_tomek          AdaBoost           0.329682  0.809689     0.243305
smote_bs2            AdaBoost           0.325775  0.807613     0.242992
smote_enn            AdaBoost           0.342788  0.815325     0.242187
smote_bs1            AdaBoost           0.323186  0.808338     0.241669
tabddpm_bgm          AdaBoost           0.327993  0.814214     0.235645
tabddpm_res_bgm      AdaBoost           0.338276  0.822108     0.235526
enn                  AdaBoost           0.368129  0.825312     0.235237
tomek                AdaBoost           0.353659  0.825039     0.234946
cwgan                AdaBoost           0.349580  0.822823     0.234874
tabddpm_res_identity AdaBoost           0.336836  0.831913     0.234839
rus                  Random Forest      0.359229  0.848064     0.151527
                     Gradient Boosting  0.375477  0.838991     0.147161
ros                  Gradient Boosting  0.374420  0.830456     0.146957
tabddpm_identity     Random Forest      0.248803  0.782873     0.130914
                     Gradient Boosting  0.334725  0.827308     0.116552
smote_bs2            Gradient Boosting  0.340910  0.821636     0.100125
smote_enn            Gradient Boosting  0.357769  0.828603     0.091772
adasyn               Gradient Boosting  0.339183  0.815890     0.089026
smote                Gradient Boosting  0.344699  0.818798     0.083547
smote_tomek          Gradient Boosting  0.347268  0.821383     0.081840
smote_bs1            Gradient Boosting  0.336933  0.821698     0.078912
smote_enn            Random Forest      0.354440  0.840615     0.067961
adasyn               Random Forest      0.301639  0.818391     0.062014
smote                Random Forest      0.305791  0.824649     0.060796
smote_tomek          Random Forest      0.306641  0.825735     0.060382
smote_bs2            Random Forest      0.302794  0.835216     0.058719
smote_bs1            Random Forest      0.315039  0.831935     0.057528
ros                  Random Forest      0.317711  0.822828     0.055993
enn                  Random Forest      0.366399  0.840301     0.055119
tabddpm_res_bgm      Random Forest      0.316624  0.792232     0.053902
tabddpm_res_identity Random Forest      0.310776  0.794845     0.053633
tabddpm_bgm          Random Forest      0.317963  0.795032     0.053583
enn                  Gradient Boosting  0.380785  0.830812     0.053193
tomek                Random Forest      0.360056  0.837585     0.051650
cwgan                Random Forest      0.357556  0.832151     0.051628
                     Gradient Boosting  0.376983  0.829356     0.051193
tomek                Gradient Boosting  0.385126  0.830312     0.050311
tabddpm_bgm          Gradient Boosting  0.355200  0.826865     0.049964
tabddpm_res_bgm      Gradient Boosting  0.354492  0.826676     0.049880
tabddpm_res_identity Gradient Boosting  0.362983  0.838434     0.049603

================================================================================

Finished python src\main.py at: Fri 12/27/2024 17:03:30.74
All tasks completed at: Fri 12/27/2024 17:03:30.74
Press any key to continue . . .
